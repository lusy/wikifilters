\cite{MuellerBirn2014}

Interesting info, not sure what to do with it

2010: 40% of newcommers' contributions rejected based on automatic tools <---
counter productive in engaging new contributors

=====================================================
\cite{GeiHal2017}

Claudia's paper:
"“In both cases of algorithmic governance
– software features and bots – making rules part of the infrastructure, to a certain extent, makes
them harder to change and easier to enforce” (p. 87)"

=====================================================
\cite{MueDoHer2013}
1st major difference EN/DE Wikipedia:
- automated vandal fighting not permitted in DE Wikipedia (ist es immer noch
  so?)
"algorithmic tools are only intended to support editors in identify-
ing possible malicious edits, but automated assessments are not
accepted"
"Fighting vandals is seen as a form of handling excep-
tions and should therefore be based upon human evaluation."

Hoe? DE Wikipedia has also edit filters which is an automatic assessment of what to disallow.
\cite{WulThaDix2017}

personal attacks

methodology: crowd-sourcing + machine learning
looked at comments on Wikipedia talk pages
100k human labeled comments
63k machine labeled (classifier)
generated data set:
https://figshare.com/Grticles/Wikipedia_Detox_Data/4054689

analysed time period: 2004-2015

"The primary contribution of this paper is a methodology for quan-
titative, large-scale, longitudinal analysis of a large corpus of on-
line comments."

Questions:
"What is the impact of allowing anonymous contributions, namely those from unregistered users?
How do attacks vary with the quantity of a user’s contributions?
Are attacks concentrated among a few users?
When do attacks result in a moderator action?
And is there a pattern to the timing of personal attacks?"

"the majority of personal attacks on Wikipedia are not
the result of a few malicious users, nor primarily the consequence
of allowing anonymous contributions from unregistered users"

"Report highlights that 73% of adult internet users have seen some-
one harassed online, and 40% have personally experienced it [5]."

"Platforms combat this with policies"
"Wikipedia has a policy of “Do not make personal attacks
anywhere in Wikipedia”[33] and notes that attacks may be removed
and the users who wrote them blocked. 1"

classifier: "character-level n-
grams result in an impressively flexible and performant classifier
for a variety of abusive language in English."

"The Wikimedia Foundation found that
54% of those who had experienced online harassment expressed
decreased participation in the project where they experienced the
harassment [23]"

"Through labeling
a random sample, we discovered that the overall prevalence of per-
sonal attacks on Wikipedia talk pages is around 1% (see Section
5.1)."

"Annotators whose accuracy on these test questions fell below a 70%
threshold would be removed from the task."
"This allows us to aggregate judgments
from 10 separate people when constructing a single label for each
comment. We chose 10 judgments based on experiments in Sec. 4.3"

note: the annotators could be biased and their judgement on what is a personal attack may differ from this of the Wikipedian community

"approximately 30% of attacks
come from registered users with over 100 contributions."
"that less than a fifth of personal attacks currently trigger any action
for violating Wikipedia’s policy."
"personal attacks clus-
ter in time - perhaps because one personal attacks triggers another.
If so, early intervention by a moderator could have a disproportion-
ately beneficial impact."

discussion:
"While there are many such questions to analyze,
some notable examples include:
1. What is the impact of personal attacks on a user’s future con-
tributions?
2. What interventions can reduce the level of personal attacks
on a conversation?
3. What are the triggers for personal attacks in Wikipedia com-
ments?"

======================================================
\cite{Geiger2011}

discusses the acceptance, coming into existence and live cycle of bots
begins by a story of elections for the Arbitration Committee (ArbCom), where a bot was nominated, which was taken with different degrees of seriousness (and frustration) by different editors
then discusses at length the discussions around the HagermanBot (signing discussion entries)
and the eventually created opt-out possibilities when it comes to be processed by a bot

touches on philosophical and moral discussions about what is a bot, what rights a bot has and what rights of human editors must be put first

touches on ANT when justifying the need to discuss bots as actors on the same level as humans

"Tawker,
speaking through his bot, was ironically claiming that computerized editors ought to have the
same sociopolitical rights and responsibilities as human editors,"
"I argue (with all seriousness) that these automated software
agents already have a similar level of influence on how Wikipedia as a free and open ency-
clopedia project is constituted."
"we must be careful not to fall into familiar narra-
tives of technological determinism when asking who – or what – actually controls Wikipedia."

cites some stats to underline the (growing) importance of bots
then discusses previous research which seem not to pay the due attention to bots (sometimes with the justification bots are not important enough which lies on old data and sometimes with no justification at all)

"While bots were
originally built to perform repetitive editorial tasks that humans were already doing, they are
growing increasingly sophisticated and have moved into administrative spaces. Bots now po-
lice not only the encyclopedic nature of content contributed to articles, but also the sociality
of users who participate in the community."

"Administrative Intervention against Vandalism, or AIV),
bots make about 50% of all edits, and users with semi-automated editing tools make another
30%"

"My goal in this chapter is to describe the complex social and technical environment in which
bots exist in Wikipedia, emphasizing not only how bots produce order and enforce rules, but
also how humans produce bots and negotiate rules around their operation."

"Operators of bots have also expressed
frustration when their bots become naturalized, that is, when users assume that the bot’s
actions are features of the project’s software instead of work performed by their diligent
computerized workers. In general, bots tend to be taken for granted, and when they are
discussed, they are not largely differentiated from human editors"

"We must pay close attention to both the material and semiotic conditions
in which bots emerge within the complex collective of editors, administrators, committees,
discussions, procedures, policies, and shared understandings that make up the social world
of Wikipedia."

"Bots, like infrastructures in general, 23 simultaneously produce and rely upon
a particular vision of how the world is and ought to be, a regime of delegation that often
sinks into the background – that is, until they do not perform as expected and generate
intense controversies."
//important! signal when discussing filters as well!
//quotes Star 1999 "The Ethnography of Infrastructure"

". This controversy illustrated that a particular kind of normative enforce-
ment and correction, while acceptable when casually performed on a fraction of violations
sometimes days or weeks after, became quite different when universally and immediately
implemented by a bot."

high level issues of rights and responsibilities emerged
"social understanding that ‘bots ought to be better behaved than people’,"

HagermanBot
- explains the origins of the signatures guideline

criticism of the HagermanBot:
- "For these editors, HagermanBot’s message was ‘embarrassing’, as one editor stated, making them ap-
pear as if they had blatantly violated the Signatures guideline"
- "Others did not want bots editing messages other users left for them on their own user talk pages as a matter of principle,"
- "and an equally vocal group did not want the bots adding signatures to their own comments."

Reasons for
"it seemed that Hagerman had a strong set of allies: a growing number of
enthusiastic supporters, the BAG, the Signatures guideline, ideals of openness and transpar-
ency, visions of an ideal discursive space, the {{unsigned}} template, and a belief that signing
unsigned comments was a routine act that had long been performed by humans."

objections
"Yet for some reason, a growing number of editors objected to this typical, uncontroversial practice
when HagermanBot performed it."
"they portrayed as an unfair imposition of what they believed
ought to be optional guidelines. The anti-HagermanBot group was diverse in their stated ra-
tionales and suggested solutions, but all objected to the bot’s operation on some level."

"In the ensuing discussion – which was comprised of BAG members, administrators, and
other Wikipedians – it became clear that this was not simply a debate about signatures and
timestamps."
"full-blown controversy about the morality of delegat-
ing social tasks to technologies,"

Supporters:
"claiming that HagermanBot was only acting in line with a
well-established and agreed-upon understanding that the community had reached regarding
the importance of signatures in discussion spaces. For them, the burden was on the critics
to reach a consensus to amend the Signatures guideline if they wanted to stop the bot from
operating."
"placing strong moral emphasis on the role of signatures
and timestamps in maintaining discursive order and furthering the ideals of openness and
verifiability."

Opposers:
"did not directly contest the
claims made regarding the importance of signatures, discussion pages, and communicative
conventions."
"opposing view of how users, and
especially bot operators, ought to act toward each other in Wikipedia, a view that drew heavily
on notions of mutual respect:"

"botophobia"

end of the discussion: the HagermanBot should allow an opt-out mechanism

"Declarations of either side’s entitlements, largely articulated in the language of positive rights,
were displaced by the notion of responsibility, good behavior, and mutual respect."

"However, a
much stronger ally proved to be the opt-out list through which angry editors could be made to
lose interest in the debate altogether."
"The strength of the opt-out list was its flexibility in rebutting the objections from two kinds of
arguments: first, the largely under-articulated claims that the bot was annoying or trouble-
some to them; and second, the ideological or rights-based arguments that the bot was acting
against fundamental principles of the project’s normative structure."

an extention of the opt-out mechanism to all bots:
"However, seeing a need for extending this functionality to all possible bots, he [Rich Farmbrough]
created a template called {{nobots}}, which was to perform the same function as Hagerman-
Bot’s exclusion tag, except apply to all compliant bots."
"with no actual technical ability to restrict non-compliant bots from editing."

but the BAG can require a bot to comply with {{nobots}} in order to allow its operation

the compliance then found place in standard bot building toolkits

"The case of HagermanBot shows us how a weak but pre-existing social norm was controver-
sially reified into a technological actor."

"In all, bots defy simple single-sided categorizations: they are both editors and software, social
and technical, discursive and material, as well as assembled and autonomous."

============================================
\cite{HalTar2015}

"Today, we’re announcing the release of a new artificial intelligence service designed **to improve the way editors maintain the quality** of Wikipedia" (emphsis mine)
" This service empowers Wikipedia editors by helping them discover damaging edits and can be used to immediately “score” the quality of any Wikipedia article."

"these specs actually work to highlight potentially damaging edits for editors. This allows editors to triage them from the torrent of new edits and review them with increased scrutiny. " (probably triage the edits, not the specs)

"By combining open data and open source machine learning algorithms, our goal is to make quality control in Wikipedia more transparent, auditable, and easy to experiment with."

//so, purpose of ORES is quality control

"Our hope is that ORES will enable critical advancements in how we do quality control—changes that will both make quality control work more efficient and make Wikipedia a more welcoming place for new editors."

"ORES brings automated edit and article quality classification to everyone via a set of open Application Programming Interfaces (APIs). The system works by training models against edit- and article-quality assessments made by Wikipedians and generating automated scores for every single edit and article."

"English Wikipedians have long had automated tools (like Huggle and STiki ) and bots (like ClueBot NG) based on damage-detection AI to reduce their quality control workload.  While these automated tools have been amazingly effective at maintaining the quality of Wikipedia, they have also (inadvertently) exacerbated the difficulties that newcomers experience when learning about how to contribute to Wikipedia. "
"These tools encourage the rejection of all new editors’ changes as though they were made in bad faith," //NB!!!
"Despite evidence on their negative impact on newcomers, Huggle, STiki and ClueBot NG haven’t changed substantially since they were first introduced and no new tools have been introduced. " //what about the edit filters? when were Huggle,STiki and ClueBotNG introduced?

"decoupling the damage prediction from the quality control process employed by Wikipedians, we hope to pave the way for experimentation with new tools and processes that are both efficient and welcoming to new editors. "

caution: biases in AI
" An algorithm that flags edits as subjectively “good” or “bad”, with little room for scrutiny or correction, changes the way those contributions and the people who made them are perceived."

"Examples of ORES usage. WikiProject X’s uses the article quality model (wp10) to help WikiProject maintainers prioritize work (left). Ra·un uses an edit quality model (damaging) to call attention to edits that might be vandalism (right)." //interesting for the memo

"Popular vandal fighting tools, like the aforementioned Huggle, have already adopted our revision scoring service."

further ORES applications:
"  But revision quality scores can be used to do more than just fight vandalism. For example, Snuggle uses edit quality scores to direct good-faith newcomers to appropriate mentoring spaces,[4] and dashboards designed by the Wiki Education Foundation use automatic scoring of edits to surface the most valuable contributions made by students enrolled in the education program"

==========================================================
\cite{Kitchin2017}

importance of studying algorithms
viewpoints/perspectives/scientific traditions from which algorithms can be studied
challenges researchers face when trying to study algorithms
strategies for studying algorithms

"largely black boxed and beyond query or question"

common def of algorithms:
"set of defined steps to produce particular outputs"
"What constitutes an algorithm has changed over time"

different lenses to study them:
"technically, computationally, mathematically, politically, culturally, economically, contex-
tually, materially, philosophically, ethically and so on"

"formulation of an algorithm is, in theory at least, independent of programming languages"

translation challenges of coding
"translating a task or problem into a structured formula with an appropriate rule set (pseudo-code)."
"translating this pseudo-code into source code that when compiled will perform the task"

"The consequences of mistranslating
the problem and/or solution are erroneous outcomes and random uncertainties (Drucker,2013)."

"The processes of translation are often portrayed as technical, benign and commonsensical."

"As Montfort et al. (2012, p. 3) note, ‘[c]ode is not purely abstract and mathemat-
ical; it has significant social, political, and aesthetic dimensions,’"

"Nor can they escape factors such as available
resources and the choice and quality of training data; requirements relating to standards,
protocols and the law; and choices and conditionalities relating to hardware, platforms,
bandwidth and languages"

"algorithms are created for purposes that are often far from neutral"

algorithms change!
"creating an algorithm
unfolds in context through processes such as trial and error, play, collaboration, discussion
and negotiation. They are ontogenetic in nature (always in a state of becoming)"

"always somewhat uncertain, provisional and messy fragile accomplishments"

algorithms are not "stand-alone little boxes", but a socio-technical assemblage:
"complemented by many others, such
as researching the concept, selecting and cleaning data, tuning parameters, selling the idea
and product, building coding teams, raising finance and so on"

"reifying traditional pathologies, rather than reforming them"

not linear/predictable, bc
- part of a wider network
- have side effects
- subverting of computations made public

challenges:
- access/black boxed
  "Coding often happens in private settings, such as within companies"
  "since it is often a company’s algorithms that provide it with a competitive
advantage and they are reluctant to expose their intellectual property even with non-dis-
closure agreements in place."

- heterogeneous and embedded
  "rarely straightforward to deconstruct"
  "algorithms are usually woven together with hundreds of other algorithms"
  "it is unlikely that any one programmer has a complete understanding of a system, especially large, complex ones"

- ontogenetic, performative and contigent (always changing)
  "rarely fixed in form"
  "algorithms and their instantiation in
code are often being refined, reworked, extended and patched, iterating through various
versions"
  "no guarantee that the version a user interacts with at one moment in time is the same
as five seconds later"
  randomness might be built in
  "outcomes are sometimes not easily anticipated"

Approaches to studying algorithms
- Examining pseudo-code/source code
  "carefully deconstruct the pseudo-code and/or source code, teasing apart the
rule set to determine how the algorithm works to translate input to produce an outcome"
  "carefully siftign through documentation, code and programmer comments"
  "map out a genealogy of how an algorithm mutates and
evolves over time as it is tweaked and rewritten across different versions of code."
  "examine how the same task is translated into various software languages and how it
runs across different platforms."

  Limitations:
  not straightforward
  "Even those that have produced it can find it very difficult to unpack its algorithms and routines"
  "it requires that
the researcher is both an expert in the domain to which the algorithm refers and possesses
sufficient skill and knowledge as a programmer that they can make sense of a ‘Big Ball of
Mud’"
  "these approaches largely decontextualise the algorithm from its wider socio-technical assem-
blage and its use."

- Reflexively producing code
  "auto-ethnographies of translating tasks into pseudo-code"
  "researcher reflects on and critically interrogates their own experi-
ences of translating and formulating an algorithm."

  Limitations:
  "difficulties of detaching oneself and gaining critical distance"
  "excludes any non-representational, unconscious acts from analysis."
  "one generally wants to study algorithms and code that have real concrete effects on peoples’ everyday lives,"

- Reverse engineering
  "While software producers might desire their products to remain opaque, each pro-
gramme inherently has two openings that enable lines of enquiry: input and output"
  "carefully selected dummy data and seeing what is outputted under different scenarios"
  "follow debates on online forums by users about how they perceive an algorithm works"

  Limitations:
  "generally cannot do so with any specificity"
  "fuzzy glimpses"
  employ bots to test more systematically, many (proprietary) systems "seek to identify and block bot users."

- Interviewing designers or conducting an ethnography of a coding team
  "uncovering the story behind the production
of an algorithm and to interrogate its purpose and assumptions."
  "respondents are questioned as to how they framed objectives, created
pseudo-code and translated this into code,"
  "researcher seeks to spend time within a coding team,"

  Limitations
  "neither case are the specificities of algorithms and their
work unpacked and detailed."

- Unpacking the full socio-technical assemblage of algorithms
  "form part of a technological stack that includes infrastructure/hardware, code platforms, data and
interfaces, and are framed and conditions by forms of knowledge, legalities, governmen-
talities, institutions, marketplaces, finance and so on."
  "Interviews and ethnographies of coding projects, and the wider institutional apparatus
surrounding them (e.g., management and institutional collaboration)"

  Limitations:
  a lot of work!
  "manageable as a large case study,
especially if undertaken by a research team rather than a single individual."

- Examining how algorithms do work in the world
  "how they are deployed within different domains to perform a multitude of tasks."
  "what an algorithm is designed to do in theory and what it actually does in practice do not always correspond"
  "algorithms perform in context – in collaboration with data, technologies, people, etc. under varying conditions"
  "producing localised and situated outcomes."

===========================================================
\cite{GeiHal2013}

studies (EN) Wikipedia's quality control processes
in particular for 4 periods in 2011 when ClueBot NG (leading bot in vandalism fighting) was down
propose a typology of vandal fighting mechanisms:
* fully automated bots
* automation aided humans ("cyborgs") (e.g. human editors reverting vandalism via Huggle or STiki)
* manually editing humans (using the revert button or actually editing the article in a browser and clicking "save")
* batch scripts run for a particular occasion (e.g. revert all edits of malfunctioning bot or of a malicious editor)

time-to-revert is their primary metric
finds that median times to revert nearly doubled for the periods ClueBot NG was down
(also that Wikipedia editing activity is periodic, with uneven distribution of activity throughout a week, there are daily and weekly cycles)
also finds that more or less same amount of malicious edits were *eventually* reverted when the bot was down, it just took longer
(warns however, that they didn't study these situations further, e.g. by interviewing vandal fighters or similar, so it's unclear whether editors were putting in an extraordinary effort in order to maintain Wikipedia quality in an emergency situation and for how long this is possible)

ClueBot NG:
"to scan every edit made to Wikipedia in real time"
"Built on Bayesian neural networks and trained with data
about what kind of edits Wikipedians regularly revert as
vandalism"

"refining an edit is not considered a revert, and Wikipedians
are discouraged from excessively reverting edits made in
good faith. As such, reverts are the predominant
mechanism used to remove undesirable material, notably
vandalism and spam"

"Huggle, the most widely-used, fully assisted, counter-
vandalism tool, were made within 1 minute of the
offending edit. It is interesting that reverts with STiki, a
newer and more sophisticated queue-based vandal fighting
tool, are more often made to somewhat older edits, with a
time-to-revert distribution that is closer to unassisted edits.
This suggests that Huggle and STiki are targeting different
kinds of edits"

"bots like AWB, DumbBOT, and EmausBot are
less like the fully-automated counter-vandalism bots and
closer to batch scripts that routinely perform cleanup,
administrative, and categorization tasks."

===========================================================
\cite{HalRied2012}

"The first tools to redefine the
way Wikipedia dealt with van-
dalism were AntiVandalBot and
VandalProof."

"AntiVandalBot used a simple set
of rules and heuristics to monitor
changes made to articles, identify the
most obvious cases of vandalism, and
automatically revert them"

1st vandalism fighting bot:
"this bot made it possible, for the first
time, for the Wikipedia community
to protect the encyclopedia from
damage without wasting the time
and energy of good-faith editors"

"it
wasn’t very intelligent and could only
correct the most egregious instances
of vandalism."

"VandalProof, an early cyborg
technology, was a graphical user
interface written in Visual Basic that
let trusted editors monitor article
edits as fast as they happened in
Wikipedia and revert unwanted
contributions in one click."

"Several years and many iterations later [...] these tools have had an
increasing role in maintaining
article quality on Wikipedia."

"ClueBot _ NG has replaced
AntiVandalBot’s simple rules"

"Huggle has replaced VandalProof with
a slick user interface, configurablity,
and an intelligent system for sorting
edits by vandalistic likelihood"

"Huggle, one of the most popular
antivanda lism editing tools on
Wikipedia, is written in C#.NET
and any user can download and
install it. Huggle lets editors roll back
changes with a single mouse click,
but because the tool is so powerful,
rollback permission is restricted to
administrators and a few thousand
other Wikipedia users."
"Huggle makes it easy to review
a series of recent revisions by
filtering them according to the
user’s preferences."

huggle also sends out warnings to the offending editor on revert

"Some Wikipedians feel that such
motivational measures have gone
too far in making Wikipedia like a
game rather than a serious project.
One humorous entry even argues that
Wikipedia has become a MMORPG—
a massively multiplayer online role-
playing game—with “monsters”
(vandals) to slay, “experience”
(edit or revert count) to earn, and
“overlords” (administrators) to submit
to (http://en.wikipedia.org/wiki/
Wikipedia:MMORPG)."

==========================================
\cite{WestKanLee2010}

"STiki is an anti-vandalism tool for Wikipedia. Unlike similar tools, STiki does not rely on natural language
processing (NLP) over the article or diff text to locate vandalism"

"STiki leverages spatio-temporal properties of revision metadata."
"The feasibility of utilizing such properties was demonstrated in our prior
work, which found they perform comparably to NLP-efforts while being more efficient, robust to evasion, and
language independent."

"It consists of, (1) a server-side
processing engine that examines revisions, scoring the likelihood each is vandalism, and, (2) a client-side GUI
that presents likely vandalism to end-users for definitive classiffcation (and if necessary, reversion on
Wikipedia"

==========================================
\cite{GeiRib2010}

revealing invisible infrastructures via trace ethnography
reconstruct the collaboration between bots, editors using semi-automated tools and administrators for banning a vandal

"often-unofficial technologies have fundamentally
transformed the nature of editing and administration in
Wikipedia"
"Of note is the fact that these tools are largely
unofficial and maintained by members of the Wikipedia
community."

"„vandal fighting‟ as an
epistemic process of distributed cognition,"

"From autonomous
software agents and semi-automated programs to user
interface enhancements and visualization tools[...]
Together, they make possible a
kind of epistemological enforcement that often requires little
to no specific knowledge about a given article."

"we claim that in same way that the navigator of
a ship can know trajectories only through the work of dozens
of crew members, so is the blocking of a vandal a cognitive
process made possible by a complex network of interactions
between humans, encyclopedia articles, software systems, and
databases."

Partial explanation why literature paid little attention to (semi-)automated tools up to this date:
- old data according to which bots accounted for a very little amount of edits (2-4%)
  ("that this number has grown
dramatically: at present, bots make 16.33% of all edits.")
- "largely involved in single-use tasks like importing public domain material" (so not the case anymore, check e.g. MusikBot)
- "characterized in the literature as mere force-multipliers,
increasing the speed with which editors perform their work
while generally leaving untouched the nature of the tasks
themselves"

BotDef
"Bots – short for „robots‟ – are fully-automated software
agents that perform algorithmically-defined tasks involved
with editing, maintenance, and administration in Wikipedia."

"At present, some of the most
active bots are those that review every edit made in real time,
using sophisticated heuristics to revert blatant incidents of
spam and vandalism."

Check Figure 1: Edits to AIV by tool (in the meantime 10 years old. is there newer data on the topic??)

huggle description
"edits are contextually
presented in queues as they are made, and the user can
perform a variety of actions (including revert and warn) with
a single click. The software‟s built-in queuing mechanism,
which by default ranks edits according to a set of vandalism-
identification algorithms,"

"Users of Huggle‟s automatic
ranking mechanisms do not have to decide for themselves
which edit they will view next"

huggle's ranking heuristics:
"in the default „filtered‟ queue, edits that contain a significant removal of content are placed
higher; those that completely replace a page with blank text
are even marked in the queue with a red „X‟."
"anonymous users are viewed as more suspicious than
registered users, and edits by bots and Huggle users are not
even viewed at all."
"Users whose edits have been previously
reverted by a number of assisted users are viewed as even
more suspicious, and those who have been left warnings on
their user talk page (a process explained below) are
systematically sent to the top of the queue."

"This edit was placed into the queues of many
Huggle users, as the software prioritizes mass removal of
content by anonymous users who have vandalism warnings
left for them. In fact, a green “1” appeared next to the
article‟s name in the edit queue, indicating that a first-level
warning had been issued."

"In reporting the anonymous user to
AIV, the Huggle program collected three edits which had been
marked as vandalism in the previously-issued warnings."

"The Huggle software took note of the
fact that a report existed for this user at AIV, and asked the
administrator if he wished to issue a temporary block."

"Yet with four warnings and an active report at AIV, there was
nothing else Huggle could do in the name of this non-
administrator except append this incident of vandalism to his
original report, further attempting to enroll a willing
administrator into the ad-hoc vandal fighting network."

"“HBC AIV helperbot7” – automatically
removed the third vandal fighter‟s now-obsolete report."

Standard procedure for blocking:
"Generally, administrators will not temporarily
block users from editing if they have not received four
warnings."

"The work performed by many distinct vandal
fighters can be collated and then compressed into a single
number, visible to a wide array of human and non-human
actors."

Twinkle description:
"user interface extension that runs inside
of a standard web browser. Twinkle adds contextual links to
pages in Wikipedia allowing editors to perform complex tasks
with the click of a button – such as rolling back multiple edits
by a single user, reporting a problematic user to
administrators, nominating an article for deletion, and
temporarily blocking a user (for administrators only)."

Lupin's anti-vandal tool
"provides a real-
time in-browser feed of edits made matching certain
algorithms"

"user‟s talk page, which was more of database for other
vandal fighters than a space for dialogue with the anonymous
editor."

"While each editor made local
judgments as to the veracity or appropriateness of specific
contextualized edits, they collectively came to identify users
who were problematic and thus deserving of a temporary ban."

!! tools not only speed up the process but:
"These tools greatly lower certain barriers to participation and render editing
activity into work that can be performed by „average
volunteers‟ who may have little to no knowledge of the
content of the article at hand"

"Such a reviewing process is in
stark contrast to the more traditional forms of professional
and academic knowledge production"

"The domain expertise of vandal fighters is in the use of the
assisted editing tools themselves, and the kinds of
commonsensical judgment those tools enable."

Importance of diffs
"the edits in question
were rendered visibly suspicious because they were displayed
in such a manner."
"removal of entire
sections is a common form of vandalism that is difficult to
detect by merely reading the article."

"The Huggle program‟s queuing mechanism is another way in
which edits are further transformed, contextualized, and
abstracted"

"one does not need to have the
technical, literary, or academic skills or motivations to author
an article in order to patrol it."

"other users do not
have to trawl through the user‟s recent contributions: unassisted
vandal fighters can visit the user talk page to see previous
warnings, and assisted users simply have the software
automatically incorporate this information into its decision-
making process."

critical discussion
"Such acts of inclusion and exclusion may be necessary, but
they are inherently moral in quality, speaking to questions of
who is left out and what knowledge is erased."

"It is for
this reason that the argument that bots and assisted editing
tools are merely force multipliers is narrow and dangerous"

"In and outside of the Wikipedian community, tools
like Huggle are often compared with video games in both
serious critiques and humorous commentaries:"

"We should not fall into the trap of speaking of bots and
assisted editing tools as constraining the moral agency of
editors"

"these tools makes certain pathways of action easier for vandal
fighters and others harder"

"Similarly, users can
reconfigure their queues to not view anonymous edits as more
suspicious,"

"While these and many other workarounds are possible,
they require a greater effort and a certain technical savvy on
the part of their users."

"Ultimately, these tools take their users
through standardized scripts of action in which it always
possible to act otherwise, but such deviations demand
inventiveness and time."

======================================================================
\cite{Geiger2017}

situated in critical algorithmic studies, critical data studies,
discusses issues in fairness, accountability and transparency

(algorithmic) governance
gatekeeping

"the organizational culture of Wikipedia is deeply intertwined with various data-driven algorithmic
systems, which Wikipedians rely on to help manage and govern the ‘‘anyone can edit’’ encyclopedia at a massive
scale."
"These bots, scripts, tools, plugins, and dashboards make Wikipedia more efficient for those who know how
to work with them, but like all organizational culture, newcomers must learn them if they want to fully participate."

Beschreibt die Prozesse für 2 verschiedene, oft vorkommende Edit-Abläufe, mit den Bots und automatischen Tools, die diese unterstützen
* editing (your own) page on Wikipedia: Conflict of interest requests --> newcommers werden hier abgeschreckt
* Speedy Deletion

"But I realized that the more interesting question is why I had so internalized this socio-techni-
cal assemblage and the values it enacts." // People internalise the way a system works and stop questioning it!!

"They are deeply imbued with particular values, prin-
ciples, norms, and ideals, and learning them is not
just about technical competency, but also socialization
into a complex organizational culture"

Some descriptive statistics:
"In the English-language Wikipedia, 22 of the 25 most active editors (by
number of edits) are bot accounts, and July 2017, they made about 20% of all edits to encyclopedia articles."
--> vgl: https://quarry.wmflabs.org/query/20703

What does it take to be a Wikipedian:
"when that partici-
pation requires not only learning Wikipedia-specific
jargon, norms, style guides, and rules, but also learning
how to interact with all the bots and power tools"

"Wikipedia demonstrates how the issues in and around
algorithmic systems are as much social as they are tech-
nical, going far beyond the opacities that arise around
proprietary source code. My argument extends Burrell’s
(2016) discussion of three different forms of opacity in
machine learning: intentional secrecy (proprietary
source code), technical literacy (such as learning to
read code), and opacities inherent in machine learning
(such as issues of interpretability). To these forms, I add
another: the opacities in learning a particular institu-
tional or organizational culture that is supported by
algorithmic systems."
// source is open, but who can actually read it? and is willing to invest the time and energy in order to hold the system accountable?
// vgl auch Gedanke von Claudia: "Wikipedia is spannend, weil wir daran das erforschen können, was wir an Facebook nicht können. Und weil die ein Abbild der Gesellschaft im Kleinen ist."
// vgl auch Web Science def: observe micro behaviours in order to study macro phenomenons (governance, ..)

Def "algorithmic": "as involving encoded proced-
ures, which are typically—but not exclusively—compu-
tationally implemented."

"Like all algorithmic systems, the ones I studied in
Wikipedia were designed, developed, and deployed by
people." //all developers are human and all humans make mistakes^^

"As Gillespie (2014) argues: ‘‘A socio-
logical analysis must not conceive of algorithms as
abstract, technical achievements, but must unpack the
warm human and institutional choices that lie behind
these cold mechanisms.’’"

Seaver (2013: 9–10) Def Algorithmic System:
"It is not the algorithm, narrowly defined, that has
sociocultural effects, but algorithmic systems — intri-
cate, dynamic arrangements of people and code. . .
When we realize that we are not talking about algo-
rithms in the technical sense, but rather algorithmic
systems of which code strictu sensu is only a part,
their defining features reverse: instead of formality,
rigidity, and consistency, we find flux, revisability,
and negotiation."
"In this context, I ask: for whom are algorithmic systems
(and the organizations that rely on them) formal, rigid,
and consistent, and for whom are they in flux, revisable,
and negotiable?" //Vorwissen, das die Menschen mitbringen, ist wichtig!

2nd level digital divide:
"who had
the knowledge, skills, and sense of empowerment to use
the Internet in ways that further engaged, empowered,
and benefitted their lives." nach Hargittai 2002

Everything's open:
"We must look at more than the fact that partici-
pation in Wikipedia is open to the public; that the infra-
structure supporting it is open sourced; and that the
community’s policies, procedures, and norms are docu-
mented in thousands and thousands of pages of text."

BUT
"We must also look at what kind of skills, knowledge, and
investment is required to fully and successfully partici-
pate,"

speaks of deletion of substandard encyclopedic articles: Where and how are the standards defined? Who defines them?

"article about a
website fails the A7 criteria in the CSD process,
which demands that articles ‘‘credibly indicate the
importance or significance of the subject.’’ (A majority
of speedy deleted articles are tagged with templates
containing A7 rationales (Geiger and Ford, 2011).)"
// ich finde dieses Kriterium ist äußerst mit Vorsicht zu genießen, da die Tür weit aufgemacht wird für Sexismus, Rassismus und andere Arten von Diskriminierung von Inhalten, die der Mehrheit von white dudes nicht passen

"Yet I had a
second, subtler motivation, hoping that in properly
demonstrating correct usage of such a template within
the established workflow of this process, I would be
made legible as a Wikipedian who knew the CSD pro-
cess and should be given some more leeway—unlike
most of the people who were creating articles that
they were deleting." //making use of the brocode^^

"Such systems do not eliminate the need for human
labor, but instead transform the kind of work that
takes place,"

erwähnt auch Abschrecken von newcommers

"As
Seaver (2013) notes with his critiques of various ‘‘crit-
ical algorithms studies’’ literature, it is easy to slip into
a mode of analysis where social factors are contextua-
lized, while infrastructure remain static and determin-
ing. Such an approach ‘‘keeps algorithms themselves
untouched, objective stones tossed about in a roily
social stream’’ (10)."

"the algorithmic
systems themselves are constructed, negotiated, contex-
tualized, and differently interpreted and enacted." // aber wer kann beim Aushandeln mitmachen?

"Wikipedia’s computational infrastructure is
also designed and governed in a relatively open
manner by the project’s volunteer community of edi-
tors (Forte and Bruckman , 2008; Gilbert and Zachry,
2015; Kennedy, 2010), unlike most of the automated
systems that are increasingly prevalent in digitally
mediated environments." //jup. und selbst da blick man nicht durch

Requirements/Expectations for bot developers:
"bot developers are generally expected to be responsive
to reasonable requests and concerns from the
community."

"Wikipedians discuss and debate
about what kinds of bots should exist in the project,"

"one of the
paradoxes of openness is that it can take substantial
time, energy, investment, and resources to fully take
advantage of all the materials released"

Veterans vs newcomers:
"they make it far easier for veteran Wikipedians to
engage in the kind of specific, complex, multifacted
work involved in the governance of Wikipedia. This
can make it far more difficult for newcomers to partici-
pate—not necessarily because bots, algorithms, or
assisted tools are inherently difficult to deal with, but
rather because bots support more complex kinds of gov-
ernance practices in Wikipedia, and complex govern-
ance practices are difficult for newcomers."

==========================================================
\cite{AstHal2018}

introduce algorithmically aided method for quality control/review of newly created articles, for which a bottleneck seems to exist

compare quality control/revision mechanisms for
newly created articles with <-- huge backlog, overwhelmed editors
new edits (on existing articles) <-- seem to have a more elaborate funnel workflow which filters out more blatant ofensive edits among the way and make work for human editors easier

propose an algorithm based on topic models computed from WikiProjects categories (compare linear SVM classification, Random Forrests and Gradient Boosting Classifiers)
(discarded the idea of using the category system of Wikipedia, because it was lacking proper hierarchy between categories)
"WikiProjects are subject-focused working groups – the kind of subject-interested working groups
we wanted to target with our topic modeling. These working groups tag articles that fall within
the scope of their projects."
"The WikiProjects Directory 13 provides a convenient intermediate ontology of WikiProjects that
starts with four broad topics: Culture, Geography, History & Society and Science, Technology and
Mathematics (STEM) (Fig. 2). From there, the directory drills down into sub-topics and eventually
specific WikiProjects."
"We utilized the well defined hierarchy of the directory pages to group WikiProjects under 43
different mid level categories which formed our prediction space."

aim: predict topic of a new article in order to give it to a patroller with expertise on the topic
"Our work is focused on making it possible to implement
AI-augmented multi-stage review processes for new article review that lessens the burden of TCJCs
on current patrollers by routing new pages to subject matter experts."

"Wikipedia flips the traditional publication review process–publish first and ask questions later." xD

they have a diagram on what happens with new edits (Fig.1). However, filters are missing altogether.

Argument for machine support in creating topic labels:
"rather than having the authors tag the topics because 1)
newcomers would be inconsistent with tagging and their tags would likely not match Wikipedia’s
existing ontology, 2) we have a huge backlog of untagged drafts that needs addressal, 3) with a
model, changes to the tagging ontology can be automatically propagated to all drafts."

"the Scoring Platform team at the Wikimedia Foundation
released "draftquality" 8 model that is designed to detect "vandalism", "spam", and "personal attack"
articles for quick deletion. Predictions of all of these models can be accessed by the ORES[8]
webservice that the Wikimedia Foundation provides via public apis."

"There’s no such alternative that exists for new article creations since an editor can’t "watch" an
article before it is created. Therefore, we seek to find a signal for the topic space of new article
creations such that interested editors can watch new page creations too"

warn against possible biases, since WikiProjects are assigned manually by editors.
Also point out that article's belonging to categories is not exhaustive and oftentimes the proposed method finds topics an article logically belongs to but which was never assigned.

"This observation about missing labels suggests that the fitness
measurements are generally conservative. It also suggests that there is a surprising utility to our
topic model: It can be used to predict missing topics for existing full-fledged articles!"

====================================================
\cite{Livingstone2016}

Background on bots, bot policies, governance, etc
IV with Ram-Man

https://stats.wikimedia.org/EN/BotActivityMatrixEdits.htm
// stats about bot edits; not clear for me which period is covered

Bot Def
"bots are processes that run in the background, are normally
invisible, react to their environment, and most importantly, run autonomously. Leonard (1997) claims autonomy
is the “crucial variable”"

Concerns with the rambot (and bots in general):
"1. It affected performance. // wikipedia resided on one server at the time; at the end an edit rate limit of 1 edit/sec was established
2. The percentage of articles got skewed. This directly affected “Random Page.”
3. It cluttered up the “Recent Changes.”
4. It generated lots of discussion on the merits of the articles themselves and the various camps all came
out to play (Deletionists, Inclusionists, etc.).
5. People started to be concerned about how a bot could be used to “mess up” the project." //"botophobia"

Ram-Man authored the original bot policy (so that he can be "left alone")

"In 2006, the Bots Approval Group (BAG) was formed on the English Wikipedia to review Bot Request for
Approvals (BRFA) (Figure 2). Consisting of experienced bot operators, the group would both review the
soundness of a bot request and determine if there was community consensus for the task. By 2007, the BAG
was facing accusations of being a technical cabal on the site, making decisions on bots without fully gauging
community consensus, and adding unnecessary bureaucracy and process to the site (Wikipedia, 2016e)."
"BAG stood by the fact that the BRFA process is always open to the broader community, but few outside
contributors regularly participated in the process."

One of the reasons for bot policy:
Bug in the MediaWiki software that was counting every bot edit as a new article, which distorted the stats and public and private perception of Wikipedia

bot flag was developed in order to be able to filter out bot edits from the recent changes logs (were flooded) so that editors can watch for vandalism

Bot policy was in place before BAG was first created

"What is a bot?
In the Wikimedia software, there are tasks that do all sorts of things, like count pages, count orphan pages, etc.
If these things are not in the software, an external bot could do them. It is just a small step to editing pages. The
main difference is where it runs and who runs it."

problem with BAG: not enough people participate
"the management of bots was a largely ignored and thankless job."
a (probably) wrong perception by the community how the BAG worked, so it was criticised and challenged for being a cabal

====================================================================
\cite{Lessig2006}

"A locked door is not a command “do not enter”
backed up with the threat of punishment by the state. A locked door is a
physical constraint on the liberty of someone to enter some space.
My claim is that this form of regulation will become increasingly com-
mon in cyberspace."(p.82)

"Code is a regulator in cyberspace because it defines the terms upon which cyberspace is offered." (p.84)

"Communities,
exchange, and conversation all flourish in a certain type of space; they are
extinguished in a different type of space." (p.85)
"Spaces have values. 6 They manifest these values through the practices or lives
that they enable or disable."(p.85)

narrow bandwidth and text-centered communication
"Most think of this fact about the early Net as a limitation. Technically, it
was. But this technical description does not exhaust its normative description
as an architecture that made possible a certain kind of life. From this perspec-
tive, limitations can be features; they can enable as well as disable. And this
particular limitation enabled classes of people who were disabled in real-
space life." (p.86)

"Every time AOL decides that it
wants to regulate a certain kind of behavior, it must select from among at least
four modalities—rules, norms, prices, or architecture." (p.94)

"Norms become different when ballots can overrule them," (p.102)

// TODO vgl Wikipedia!
"politics [is] implemented through technology.” 41 Differ-
ences in the code constitute them differently, but some code makes community
thicker than others. Where community is thick, norms can regulate."(p.102)

"End-to-end is a para-
digm for technology that embeds values. Which architecture we encourage is
a choice about which policy we encourage." (p.112)
// TODO: What values are embedded in Wikipedia's architecture? In the architecture of the edit filter system?

"In places where community is not fully self-enforcing, norms are supple-
mented by rules imposed either through code or by the relevant sovereign." (p.113)

"They have the right to
exit, but in the sense that Soviet citizens had the right to exit—namely, with
none of the assets they had built in their particular world." (p.113)

quoting Second Life CEO's
"[O]ur feeling is . . . that we should aggressively move into code anything we can,
because of the enhanced scalability it gives us. And we should execute policy out-
side of code only when absolutely necessary or unfeasible." (p.114)
"If Second Life can use code to better control behavior, what about first-life?" (p.114)

"Individuals can act
to resist the force of code directly. Or individuals can act to resist the force of
code through code." (p.118)

quoting Tim Wu
"The reason that code matters for law at all is its capability to define behavior on
a mass scale." (p.119)
"In this second sense, code functions “as an anti-regulatory mechanism: a
tool to minimize the costs of law that certain groups will use to their advan-
tage.” " (p.119)

Chapter 7: things are regulated by; laws, norms, market, technology

"Norms constrain through the
stigma that a community imposes; markets constrain through the price that
they exact; architectures constrain through the physical burdens they impose;
and law constrains through the punishment it threatens." (p.124)

Indirection and accountability
"Indirection misdirects responsibility. When a government uses other
structures of constraint to effect a constraint it could impose directly, it mud-
dies the responsibility for that constraint and so undermines political
accountability." (p.133)

Chapter 8

"The key criticism that I’ve identified so far is transparency. Code-based
regulation—especially of people who are not themselves technically
expert—risks making regulation invisible." (p.138)

"Open code means open control—there is control, but the user is
aware of it."(p.151)

Chapter 9

"It is reading the amendment differ-
ently to accommodate the changes in protection that have resulted from
changes in technology. It is translation to preserve meaning." (p.163)

"When we know what values we
want to preserve, we need only be creative about how to preserve them." (p.165)

"My fear about cyberspace is that we will respond in the first way—that the
courts, the institutions most responsible for articulating constitutional values,
will stand back while issues of constitutional import are legislatively deter-
mined. My sense is that they will step back because they feel (as the balance of
this book argues) that these are new questions that cyberspace has raised.
Their newness will make them feel political, and when a question feels polit-
ical, courts step away from resolving it." (p.167)

Chapter 11

"technology enables perpetual and cheap monitoring of behavior" (p.200)

Chapter 12

"In the
United States at least, there are few places where you can stand before the
public and address them about some matter of public import without most
people thinking you a nut or a nuisance. There is no speakers’ corner in every
city; most towns have no town meeting. “America offline,” in this sense, is
very much like America Online—not designed to give individuals access to a
wide audience to address public matters." (p.235)
"Only professionals get to address
Americans on public issues—politicians, scholars, celebrities, journalists, and
activists, most of whom are confined to single issues. The rest of us have a
choice—listen, or be dispatched to the gulag of social lunacy."(p.235)

"With the relative anonymity of cyberspace and its growing size, norms
do not function well there. Even in cyberspaces where people know each other
well, they are likely to be more tolerant of dissident views when they know (or
believe, or hope) the dissident lives thousands of miles away."(p.236)

"The architecture of
cyberspace is the real protector of speech there;"(p.236)

"My aim is to obsess
about the relationship between architecture and the freedom it makes possi-
ble, and about the significance of law in the construction of that architecture."(p.237)

"I say “politics” because this building is not over. As I have argued (over
and over again), there is no single architecture for cyberspace; there is no
given or necessary structure to its design."

"There is no reason to think, in other words, that
this initial flash of freedom will not be short-lived. And there is certainly no
justification for acting as if it will not."
"Already the Net is changing from free to controlled space."

"The architecture of the Internet, as it is right now,
is perhaps the most important model of free speech since the founding."
"If we take this meaning seriously, then the First
Amendment will require a fairly radical restructuring of the architectures of
speech off the Net as well. "

"Publishing requires a publisher, and a publisher
can be punished by the state. But if the essence or facts of the publication are
published elsewhere first, then the need for constitutional protection disap-
pears. Once the piece is published, there is no further legal justification for
suppressing it."(p.240)

Wikipedia
"This collaboration comes with no guarantees, except the guarantee of a
process. The most extraordinary collaborative process in the context of con-
tent is Wikipedia. Wikipedia is a free online encyclopedia, created solely by
volunteers. Launched at the beginning of 2001, these (literally thousands of)
volunteers have now created over 2 million articles. There are nine major lan-
guage versions (not including the Klingon version), with about half of the
total articles in English.
The aim of the Wikipedia is neutrality. The contributors edit, and reedit,
to frame a piece neutrally. Sometimes that effort fails—particularly controver-
sial topics can’t help but attract fierce conflict. But in the main, the work is an
unbelievable success. With nothing more than the effort of volunteers, the
most used, and perhaps the most useful encyclopedia ever written has been
created through millions of uncoordinated instances of collaboration.
Wikipedia, however, can’t guarantee its results. It can’t guarantee that, at
any particular moment, there won’t be errors in its entries. But of course, no
one can make that guarantee. Indeed, in one study that randomly collected
entries from Wikipedia and from Encyclopedia Britannica, there were just as
many errors in Britannica as in Wikipedia. 28
But Wikipedia is open to a certain kind of risk that Britannica is not—
maliciousness. In May 2005, the entry to an article about John Seigenthaler Sr.
was defaced by a prankster. Because not many people were monitoring the
entry, it took four months before the error was noticed and corrected. Seigen-
thaler wasn’t happy about this. He, understandably, complained that it was the
architecture of Wikipedia that was to blame.
Wikipedia’s architecture could be different. But the lesson here is not its
failures. It is instead the extraordinary surprise of Wikipedia’s success. There
is an unprecedented collaboration of people from around the world work-
ing to converge upon truth across a wide range of topics. That, in a sense, is
what science does as well. It uses a different kind of “peer review” to police
its results. That “peer review” is no guarantee either—South Koreans, for
example, were quite convinced that one of their leading scientists, Hwang
Woo-Suk, had discovered a technique to clone human stem cells. They
believed it because peer-reviewed journals had reported it. But whether
right to believe it or not, the journals were wrong. Woo-Suk was a fraud, and
he hadn’t cloned stem cells, or anything else worth the attention of the
world." (243-244)

"if we rely upon pri-
vate action alone, more speech will be blocked than if the government acted
wisely and efficiently." (p.255)

"The private filters the market has produced so far are both expensive and
over-inclusive. They block content that is beyond the state’s interest in regu-
lating speech. They are effectively subsidized because there is no less restrictive
alternative."(p.256)

"If that filtering
were in private software, there would be no opportunity to fight it through
legal means."(p.256)

"The filtering regime would establish an architecture that could
be used to filter any kind of speech"(p.258)

"In real space we do not have to worry about this problem too much
because filtering is usually imperfect.[...] All sorts of issues I’d rather not think about
force themselves on me. They demand my attention in real space, regardless
of my filtering choices." (p.259)
"We must confront the problems of others and
think about issues that affect our society. This exposure makes us better citi-
zens. 52 We can better deliberate and vote on issues that affect others if we
have some sense of the problems they face"(p.259)

citing Ithiel de Sola Pool:
"The cohesion and effective functioning of a democratic
society depends upon some sort of public agora in which everyone participates
and where all deal with a common agenda of problems, however much they
may argue over the solutions."(p.260)

"Less law does not necessarily mean more freedom.”"(p.268)

"Copyright law regulates, at a
minimum, “copies.” Digital networks function by making “copies”"(p.268)

"A significant
portion of creative activity has now moved from a free culture to a permission
culture."(p.269)

"Among those are at least the requirements that copyright not regulate
“ideas,” and that copyright be subject to “fair use.”
But these “traditional First Amendment safeguards” were developed in a
context in which copyright was the exception, not the rule."(p.269)

"How much control should we allow over
information, and by whom should this control be exercised?"(p.276)

Chapter 14

"Sometimes, in other words, better code can weaken community." (p.284)

"And with the exception of Wikipedia, and “A Tale in the Desert,” there
are very few major Internet or cyberspace institutions that run by the rule of
the people." (p.285)

merchant-sovereignty
"The power you have over these institutions is your ability to exit." (p.287)
"What makes them work well
is this competition among these potential sources for your custom."(p.287)

"There are no states that get to say to
their citizens: “You have no right to vote here; if you don’t like it, leave.”"

citizen sovereignties: churches, universities, social clubs, where we are members and not just mere consumers

"We are remaking the values of the
Net, and the question is: Can we commit ourselves to neutrality in this recon-
struction of the architecture of the Net?" (p.292)
"I don’t think that we can. Or should. Or will" (p.293)

"cyberspace has no intrinsic nature." (p.317)
"They were making, not finding, the nature of cyberspace; their
decisions are in part responsible for what cyberspace will become."
"over time courts will see that there is little in cyberspace that is “natural.”"
"What was “impossible” will later become possible"

In a nutshell:
"Cyberspace, however, has different architectures, whose regulatory power
are not so limited. An extraordinary amount of control can be built into the
environment that people know there. What data can be collected, what
anonymity is possible, what access is granted, what speech will be heard—all
these are choices, not “facts.” All these are designed, not found." (p.318)

"pathetic resignation that most of us feel about the
products of ordinary government." (p.322)
"We have lost the idea that ordinary government might work, and so deep is
this despair that not even government thinks the government should have a
role in governing cyberspace."
"One central cause of the dysfunction of government is the corruption
suggested by the way government is elected"

"To raise this money, members of Congress must spend their time making
those with money happy. They do this by listening to their problems, and
sometimes, pushing legislation that will solve those problems."
"This is not capitalism as much as lobby-ism." (p.323)

"the failure is not even to try." (p.324)
"Courts are disabled, legisla-
tures pathetic, and code untouchable. That is our present condition. It is a
combination that is deadly for action"

"It is an important check on government power to say that the only
rules it should impose are those that would be obeyed if imposed transparently." (p.328)
"What a code regulation does
should be at least as apparent as what a legal regulation does. Open code
would provide that transparency—not for everyone (not everyone reads
code), and not perfectly (badly written code hides its functions well), but
more completely than closed code would."

"Some closed code could provide this transparency.[...] Componentized architecture could be as transparent as an
open code architecture, and transparency could thus be achieved without
opening the code." //I'm not quite sure I agree

"The best code (from the perspective of constitutional values) is both
modular and open."

"We are all still democrats; we simply do not like what our
democracy has produced. And we cannot imagine extending what we have to
new domains like cyberspace" (p.330)

"The cost of “piracy” is significantly less
than the cost of spam. Indeed, the total cost of spam—adding consumers to
corporations—exceeds the total annual revenues of the recording industry."(p.337)

"But we can’t translate skepticism into disengagement" (p.338)

"once instituted, architectural constraints have their effect until
someone stops them." (p.343)

"Architecture and the market constrain up front; law and norms let you play
first." (p.343)

"For those who are fully mature, or fully integrated, all objective con-
straints are subjectively effective prior to their actions. They feel the con-
straints of real-space code, of law, of norms, and of the market before they act.
For the completely immature, or totally alienated, few objective constraints
are subjectively effective." (p.344)

"Law and norms are more efficient the
more subjective they are, but they need some minimal subjectivity to be effec-
tive at all. The person constrained must know of the constraint. A law that
secretly punishes people for offenses they do not know exist would not be
effective in regulating the behavior it punishes."

"Architectural constraints, then, work whether or not the subject knows
they are working, while law and norms work only if the subject knows some-
thing about them." (p.345)

======================================================================
\cite{Geiger2014}

bespoke = custom taylored

bespoke code - runs alongside the "official system", on separate technical infrastructure
bots on Wikipedia are an example for bespoke code, since they are not part of the core MediaWiki software, often run on private desktops of their developers and can disappear at any moment
Wikipedia is often black-boxed, i.e. people from outside don't see the tons of bespoke code that make Wikipedia what it is and oftentimes bots performing vital tasks (for example in vandal fighting) are taken for granted (for their developers' aggravation)

"Most people think that to understand law, you need to understand a set of rules. That’s a mistake ...
The law is best understood through stories – stories that teach what is later summarized in a catalog of
rules. (Lawrence Lessig, Code and Other Laws of Cyberspace)"

Def:
"runs alongside a platform or system, rather than being integrated into server-side codebases by
individuals with privileged access to the server. Bespoke code complicates the common
metaphors of platforms and sovereignty that we typically use to discuss the governance and
regulation of software systems through code."
"must be continuously operated on computers that are independent from the servers
hosting the site, they involve alternative relations of power and code"
"I define the term bespoke code as a software code that runs alongside a platform or system, in
contrast to code that is integrated into server-side codebases and runs on the same servers that
host the platform or system."

(
fun fact: vgl Aaron Swartz: volunteers also run the servers, but that's not really the case anymore;
http://www.aaronsw.com/weblog/whorunswikipedia
Wikimedia Foundation was founded 2003
there was one server till 2004 (https://en.wikipedia.org/wiki/Wikimedia_Foundation)

)

"focus on the materiality of code"
"examine not only the software code itself, but also the concrete, historically
contingent material conditions under which this code is run"

Wikipedia has custom extentions that are not part of "stock" MediaWiki

"By my estimate, the code that runs alongside the official platform rather than
being directly integrated into MediaWiki is easily an order of magnitude larger than the
∼600,000 lines of code that comprise MediaWiki. This code – some of which fundamentally
changes how the wiki operates as a wiki – takes many forms, including PHP extensions, template
scripts, user scripts, standalone tools, browser extensions, and fully automated bots. This code is
written in a multitude of programming languages, coded in a variety of environments, and is often
executed on computers that are relatively independent from those run by the Wikimedia Foun-
dation to host Wikipedia."

"Bots aren’t usually part of some master plan – if they were, they probably wouldn’t be bots."
"And when you first see a bot, you may not be sure how to react, or even what it is you’re
looking at"

"together, bots and software platforms like MediaWiki are more than the sum of their parts."

"Many of the commonplace socio-technical practices that Wikipedians have developed to help
them collaboratively build an encyclopaedia – even the code that supports leaving the famous
[citation needed] tag in articles – are not supported in the stock version of the MediaWiki plat-
form. The [citation needed] tag is supported by a relatively simple template script, of which
there are over one hundred thousand in the English-language version alone (Lanzara & Patriotta,
2007)"

Examples of bespoke code on wikipedia:
bots
templates
semi-automated tools

Different bots' roles:
"help create new articles, edit existing articles, enforce rules and standards, patrol for
spam and vandalism, and generally work to support encyclopaedic or administrative work"

"Because bots are built to algorithmically enact a particular vision of what
encyclopaedia articles or wiki-based collaboration is and ought to be, they have profound influ-
ence on what Wikipedia is and how it operates, both an encyclopaedia and a community."

"bots make it possible to achieve a certain level of uniformity in style and
content" (vgl aaron swartz comment on content and format http://www.aaronsw.com/weblog/whowriteswikipedia)

"Bots also serve key gov-
ernance roles, independently enforcing discursive and epistemological norms, particularly for
newcomers"

"bots should not just be seen as force multipliers that merely operate on content"
"Bots also dramatically extend the functionality of MediaWiki as a platform"

"many bureaucratic and formalized
procedures in place that make up ‘the hidden order of Wikipedia’ (Wattenberg, Viegas, &
McKeon, 2007). Much of this work is supported by code not built into the MediaWiki platform,
notably template scripts, tools, and bots."

"In some cases, these
bots subvert the fundamental idea that wiki pages are documents that users edit, using these
flat text files as semi-structured databases. For example, bots implement queuing mechanisms
for processing and distributing administrative requests, such as requests to block a particular
user for vandalism."

AfDStatBot: Geiger's first bot
"created for my quantitative research purposes,
to archive and capture statistics on these deletion discussions in real time"
"It generated a near real-time notice-
board of active and recently closed deletion discussions, with statistics such as how long the dis-
cussion had been open, how many Wikipedians were in favor of keeping vs. deleting the article,
when the last comment was made, and so on. Then for the few who opted in, it curated person-
alized watchlists"

ontological argument, materiality of software, bots are more than code:
"software cannot be reduced to code and divorced
from the conditions under which it is developed and deployed. It is a materialist argument, oppos-
ing the ‘trope of immateriality’ (Blanchette, 2011, p. 3), a discourse that alternatively celebrates or
laments the disembodied nature of information technology."

"Hayles in arguing that this trope in which digital information is proclaimed to be ‘free
from the material constraints that govern the material world’ (Hayles, 1999, p. 13) is not just a
casual metaphor of technologists but a fundamental assumption in contemporary post-human
society."

"The ways in which the artificial agent
appears to the user are just as important and essential as the code behind it."
//for instance, does the user get it, it's an artificial agent at all?

"scholars have interrogated
the material infrastructures and artefacts that are taken for granted in a variety of social insti-
tutions: prisons (Foucault, 1977), museums and art galleries (Becker, 1982; Star & Griesemer,
1989), hospitals (Garfinkel, 1967), scientific research (Latour & Woolgar, 1986; Shapin & Schaf-
fer, 1985), public infrastructure (Winner, 1986), economic markets (Mackenzie, 2006), and
organizations and firms (Orlikowski & Scott, 2008),"

"Abstract, high-level, see-
mingly immaterial entities like art, culture, discipline, science, truth, value, power, or profit all
rely on materially existing infrastructures, artefacts, people, and practices – often operating
behind the scenes"
//vgl Code is Law: regulating behaviour via architecture

"‘Platforms’ obscure bespoke code"
"instead run by developers on servers that are often independent of those used to run the website."

"Bots problematize what I call the discourse of platform sovereignty, in which platforms are cast as
spatially bounded territories, code is cast as having the force of law over such territories, devel-
opers are cast as sovereigns who govern territories with code-as-law, and users are cast as subjects" //vgl also Lessig's accounts of AOL, Second Life, etc.

"have implications for how we understand the nature of authority. The discourses of ‘platforms’
work to obscure and consolidate a variety of assumptions, practices, and ideologies (Gillespie,
2010), particularly the material conditions necessary for the code to operate as a kind of
architecture."

"At the heart of this assumption of sovereignty is a fetishization of Lessig’s famous ‘code is
law’ (Lessig, 1999) slogan, one that sees code as abstract rules that autonomously govern and
regulate according to their own immaterial logic. This abstraction of code into law separates
code from the material conditions under which it was not only produced, but also put into
force;"
// is that so? I read this as, among other things, material conditions/physical constraints being "law"

", thinking of platforms as governed by code-as-law can obscure the con-
ditions under which that code-as-law comes to be run at all"

"bot operators in Wikipedia occasionally lament how their bots become invis-
ible and taken for granted, as users see them as always existing inherent features that were written
once and forever built into MediaWiki, rather than code that must be constantly run on their own
servers."
//bots taken for granted --> rendered invisible

"obscure the specific work that is done to make the platform
appear to be a stable, coherent infrastructure – and the more successful infrastructures are, the
more invisible they become (Star, 1999)"

critique of the term "platform"
"‘platform’ is increasingly deployed in ways that silently consoli-
date a number of otherwise disparate technical and political concepts, implicitly erasing the ten-
sions"
"the term ‘platform’ had political, archi-
tectural, and figural implications long before it was a computational term."

"critiques of ‘digital dualism’"
"discourses that draw sharp boundaries between the
‘online’ and the ‘offline’ world – particularly those that deploy a moralistic argument claiming
what occurs ‘in real life’ is inherently more social, substantive, significant, and healthy than
what occurs in ‘the virtual world’"

networks vs bounded spatiality

"the idea that Wikipedia only takes place on wiki-
pedia.org – or even entirely on the Internet – is a huge misunderstanding (Konieczny, 2009;
Reagle, 2010). Wikipedia is not a virtual world, especially one located entirely on the wiki."
e.g. in order to get hold of abuse_filter_history I had to engage with
- wikipedia.org
- mediawiki.org
- irc channels
- phabricator
- gerrit
- toolserver/cloudservices
----
other spaces Wikipedia takes place
- mailinglists
- WomenEdit/offenes Editieren @Wikimedia
- Wikimania
- Wikimedia's office and daily work

"In Wikipedia, what comes to look like a unified
platform that structures and governs interaction is actually the product of a wide variety of
code running on a multitude of different servers. This has specific consequences for those who
are concerned with how these systems operate, particularly because as part of this assemblage,
bespoke code can break or disappear in ways that platform code typically cannot."

"when the volunteer-run servers hosting the counter-
vandalism bot ClueBot NG – which in January 2011 was singlehandedly responsible for
making 13.7% of all reverts (or rejections of a change to an article) – inexplicably went down
for days at a time in Spring 2011." //see also Aaron Swartz's blog (Who runs Wikipedia)

"The discourse of platform sovereignty arises at the intersection of two pervasive metaphors:
sovereignty derives from the familiar ‘code as law’ metaphor from Lessig, while the concept
of platforms is based on spatial metaphors of ‘cyberspace’, in which interaction is seen as
taking place in a territory."

"Early writings about governance and the Internet explicitly made such compari-
sons, most notably Barlow’s ‘Declaration of the Independence of Cyberspace’ (Barlow, 1996)."

" history of spatial metaphors"
"casting the Internet as an unregulated territorial space
emerged alongside discourse that framed the Internet as a problem of governance for states."

"Lessig’s famous ‘code as law’ (Lessig, 1999) slogan often undergirds contemporary dis-
courses of platform sovereignty, although Lessig has a much more materialist foundation to his
analysis of code" //yup, I think so as well

". Lessig’s writ-
ings in Code implicitly cast cyberspace as governed territory, but Code 2.0 (Lessig, 2006) not only
defends but also embraces these territorial metaphors as productive ways of talking about the
implications of where code is run from."

", Lessig’s argument and his use of discourse of platform sover-
eignty is actually a materialist one at its most fundamental level, as it is based on the implications
that arise from the historically contingent fact that websites and other Internet-mediated platforms
are regulated by code that runs on servers owned and operated by specific individuals or firms."

"For Lessig, code has the power to regulate because the people who own the servers have
sovereign control over what code is run, "

"What if, from the beginning, I had decided to run my bot on the toolserver, a
shared server funded and maintained by a group of German Wikipedians for all kinds of pur-
poses, including bots? If so, the bot may have run the same code in the same way, producing
the same effects in Wikipedia, but it would have been a different thing entirely."
"when life got in the way, it was something I literally pulled the plug on
without so much as a second thought."

"Taking the platform for granted submerses all the work performed by these
bespoke codes into the infrastructure, making Wikipedia seem like the kind of community in
which everything spontaneously self-organizes out with little to no rules or regulatory structures."

============================================================================
\cite{HalGeiTer2014}

"In 2005, Wikipedia’s volunteer editor community and the size
of the encyclopedia began growing exponentially[35]. Dur-
ing this time Wikipedia faced a series of crises in the public
sphere over its trustworthiness and legitimacy. Wikipedia’s
“vandal fighters” came to see Wikipedia as a firehose of ed-
its needing constant surveillance. By 2007, they had devel-
oped quality control practices around a suite of standards, dis-
courses, procedures, and roles. To make their work practical,
they formalized the practice of reviewing edits around a suite
of algorithmically-assisted, semi-automated tools[15]."

"Tools like Huggle raise practical design challenges and eth-
ical issues for HCI researchers. In previous work, we have
critiqued the “professional vision”[17] they enact and the as-
sumptions and values they embody: most tools situate users
as police, not mentors, affording rejection and punishment."

=======================================================================
\cite{KitSuhPenChi2007}

"First we demonstrate that at the global level, conflict and
coordination costs in Wikipedia are growing. Specifically,
direct work (on articles) is decreasing, while indirect work
such as discussion, procedure, user coordination, and
maintenance activity (such as reverts and anti-vandalism) is
increasing."

"As social collaborative knowledge systems grow, so do
opportunities for conflict and coordination costs. In the first
part of this article we demonstrate a way to quantify these
costs at the global level that provides insights into how
growth in Wikipedia is occurring. We show that, even
though Wikipedia continues to grow exponentially, the rate
of creation of new articles and content is decreasing, while
levels of maintenance and indirect work are increasing.
These results provide the first comprehensive view of this
phenomenon, and reflect the entire history of all Wikipedia
articles rather than a small sampling of pages."
"These data are consistent with the findings from studies of
group work systems which suggest that, to keep functioning,
a group must engage in both task-focused and group
Continued growth in
maintenance activity [4][10].
Wikipedia is not maintained merely by an increase in articles
and quality content; the sophisticated procedures developed
for coordinating users and dealing with conflict are vital for a
community where people may not agree on everything."

====================================================================
\cite{GeiHalPinWal2012}

- paper describes defense mechanisms on wikipedia
- goes over socialisation tactics for newcomers in different types of organisations
- underlines, Wikipedia is a special case, bc of being a volunteers driven project, with little formal mentoring programms, etc

and describes an experiment in which wording of templated messaged for engaging new users/discouraging them from doing unproductive edits is altered in order to find optimal one which retains new collaborators
results are inconclusive, but point towards personal message without directives seems to incent the most user talk, but maybe that's due simply to the fact that there are no other actionable links in this template.)
also, the majority of new users studied here were unregistered

"During its period of
exponential growth, the Wikipedian community developed
specialized socio technical defense mechanisms to protect
itself from the negatives of massive participation: spam,
vandalism, falsehoods, and other damage."
// check following paper \cite{HalGeiMorRied2012} for period of exponential growth
// compare with timeline to confirm

"2006-07 -- the period in which the
community experienced exponential growth in terms of
both users and content"

"first illustrate and describe the various defense mechanisms"
"Next, we present re
sults from an experiment aimed at increasing both the quan
tity and quality of editors by altering various elements of
these defense mechanisms,"

"New mem-
bers are thrown into the project and left to fend for them-
selves, often learning the various rules of the project by
breaking them."

"socialization of new members increasing-
ly takes place through the project's highly-automated de-
fense mechanisms, with new members' first interaction
with another Wikipedian taking the form of having one's
contributions reverted and being sent a warning message."

"Wikipedia faces a unique chal-
lenge in that it has an astounding lack of formal gatekeep-
ing mechanisms."
"almost any Internet user
has the technical ability to edit almost every encyclopedia
article in whatever manner they see fit"
"This model is a
foundational principle of the Wikipedian community, and
both communal wisdom and academic research holds that
these lower barriers to entry do make the encyclopedia
vulnerable to error and vandalism, but dramatically in-
crease rates of participation (Wilkinson, 2008)."

no ownership of content

literature shows
- damage is, in general, reverted very quickly
- describes Wikipedia as an informal per review system
- fully or semi-automated mechanisms are used for quality control

"44%, almost half of all
activity on User talk: pages, originates from highly-
automated human and bot users – and many but not all of
whom are engaged in counter-vandalism activity."

"Task directives are highly prevalent in
such messages, instructing new editors to do a number of
tasks except editing encyclopedia articles again"

===================================================
\cite{PotSteGer2008}

discusses vandalism from information retreival perspective
one-class classification problem (similar to spam filters)
distinguishes 3 types of misuse of the the encyclopedia by users: lobbyism, spam and vandalism

"We present results of a new approach to detect destructive article revisions, so-called vandalism, in Wikipedia. Vandalism detection is a one-class classification problem, where vandalism edits are the target to be identified among all revisions. Interestingly, vandalism detection has not been addressed in the Information Retrieval literature by now."

"We compiled a large number of vandalism edits in a corpus, which allows for the comparison of existing and new detection approaches."

"Compared to the rule-based methods that are currently applied in Wikipedia, our approach increases the F -Measure performance by 49% while being faster at the same time"

"We distinguish them into three groups: (i) lobbyists, who try to push their own agenda, (ii) spammers, who solicit products or services, and (iii) vandals, who deliberately destroy the work of others. "

===================================================
\cite{HalGeiMorRied2013}

"recent research has shown that the number of active
contributors in Wikipedia has been declining steadily for years, and suggests that a sharp
decline in the retention of newcomers is the cause"

"a massive growth in participation have ironically crippled the very growth they were designed
to manage"

"community’s formal mechanisms for norm
articulation are shown to have calcified against changes – especially changes proposed by
newer editors."

"open collaboration systems need to maintain an inner circle of highly
invested contributors to manage and direct the group. However, with statistical predictability, all
contributors to such systems will eventually stop contributing"

"The success of an open collaboration project appears to be highly correlated with the number of
participants it maintains"

"Some newcomers must move from the periphery of
the community to the center"

"The community grew from
hundreds of active editors in 2001 to thousands in 2004 and peaked in March of 2007 at 56,400
active editors."

"growth as a self-
reinforcing mechanism: as Wikipedia became more valuable, the project attracted more
contributors to increase its value."

figure 1: The English Wikipedia’s editor decline. The number of active,
registered editors (>= 5 edits/month) is plotted over time. //consider plotting such a thing myself (and get data from quarry?) if deemed relevant
TODO: check https://meta.wikimedia.org/wiki/Wikilytics if I need editors stats

Lit overview: explanations for the decline:
1) increasing completion of the articles
2) failed socialisation of newcomers
3) "right-sizing": main work is done, no need for so many editors
refutes against 1) and 3): vast majority of articles are below the community's standards for "good" articles

"underrepresented
groups still find it challenging to join. For instance, one study found that only 9% of edits are
made by female editors, and that articles of particular interest to women are shorter than articles
of interest to men (Lam, 2011). Until editors are representative of the population of potential
contributors, it is difficult to argue that the socialization practices are sufficiently effective"

Def desirable newcomer: "trying to contribute productively (i.e. acting
in good-faith) and, therefore, likely will become valuable contributors if they remain in the
community."
"the proportion of desirable newcomers who arrive
at Wikipedia has been holding steady in recent years, a decreasing fraction of these newcomers
survive past their initial contributions."

community's goals changed during the period of exponential growth

"resulted in a new Wikipedia, in which newcomers are rudely greeted by
automated quality control systems and are overwhelmed by the complexity of the rule system."

contributions of the paper:
"1) we implicate Wikipedia’s primary quality control mechanism (Stvilia, 2005), the rejection of
unwanted contributions, as a strong, negative predictor of the retention of high quality
newcomers and show that these newcomers’ contributions are being rejected at an increasing
rate
2) we show how algorithmic tools, which were built to make the work of controlling the
quality of Wikipedia’s content more efficient, exacerbate the effect of rejection on desirable
newcomer retention and circumvent Wikipedia’s conflict resolution process.
3) we show how calcification has made Wikipedia’s policy environment less adaptable and increased the
difficulty of contributing to community rules – especially for newcomers."

"Wikipedia’s open contribution system constitutes an informal
peer review where all contributions are initially accepted;"

"the definition of “unwanted” contribution has certainly
changed over time. While presenting at Wikimania in 2006, Jimmy Wales urged Wikipedians to
change their focus from quantity to quality."
"shift from Wikipedia as
a catch-all for encyclopedia-like content to a more restrictive project."

"external pressures for Wikipedia to tighten its review process. After high profile
cases of libel"

"the length of the article at the time of contribution was a significant predictor of
rejection."

"Hypothesis: Rejection & retention. Increasing rates of rejection have caused a decrease in
the retention of desirable newcomers."

"Hypothesis: Tool use & consequences. The use of algorithmic tools to reject newcomer
contributions is exacerbating the decrease in desirable newcomer retention."

"Hypothesis: Norm formalization & calcification: Formalization of norms has made it more
difficult for newer generations of editors to shape the official rules of Wikipedia."

"In 2006, Wikipedia administrator Tawker initiated a new
genre: the vandal fighter bot." //TODO: Would be interesting for timeline; however I cant find which bot it was
"using a simple text pattern matcher."
"After years of iteration, vandal fighter bots are in wide use
in mid-2012. ClueBot NG uses machine learning and neural network approaches to identify and
reject over 40,000 acts of vandalism a month, with a median time to revert of five seconds."
//TODO median time can be used in the funnel diagram

"Human-computation tools [..] catch the damage the bots miss"
"re-introduce human judgment into the vandal fighting task."

"These algorithmic tools have apparently made quality control both more efficient and more
effective. Previous work has shown that the duration during which vandalism is visible in an
article has been decreasing (Kittur, 2007; Priedhorsky, 2007). These tools also reduce the
amount of volunteer effort that must be devoted to rejecting unwanted contributions"
//argument in favour of not only a difference of scale, but also of substance

"As the editor community grew implicit norms were formalized into a
growing corpus of official rules and procedures (Butler 2008), and rule creation and enforcement
became increasingly decentralized"
"Formally documenting
community practices facilitated wider dissemination in the expanding community"
"new rules were created to meet emergent needs."

". By 2005, three primary types of documented norms had
emerged: policies, guidelines and essays. Formal norms (policies and guidelines) reflect
community consensus, and can be enforced. Informal norms (essays) are not enforceable rules
per se and need not reflect consensus, but do often reflect community concerns"

"formalization of implicit norms into rules, and the embedding of these rules in technologies
such as bots and templates," //code is law

"gradual decline in
participation by newer editors in the areas of Wikipedia dedicated to drafting and discussing
policy, indicating that senior Wikipedians may now be more responsible for curating and
interpreting community policy than ever before."

"decline-era newcomers may face entrenched social practices and
technologically-embedded processes that are no longer open to re-negotiation"
"policy calcification and increasing centralization of policy"

"This means that rejection was likely to be a demotivator
to newcomers who joined the project long before retention of newcomers became an issue."
"We also found that over the lifetime of Wikipedia the probability that contributions made by
desirable newcomers are rejected has increased."
"these rejections were due to misunderstandings about the norms of the community"
"This result suggests that “unwanted” but
not intentionally damaging contributions may have been handled differently in the past."

"One such way of dealing with imperfect contributions without sacrificing quality is to “massage”
them into a form that is valuable for an article. Perhaps the increasing use of tools that afford
only two possible reactions – accept or reject – are making it more likely that contributions are
rejected outright."

"For editors who revert manually, the rate of reciprocation has
dropped slightly, from a peak of 67% in 2005 to 56% in 2010. The overall rate of reciprocation
has dropped dramatically, since none of the major bots are programmed to reciprocate BRD
initiations." //reciprocation = answer to posts on their talk pages initiated by reverted editors

"Figure 6 suggests that a large number of newcomers (2,250 BRD initializations from
918 unique registered editors) are attempting to enter into dialog with an algorithmic editor after
being reverted by them" //people not aware they are trying to talk to a bot?

"ditors who
revert using Huggle have an average response rate of 7%, compared to editors who use the
browser-based extension Twinkle, which has an average response rate of 53% – only slightly
lower than editors who revert manually."

"2006 saw an even higher number of proposed policies, but lower acceptance
with 24 out of 348 proposals accepted (7% acceptance). From 2007 forward, the rate at which
policies are proposed decreases monotonically down to a mere 16 in 2011 while the acceptance
rate stays steady at about 7.5%"

"We find an increase in essay creation that corresponds to the decline in policy creation"
"in 2006, 22% of new essays began as failed policy proposals.
However, the percentage of essays that started out as rejected policies or guidelines decreases
sharply to 12% in 2007 and 1% by 2011."

"over time, contributions to all types are more
likely to be rejected independent of the tenure of the editor making the contribution."

"rules are less open to revision by affected editors than they were during the growth period"
"increasing the power imbalance between newer and older editors."
"essays are not official, enforceable rules and are not widely cited. While
an increase in essay writing is an encouraging sign of newer editors’ continued interest in
participating in community governance, it is not an effective mechanism for social change."
"informal norms documented in essays are trumped by
formal norms embedded in bots and human computation tools."

"Wikipedia has changed from “the encyclopedia that anyone can edit” to “the encyclopedia that
anyone who understands the norms, socializes him or herself, dodges the impersonal wall of
semi-automated rejection and still wants to voluntarily contribute his or her time and energy can
edit”"
