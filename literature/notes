\cite{MuellerBirn2014}

Interesting info, not sure what to do with it

2010: 40% of newcommers' contributions rejected based on automatic tools <---
counter productive in engaging new contributors

=====================================================
\cite{GeiHal2017}

Claudia's paper:
"“In both cases of algorithmic governance
– software features and bots – making rules part of the infrastructure, to a certain extent, makes
them harder to change and easier to enforce” (p. 87)"

=====================================================
\cite{MueDoHer2013}
1st major difference EN/DE Wikipedia:
- automated vandal fighting not permitted in DE Wikipedia (ist es immer noch
  so?)
"algorithmic tools are only intended to support editors in identify-
ing possible malicious edits, but automated assessments are not
accepted"
"Fighting vandals is seen as a form of handling excep-
tions and should therefore be based upon human evaluation."

Hoe? DE Wikipedia has also edit filters which is an automatic assessment of what to disallow.
\cite{WulThaDix2017}

personal attacks

methodology: crowd-sourcing + machine learning
looked at comments on Wikipedia talk pages
100k human labeled comments
63k machine labeled (classifier)
generated data set:
https://figshare.com/Grticles/Wikipedia_Detox_Data/4054689

analysed time period: 2004-2015

"The primary contribution of this paper is a methodology for quan-
titative, large-scale, longitudinal analysis of a large corpus of on-
line comments."

Questions:
"What is the impact of allowing anonymous contributions, namely those from unregistered users?
How do attacks vary with the quantity of a user’s contributions?
Are attacks concentrated among a few users?
When do attacks result in a moderator action?
And is there a pattern to the timing of personal attacks?"

"the majority of personal attacks on Wikipedia are not
the result of a few malicious users, nor primarily the consequence
of allowing anonymous contributions from unregistered users"

"Report highlights that 73% of adult internet users have seen some-
one harassed online, and 40% have personally experienced it [5]."

"Platforms combat this with policies"
"Wikipedia has a policy of “Do not make personal attacks
anywhere in Wikipedia”[33] and notes that attacks may be removed
and the users who wrote them blocked. 1"

classifier: "character-level n-
grams result in an impressively flexible and performant classifier
for a variety of abusive language in English."

"The Wikimedia Foundation found that
54% of those who had experienced online harassment expressed
decreased participation in the project where they experienced the
harassment [23]"

"Through labeling
a random sample, we discovered that the overall prevalence of per-
sonal attacks on Wikipedia talk pages is around 1% (see Section
5.1)."

"Annotators whose accuracy on these test questions fell below a 70%
threshold would be removed from the task."
"This allows us to aggregate judgments
from 10 separate people when constructing a single label for each
comment. We chose 10 judgments based on experiments in Sec. 4.3"

note: the annotators could be biased and their judgement on what is a personal attack may differ from this of the Wikipedian community

"approximately 30% of attacks
come from registered users with over 100 contributions."
"that less than a fifth of personal attacks currently trigger any action
for violating Wikipedia’s policy."
"personal attacks clus-
ter in time - perhaps because one personal attacks triggers another.
If so, early intervention by a moderator could have a disproportion-
ately beneficial impact."

discussion:
"While there are many such questions to analyze,
some notable examples include:
1. What is the impact of personal attacks on a user’s future con-
tributions?
2. What interventions can reduce the level of personal attacks
on a conversation?
3. What are the triggers for personal attacks in Wikipedia com-
ments?"

======================================================
\cite{Geiger2011}

discusses the acceptance, coming into existence and live cycle of bots
begins by a story of elections for the Arbitration Committee (ArbCom), where a bot was nominated, which was taken with different degrees of seriousness (and frustration) by different editors
then discusses at length the discussions around the HagermanBot (signing discussion entries)
and the eventually created opt-out possibilities when it comes to be processed by a bot

touches on philosophical and moral discussions about what is a bot, what rights a bot has and what rights of human editors must be put first

touches on ANT when justifying the need to discuss bots as actors on the same level as humans

"Tawker,
speaking through his bot, was ironically claiming that computerized editors ought to have the
same sociopolitical rights and responsibilities as human editors,"
"I argue (with all seriousness) that these automated software
agents already have a similar level of influence on how Wikipedia as a free and open ency-
clopedia project is constituted."
"we must be careful not to fall into familiar narra-
tives of technological determinism when asking who – or what – actually controls Wikipedia."

cites some stats to underline the (growing) importance of bots
then discusses previous research which seem not to pay the due attention to bots (sometimes with the justification bots are not important enough which lies on old data and sometimes with no justification at all)

"While bots were
originally built to perform repetitive editorial tasks that humans were already doing, they are
growing increasingly sophisticated and have moved into administrative spaces. Bots now po-
lice not only the encyclopedic nature of content contributed to articles, but also the sociality
of users who participate in the community."

"Administrative Intervention against Vandalism, or AIV),
bots make about 50% of all edits, and users with semi-automated editing tools make another
30%"

"My goal in this chapter is to describe the complex social and technical environment in which
bots exist in Wikipedia, emphasizing not only how bots produce order and enforce rules, but
also how humans produce bots and negotiate rules around their operation."

"Operators of bots have also expressed
frustration when their bots become naturalized, that is, when users assume that the bot’s
actions are features of the project’s software instead of work performed by their diligent
computerized workers. In general, bots tend to be taken for granted, and when they are
discussed, they are not largely differentiated from human editors"

"We must pay close attention to both the material and semiotic conditions
in which bots emerge within the complex collective of editors, administrators, committees,
discussions, procedures, policies, and shared understandings that make up the social world
of Wikipedia."

"Bots, like infrastructures in general, 23 simultaneously produce and rely upon
a particular vision of how the world is and ought to be, a regime of delegation that often
sinks into the background – that is, until they do not perform as expected and generate
intense controversies."
//important! signal when discussing filters as well!
//quotes Star 1999 "The Ethnography of Infrastructure"

". This controversy illustrated that a particular kind of normative enforce-
ment and correction, while acceptable when casually performed on a fraction of violations
sometimes days or weeks after, became quite different when universally and immediately
implemented by a bot."

high level issues of rights and responsibilities emerged
"social understanding that ‘bots ought to be better behaved than people’,"

HagermanBot
- explains the origins of the signatures guideline

criticism of the HagermanBot:
- "For these editors, HagermanBot’s message was ‘embarrassing’, as one editor stated, making them ap-
pear as if they had blatantly violated the Signatures guideline"
- "Others did not want bots editing messages other users left for them on their own user talk pages as a matter of principle,"
- "and an equally vocal group did not want the bots adding signatures to their own comments."

Reasons for
"it seemed that Hagerman had a strong set of allies: a growing number of
enthusiastic supporters, the BAG, the Signatures guideline, ideals of openness and transpar-
ency, visions of an ideal discursive space, the {{unsigned}} template, and a belief that signing
unsigned comments was a routine act that had long been performed by humans."

objections
"Yet for some reason, a growing number of editors objected to this typical, uncontroversial practice
when HagermanBot performed it."
"they portrayed as an unfair imposition of what they believed
ought to be optional guidelines. The anti-HagermanBot group was diverse in their stated ra-
tionales and suggested solutions, but all objected to the bot’s operation on some level."

"In the ensuing discussion – which was comprised of BAG members, administrators, and
other Wikipedians – it became clear that this was not simply a debate about signatures and
timestamps."
"full-blown controversy about the morality of delegat-
ing social tasks to technologies,"

Supporters:
"claiming that HagermanBot was only acting in line with a
well-established and agreed-upon understanding that the community had reached regarding
the importance of signatures in discussion spaces. For them, the burden was on the critics
to reach a consensus to amend the Signatures guideline if they wanted to stop the bot from
operating."
"placing strong moral emphasis on the role of signatures
and timestamps in maintaining discursive order and furthering the ideals of openness and
verifiability."

Opposers:
"did not directly contest the
claims made regarding the importance of signatures, discussion pages, and communicative
conventions."
"opposing view of how users, and
especially bot operators, ought to act toward each other in Wikipedia, a view that drew heavily
on notions of mutual respect:"

"botophobia"

end of the discussion: the HagermanBot should allow an opt-out mechanism

"Declarations of either side’s entitlements, largely articulated in the language of positive rights,
were displaced by the notion of responsibility, good behavior, and mutual respect."

"However, a
much stronger ally proved to be the opt-out list through which angry editors could be made to
lose interest in the debate altogether."
"The strength of the opt-out list was its flexibility in rebutting the objections from two kinds of
arguments: first, the largely under-articulated claims that the bot was annoying or trouble-
some to them; and second, the ideological or rights-based arguments that the bot was acting
against fundamental principles of the project’s normative structure."

an extention of the opt-out mechanism to all bots:
"However, seeing a need for extending this functionality to all possible bots, he [Rich Farmbrough]
created a template called {{nobots}}, which was to perform the same function as Hagerman-
Bot’s exclusion tag, except apply to all compliant bots."
"with no actual technical ability to restrict non-compliant bots from editing."

but the BAG can require a bot to comply with {{nobots}} in order to allow its operation

the compliance then found place in standard bot building toolkits

"The case of HagermanBot shows us how a weak but pre-existing social norm was controver-
sially reified into a technological actor."

"In all, bots defy simple single-sided categorizations: they are both editors and software, social
and technical, discursive and material, as well as assembled and autonomous."

============================================
\cite{HalTar2015}

"Today, we’re announcing the release of a new artificial intelligence service designed **to improve the way editors maintain the quality** of Wikipedia" (emphsis mine)
" This service empowers Wikipedia editors by helping them discover damaging edits and can be used to immediately “score” the quality of any Wikipedia article."

"these specs actually work to highlight potentially damaging edits for editors. This allows editors to triage them from the torrent of new edits and review them with increased scrutiny. " (probably triage the edits, not the specs)

"By combining open data and open source machine learning algorithms, our goal is to make quality control in Wikipedia more transparent, auditable, and easy to experiment with."

//so, purpose of ORES is quality control

"Our hope is that ORES will enable critical advancements in how we do quality control—changes that will both make quality control work more efficient and make Wikipedia a more welcoming place for new editors."

"ORES brings automated edit and article quality classification to everyone via a set of open Application Programming Interfaces (APIs). The system works by training models against edit- and article-quality assessments made by Wikipedians and generating automated scores for every single edit and article."

"English Wikipedians have long had automated tools (like Huggle and STiki ) and bots (like ClueBot NG) based on damage-detection AI to reduce their quality control workload.  While these automated tools have been amazingly effective at maintaining the quality of Wikipedia, they have also (inadvertently) exacerbated the difficulties that newcomers experience when learning about how to contribute to Wikipedia. "
"These tools encourage the rejection of all new editors’ changes as though they were made in bad faith," //NB!!!
"Despite evidence on their negative impact on newcomers, Huggle, STiki and ClueBot NG haven’t changed substantially since they were first introduced and no new tools have been introduced. " //what about the edit filters? when were Huggle,STiki and ClueBotNG introduced?

"decoupling the damage prediction from the quality control process employed by Wikipedians, we hope to pave the way for experimentation with new tools and processes that are both efficient and welcoming to new editors. "

caution: biases in AI
" An algorithm that flags edits as subjectively “good” or “bad”, with little room for scrutiny or correction, changes the way those contributions and the people who made them are perceived."

"Examples of ORES usage. WikiProject X’s uses the article quality model (wp10) to help WikiProject maintainers prioritize work (left). Ra·un uses an edit quality model (damaging) to call attention to edits that might be vandalism (right)." //interesting for the memo

"Popular vandal fighting tools, like the aforementioned Huggle, have already adopted our revision scoring service."

further ORES applications:
"  But revision quality scores can be used to do more than just fight vandalism. For example, Snuggle uses edit quality scores to direct good-faith newcomers to appropriate mentoring spaces,[4] and dashboards designed by the Wiki Education Foundation use automatic scoring of edits to surface the most valuable contributions made by students enrolled in the education program"

==========================================================
\cite{Kitchin2017}

importance of studying algorithms
viewpoints/perspectives/scientific traditions from which algorithms can be studied
challenges researchers face when trying to study algorithms
strategies for studying algorithms

"largely black boxed and beyond query or question"

common def of algorithms:
"set of defined steps to produce particular outputs"
"What constitutes an algorithm has changed over time"

different lenses to study them:
"technically, computationally, mathematically, politically, culturally, economically, contex-
tually, materially, philosophically, ethically and so on"

"formulation of an algorithm is, in theory at least, independent of programming languages"

translation challenges of coding
"translating a task or problem into a structured formula with an appropriate rule set (pseudo-code)."
"translating this pseudo-code into source code that when compiled will perform the task"

"The consequences of mistranslating
the problem and/or solution are erroneous outcomes and random uncertainties (Drucker,2013)."

"The processes of translation are often portrayed as technical, benign and commonsensical."

"As Montfort et al. (2012, p. 3) note, ‘[c]ode is not purely abstract and mathemat-
ical; it has significant social, political, and aesthetic dimensions,’"

"Nor can they escape factors such as available
resources and the choice and quality of training data; requirements relating to standards,
protocols and the law; and choices and conditionalities relating to hardware, platforms,
bandwidth and languages"

"algorithms are created for purposes that are often far from neutral"

algorithms change!
"creating an algorithm
unfolds in context through processes such as trial and error, play, collaboration, discussion
and negotiation. They are ontogenetic in nature (always in a state of becoming)"

"always somewhat uncertain, provisional and messy fragile accomplishments"

algorithms are not "stand-alone little boxes", but a socio-technical assemblage:
"complemented by many others, such
as researching the concept, selecting and cleaning data, tuning parameters, selling the idea
and product, building coding teams, raising finance and so on"

"reifying traditional pathologies, rather than reforming them"

not linear/predictable, bc
- part of a wider network
- have side effects
- subverting of computations made public

challenges:
- access/black boxed
  "Coding often happens in private settings, such as within companies"
  "since it is often a company’s algorithms that provide it with a competitive
advantage and they are reluctant to expose their intellectual property even with non-dis-
closure agreements in place."

- heterogeneous and embedded
  "rarely straightforward to deconstruct"
  "algorithms are usually woven together with hundreds of other algorithms"
  "it is unlikely that any one programmer has a complete understanding of a system, especially large, complex ones"

- ontogenetic, performative and contigent (always changing)
  "rarely fixed in form"
  "algorithms and their instantiation in
code are often being refined, reworked, extended and patched, iterating through various
versions"
  "no guarantee that the version a user interacts with at one moment in time is the same
as five seconds later"
  randomness might be built in
  "outcomes are sometimes not easily anticipated"

Approaches to studying algorithms
- Examining pseudo-code/source code
  "carefully deconstruct the pseudo-code and/or source code, teasing apart the
rule set to determine how the algorithm works to translate input to produce an outcome"
  "carefully siftign through documentation, code and programmer comments"
  "map out a genealogy of how an algorithm mutates and
evolves over time as it is tweaked and rewritten across different versions of code."
  "examine how the same task is translated into various software languages and how it
runs across different platforms."

  Limitations:
  not straightforward
  "Even those that have produced it can find it very difficult to unpack its algorithms and routines"
  "it requires that
the researcher is both an expert in the domain to which the algorithm refers and possesses
sufficient skill and knowledge as a programmer that they can make sense of a ‘Big Ball of
Mud’"
  "these approaches largely decontextualise the algorithm from its wider socio-technical assem-
blage and its use."

- Reflexively producing code
  "auto-ethnographies of translating tasks into pseudo-code"
  "researcher reflects on and critically interrogates their own experi-
ences of translating and formulating an algorithm."

  Limitations:
  "difficulties of detaching oneself and gaining critical distance"
  "excludes any non-representational, unconscious acts from analysis."
  "one generally wants to study algorithms and code that have real concrete effects on peoples’ everyday lives,"

- Reverse engineering
  "While software producers might desire their products to remain opaque, each pro-
gramme inherently has two openings that enable lines of enquiry: input and output"
  "carefully selected dummy data and seeing what is outputted under different scenarios"
  "follow debates on online forums by users about how they perceive an algorithm works"

  Limitations:
  "generally cannot do so with any specificity"
  "fuzzy glimpses"
  employ bots to test more systematically, many (proprietary) systems "seek to identify and block bot users."

- Interviewing designers or conducting an ethnography of a coding team
  "uncovering the story behind the production
of an algorithm and to interrogate its purpose and assumptions."
  "respondents are questioned as to how they framed objectives, created
pseudo-code and translated this into code,"
  "researcher seeks to spend time within a coding team,"

  Limitations
  "neither case are the specificities of algorithms and their
work unpacked and detailed."

- Unpacking the full socio-technical assemblage of algorithms
  "form part of a technological stack that includes infrastructure/hardware, code platforms, data and
interfaces, and are framed and conditions by forms of knowledge, legalities, governmen-
talities, institutions, marketplaces, finance and so on."
  "Interviews and ethnographies of coding projects, and the wider institutional apparatus
surrounding them (e.g., management and institutional collaboration)"

  Limitations:
  a lot of work!
  "manageable as a large case study,
especially if undertaken by a research team rather than a single individual."

- Examining how algorithms do work in the world
  "how they are deployed within different domains to perform a multitude of tasks."
  "what an algorithm is designed to do in theory and what it actually does in practice do not always correspond"
  "algorithms perform in context – in collaboration with data, technologies, people, etc. under varying conditions"
  "producing localised and situated outcomes."

===========================================================
\cite{GeiHal2013}

studies (EN) Wikipedia's quality control processes
in particular for 4 periods in 2011 when ClueBot NG (leading bot in vandalism fighting) was down
propose a typology of vandal fighting mechanisms:
* fully automated bots
* automation aided humans ("cyborgs") (e.g. human editors reverting vandalism via Huggle or STiki)
* manually editing humans (using the revert button or actually editing the article in a browser and clicking "save")
* batch scripts run for a particular occasion (e.g. revert all edits of malfunctioning bot or of a malicious editor)

time-to-revert is their primary metric
finds that median times to revert nearly doubled for the periods ClueBot NG was down
(also that Wikipedia editing activity is periodic, with uneven distribution of activity throughout a week, there are daily and weekly cycles)
also finds that more or less same amount of malicious edits were *eventually* reverted when the bot was down, it just took longer
(warns however, that they didn't study these situations further, e.g. by interviewing vandal fighters or similar, so it's unclear whether editors were putting in an extraordinary effort in order to maintain Wikipedia quality in an emergency situation and for how long this is possible)

ClueBot NG:
"to scan every edit made to Wikipedia in real time"
"Built on Bayesian neural networks and trained with data
about what kind of edits Wikipedians regularly revert as
vandalism"

"refining an edit is not considered a revert, and Wikipedians
are discouraged from excessively reverting edits made in
good faith. As such, reverts are the predominant
mechanism used to remove undesirable material, notably
vandalism and spam"

"Huggle, the most widely-used, fully assisted, counter-
vandalism tool, were made within 1 minute of the
offending edit. It is interesting that reverts with STiki, a
newer and more sophisticated queue-based vandal fighting
tool, are more often made to somewhat older edits, with a
time-to-revert distribution that is closer to unassisted edits.
This suggests that Huggle and STiki are targeting different
kinds of edits"

"bots like AWB, DumbBOT, and EmausBot are
less like the fully-automated counter-vandalism bots and
closer to batch scripts that routinely perform cleanup,
administrative, and categorization tasks."

===========================================================
\cite{HalRied2012}

"The first tools to redefine the
way Wik ipedia dea lt with va n-
dalism were AntiVandalBot and
VandalProof."

"AntiVandalBot used a simple set
of rules and heuristics to monitor
changes made to articles, identify the
most obvious cases of vandalism, and
automatically revert them"

1st vandalism fighting bot:
"this bot made it possible, for the first
time, for the Wikipedia community
to protect the encyclopedia from
damage without wasting the time
and energy of good-faith editors"

"it
wasn’t very intelligent and could only
correct the most egregious instances
of vandalism."

"VandalProof, an early cyborg
technology, was a graphical user
interface written in Visual Basic that
let trusted editors monitor article
edits as fast as they happened in
Wikipedia and revert unwanted
contributions in one click."

"Several years and many iterations later [...] these tools have had an
increa sing role in ma int a ining
article quality on Wikipedia."

"ClueBot _ NG ha s replaced
AntiVandalBot’s simple rules"

"Huggle has replaced VandalProof with
a slick user interface, configurablity,
and an intelligent system for sorting
edits by vandalistic likelihood"

"Huggle, one of the most popular
antivanda lism editing tools on
Wikipedia, is written in C#.NET
and any user can download and
install it. Huggle lets editors roll back
changes with a single mouse click,
but because the tool is so powerful,
rollback permission is restricted to
administrators and a few thousand
other Wikipedia users."
"Huggle makes it easy to review
a series of recent revisions by
filtering them according to the
user’s preferences."

huggle also sends out warnings to the offending editor on revert

"Some Wikipedians feel that such
motivational measures have gone
too far in making Wikipedia like a
game rather than a serious project.
One humorous entry even argues that
Wikipedia has become a MMORPG—
a massively multiplayer online role-
playing game—with “monsters”
(va nda ls) to slay, “experience”
(edit or revert count) to earn, and
“overlords” (administrators) to submit
to (http://en.wikipedia.org/wiki/
Wikipedia:MMORPG)."

==========================================
\cite{WestKanLee2010}

"STiki is an anti-vandalism tool for Wikipedia. Unlike similar tools, STiki does not rely on natural language
processing (NLP) over the article or diff text to locate vandalism"

"STiki leverages spatio-temporal properties of revision metadata."
"The feasibility of utilizing such properties was demonstrated in our prior
work, which found they perform comparably to NLP-efforts while being more efficient, robust to evasion, and
language independent."

"It consists of, (1) a server-side
processing engine that examines revisions, scoring the likelihood each is vandalism, and, (2) a client-side GUI
that presents likely vandalism to end-users for definitive classiffcation (and if necessary, reversion on
Wikipedia"
