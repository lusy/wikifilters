\cite{WulThaDix2017}

personal attacks

methodology: crowd-sourcing + machine learning
looked at comments on Wikipedia talk pages
100k human labeled comments
63k machine labeled (classifier)
generated data set:
https://figshare.com/articles/Wikipedia_Detox_Data/4054689

analysed time period: 2004-2015

"The primary contribution of this paper is a methodology for quan-
titative, large-scale, longitudinal analysis of a large corpus of on-
line comments."

Questions:
"What is the impact of allowing anonymous contributions, namely those from unregistered users?
How do attacks vary with the quantity of a user’s contributions?
Are attacks concentrated among a few users?
When do attacks result in a moderator action?
And is there a pattern to the timing of personal attacks?"

"the majority of personal attacks on Wikipedia are not
the result of a few malicious users, nor primarily the consequence
of allowing anonymous contributions from unregistered users"

"Report highlights that 73% of adult internet users have seen some-
one harassed online, and 40% have personally experienced it [5]."

"Platforms combat this with policies"
"Wikipedia has a policy of “Do not make personal attacks
anywhere in Wikipedia”[33] and notes that attacks may be removed
and the users who wrote them blocked. 1"

classifier: "character-level n-
grams result in an impressively flexible and performant classifier
for a variety of abusive language in English."

"The Wikimedia Foundation found that
54% of those who had experienced online harassment expressed
decreased participation in the project where they experienced the
harassment [23]"

"Through labeling
a random sample, we discovered that the overall prevalence of per-
sonal attacks on Wikipedia talk pages is around 1% (see Section
5.1)."

"Annotators whose accuracy on these test questions fell below a 70%
threshold would be removed from the task."
"This allows us to aggregate judgments
from 10 separate people when constructing a single label for each
comment. We chose 10 judgments based on experiments in Sec. 4.3"

note: the annotators could be biased and their judgement on what is a personal attack may differ from this of the Wikipedian community

"approximately 30% of attacks
come from registered users with over 100 contributions."
"that less than a fifth of personal attacks currently trigger any action
for violating Wikipedia’s policy."
"personal attacks clus-
ter in time - perhaps because one personal attacks triggers another.
If so, early intervention by a moderator could have a disproportion-
ately beneficial impact."

discussion:
"While there are many such questions to analyze,
some notable examples include:
1. What is the impact of personal attacks on a user’s future con-
tributions?
2. What interventions can reduce the level of personal attacks
on a conversation?
3. What are the triggers for personal attacks in Wikipedia com-
ments?"

======================================================
\cite{Geiger2011}

discusses the acceptance, coming into existence and live cycle of bots
begins by a story of elections for the Arbitration Committee (ArbCom), where a bot was nominated, which was taken with different degrees of seriousness (and frustration) by different editors
then discusses at length the discussions around the HagermanBot (signing discussion entries)
and the eventually created opt-out possibilities when it comes to be processed by a bot

touches on philosophical and moral discussions about what is a bot, what rights a bot has and what rights of human editors must be put first

touches on ANT when justifying the need to discuss bots as actors on the same level as humans

"Tawker,
speaking through his bot, was ironically claiming that computerized editors ought to have the
same sociopolitical rights and responsibilities as human editors,"
"I argue (with all seriousness) that these automated software
agents already have a similar level of influence on how Wikipedia as a free and open ency-
clopedia project is constituted."
"we must be careful not to fall into familiar narra-
tives of technological determinism when asking who – or what – actually controls Wikipedia."

cites some stats to underline the (growing) importance of bots
then discusses previous research which seem not to pay the due attention to bots (sometimes with the justification bots are not important enough which lies on old data and sometimes with no justification at all)

"While bots were
originally built to perform repetitive editorial tasks that humans were already doing, they are
growing increasingly sophisticated and have moved into administrative spaces. Bots now po-
lice not only the encyclopedic nature of content contributed to articles, but also the sociality
of users who participate in the community."

"Administrative Intervention against Vandalism, or AIV),
bots make about 50% of all edits, and users with semi-automated editing tools make another
30%"

"My goal in this chapter is to describe the complex social and technical environment in which
bots exist in Wikipedia, emphasizing not only how bots produce order and enforce rules, but
also how humans produce bots and negotiate rules around their operation."

"Operators of bots have also expressed
frustration when their bots become naturalized, that is, when users assume that the bot’s
actions are features of the project’s software instead of work performed by their diligent
computerized workers. In general, bots tend to be taken for granted, and when they are
discussed, they are not largely differentiated from human editors"

"We must pay close attention to both the material and semiotic conditions
in which bots emerge within the complex collective of editors, administrators, committees,
discussions, procedures, policies, and shared understandings that make up the social world
of Wikipedia."

"Bots, like infrastructures in general, 23 simultaneously produce and rely upon
a particular vision of how the world is and ought to be, a regime of delegation that often
sinks into the background – that is, until they do not perform as expected and generate
intense controversies."
//important! signal when discussing filters as well!
//quotes Star 1999 "The Ethnography of Infrastructure"

". This controversy illustrated that a particular kind of normative enforce-
ment and correction, while acceptable when casually performed on a fraction of violations
sometimes days or weeks after, became quite different when universally and immediately
implemented by a bot."

high level issues of rights and responsibilities emerged
"social understanding that ‘bots ought to be better behaved than people’,"

HagermanBot
- explains the origins of the signatures guideline

criticism of the HagermanBot:
- "For these editors, HagermanBot’s message was ‘embarrassing’, as one editor stated, making them ap-
pear as if they had blatantly violated the Signatures guideline"
- "Others did not want bots editing messages other users left for them on their own user talk pages as a matter of principle,"
- "and an equally vocal group did not want the bots adding signatures to their own comments."

Reasons for
"it seemed that Hagerman had a strong set of allies: a growing number of
enthusiastic supporters, the BAG, the Signatures guideline, ideals of openness and transpar-
ency, visions of an ideal discursive space, the {{unsigned}} template, and a belief that signing
unsigned comments was a routine act that had long been performed by humans."

objections
"Yet for some reason, a growing number of editors objected to this typical, uncontroversial practice
when HagermanBot performed it."
"they portrayed as an unfair imposition of what they believed
ought to be optional guidelines. The anti-HagermanBot group was diverse in their stated ra-
tionales and suggested solutions, but all objected to the bot’s operation on some level."

"In the ensuing discussion – which was comprised of BAG members, administrators, and
other Wikipedians – it became clear that this was not simply a debate about signatures and
timestamps."
"full-blown controversy about the morality of delegat-
ing social tasks to technologies,"

Supporters:
"claiming that HagermanBot was only acting in line with a
well-established and agreed-upon understanding that the community had reached regarding
the importance of signatures in discussion spaces. For them, the burden was on the critics
to reach a consensus to amend the Signatures guideline if they wanted to stop the bot from
operating."
"placing strong moral emphasis on the role of signatures
and timestamps in maintaining discursive order and furthering the ideals of openness and
verifiability."

Opposers:
"did not directly contest the
claims made regarding the importance of signatures, discussion pages, and communicative
conventions."
"opposing view of how users, and
especially bot operators, ought to act toward each other in Wikipedia, a view that drew heavily
on notions of mutual respect:"

"botophobia"

end of the discussion: the HagermanBot should allow an opt-out mechanism

"Declarations of either side’s entitlements, largely articulated in the language of positive rights,
were displaced by the notion of responsibility, good behavior, and mutual respect."

"However, a
much stronger ally proved to be the opt-out list through which angry editors could be made to
lose interest in the debate altogether."
"The strength of the opt-out list was its flexibility in rebutting the objections from two kinds of
arguments: first, the largely under-articulated claims that the bot was annoying or trouble-
some to them; and second, the ideological or rights-based arguments that the bot was acting
against fundamental principles of the project’s normative structure."

an extention of the opt-out mechanism to all bots:
"However, seeing a need for extending this functionality to all possible bots, he [Rich Farmbrough]
created a template called {{nobots}}, which was to perform the same function as Hagerman-
Bot’s exclusion tag, except apply to all compliant bots."
"with no actual technical ability to restrict non-compliant bots from editing."

but the BAG can require a bot to comply with {{nobots}} in order to allow its operation

the compliance then found place in standard bot building toolkits

"The case of HagermanBot shows us how a weak but pre-existing social norm was controver-
sially reified into a technological actor."

"In all, bots defy simple single-sided categorizations: they are both editors and software, social
and technical, discursive and material, as well as assembled and autonomous."

============================================
\cite{HalTar2015}

"Today, we’re announcing the release of a new artificial intelligence service designed **to improve the way editors maintain the quality** of Wikipedia" (emphsis mine)
" This service empowers Wikipedia editors by helping them discover damaging edits and can be used to immediately “score” the quality of any Wikipedia article."

"these specs actually work to highlight potentially damaging edits for editors. This allows editors to triage them from the torrent of new edits and review them with increased scrutiny. " (probably triage the edits, not the specs)

"By combining open data and open source machine learning algorithms, our goal is to make quality control in Wikipedia more transparent, auditable, and easy to experiment with."

//so, purpose of ORES is quality control

"Our hope is that ORES will enable critical advancements in how we do quality control—changes that will both make quality control work more efficient and make Wikipedia a more welcoming place for new editors."

"ORES brings automated edit and article quality classification to everyone via a set of open Application Programming Interfaces (APIs). The system works by training models against edit- and article-quality assessments made by Wikipedians and generating automated scores for every single edit and article."

"English Wikipedians have long had automated tools (like Huggle and STiki ) and bots (like ClueBot NG) based on damage-detection AI to reduce their quality control workload.  While these automated tools have been amazingly effective at maintaining the quality of Wikipedia, they have also (inadvertently) exacerbated the difficulties that newcomers experience when learning about how to contribute to Wikipedia. "
"These tools encourage the rejection of all new editors’ changes as though they were made in bad faith," //NB!!!
"Despite evidence on their negative impact on newcomers, Huggle, STiki and ClueBot NG haven’t changed substantially since they were first introduced and no new tools have been introduced. " //what about the edit filters? when were Huggle,STiki and ClueBotNG introduced?

"decoupling the damage prediction from the quality control process employed by Wikipedians, we hope to pave the way for experimentation with new tools and processes that are both efficient and welcoming to new editors. "

caution: biases in AI
" An algorithm that flags edits as subjectively “good” or “bad”, with little room for scrutiny or correction, changes the way those contributions and the people who made them are perceived."

"Examples of ORES usage. WikiProject X’s uses the article quality model (wp10) to help WikiProject maintainers prioritize work (left). Ra·un uses an edit quality model (damaging) to call attention to edits that might be vandalism (right)." //interesting for the memo

"Popular vandal fighting tools, like the aforementioned Huggle, have already adopted our revision scoring service."

further ORES applications:
"  But revision quality scores can be used to do more than just fight vandalism. For example, Snuggle uses edit quality scores to direct good-faith newcomers to appropriate mentoring spaces,[4] and dashboards designed by the Wiki Education Foundation use automatic scoring of edits to surface the most valuable contributions made by students enrolled in the education program"
