# Wichtige Fragen

* Wie funktionieren sie?
  * technisch
  * governance/community level
* Sind sie wirksam (aus wessen Perspektive)
* Was für Wirkmechanismen stecken dahinter?
* Wie komplex sind die?
* Wie sichtbar sind sie für normalsterbliche Nutzer*innen
* Wer macht sie? Wie viele Menschen sind das?
* Was sind das für Filter
* Wie sind sie entstanden?
  * Aus was für eine Debatte sind sie entstanden?
* Wie haben sie sich über die Jahre entwickelt?
* Wie ist es organisiert?
* Was sind die Auswirkungen auf die Nutzer*innen?
* Wie viele False Positives erzeugen die Filter? Wie wird damit umgegangen?


# Weitere Gedanken

* "*Can* prevent *potentially* harmful behaviour" <--- wo kommt das her?
* general context: upload filter
* check toollab (stats + graphs)
* was gibts dazu bereits für Forschung?
* was wird eigentlich als Vandalismus definiert? (falls Vandalismus der Hauptanlass für die Filter is) (auch historisch)


========================================================================
https://en.wikipedia.org/wiki/Wikipedia:Edit_filter

"The edit filter is a tool that allows editors in the edit filter manager group to set controls mainly[1] to address common patterns of harmful editing."
Current filters: https://en.wikipedia.org/wiki/Special:AbuseFilter

DEF:
"A filter automatically compares every edit made to Wikipedia against a defined set of conditions. If an edit matches the conditions of a filter, that filter will respond by logging the edit. It may also tag the edit summary, warn the editor, revoke his/her autoconfirmed status, and/or disallow the edit entirely.[2]"
Footnote 2: "The extension also allows for temporary blocking, but these features are disabled on the English Wikipedia." <-- TODO: Is there wikipedia on which it isn't disallowed?

Software: https://www.mediawiki.org/wiki/Extension:AbuseFilter
---> enabled on English Wikipedia 2009

On the name:
"The term "edit filter" rather than "abuse filter" is currently used for user-facing elements of the filter as some of the edits it flags are not harmful;[1] the terms are otherwise synonymous."

"Because even the smallest mistake in editing a filter can disrupt the encyclopedia, only editors who have the required good judgment and technical proficiency are permitted to configure filters."
--> Who are these editors? Who decides they are qualified enough?

"Filters are created and configured by edit filter managers, but they can be requested by any editor."
"all administrators can view private filters"
"This group is assignable by administrators, who may also assign the right to themselves"
"The assignment of the edit filter manager user right to non-admins is highly restricted. It should only be requested by and given to highly trusted users, when there is a clear and demonstrated need for it."
"demonstrated knowledge of the extension's syntax and in understanding and crafting regular expressions is absolutely essential"
"Editors who are not edit filter managers should consider helping out at requested edit filters and troubleshooting at false positives to help gain experience and demonstrate these skills"
"Requests for assignment of the group to non-admins can be made at the edit filter noticeboard, where a discussion will be held before a decision is made;discussions are normally held open for 7 days."
"If an edit filter manager is misusing the user right, the concern should first be raised with them directly. If discussion does not resolve the issue, a request for discussion or removal of the user right may be made at the edit filter noticeboard. "
"If you have the edit filter manager user right, please ensure you follow the Password strength requirements and appropriate personal security practices. Two-factor authentication enrollment is available for edit filter managers. Because edit filters affect every edit made, a compromised account will be blocked and its privileges removed on grounds of site security. In the unlikely event that your account is compromised, notify an administrator or bureaucrat (for administrators) immediately so they can block your account and remove any sensitive privileges to prevent damage. "
//interessanterweise is 2factor-auth auch nur für diese speziellen Benutzer*innen erlaubt; sonst kann man die Seite nicht ansehen

List of current edit filter managers
EN: https://en.wikipedia.org/wiki/Special:ListUsers/abusefilter (currently: 155)
CAT: https://ca.wikipedia.org/wiki/Especial:Usuaris/abusefilter (currently: 4 users)

-- auf Spanisch/Deutsch/Russisch existiert die Rolle nicht; interessant zu wissen, ob sie iwo subsumiert wurde
-- auf Bulgarisch übrigens auch nicht, aber da existiert auch die gesamte EditFilter seite nicht

What do filters do?/What actions they trigger (vgl DEF) in order of graveness:
- disallow -- editor is informed, if their edit is being disallowed and offered the option to report a false positive;
  "It is also possible to have a user's autoconfirmed status revoked if a user trips the filter."
  caution to use it seldomly and after a thorough discussion on what is a undesirable edit
- warn -- editor is informed that their edit may be problematic and given the option to save or abort the edit (and in report the false positive trigerred by the filter)
- add a tag - "edit is tagged for review by patrollers." -- TODO who are patrollers? are there some in lang versions other than EN?
  "Patrols are a specialized type of WikiProject used in the English Wikipedia to watch over a class of pages and take any appropriate actions. Most patrol actions are performed by individual Wikipedians, but some are performed by bots—computer programs or preprogrammed scripts that make automated edits without a need for real time human decision-making. " https://en.wikipedia.org/wiki/Wikipedia:Patrols
- log the edit - "In this case, the edit is merely added to the AbuseLog. When testing new filters, this is the suggested setting to use."

"Except in urgent situations, new edit filters should generally be tested without any actions specified (simply enabled) until a good number of edits have been logged and checked before being implemented in "warn" or "disallow" modes. If the filter is receiving more than a very small percentage of false positives it should usually not be placed in 'disallow' mode."

Alternatives:
"Edit filter managers should be familiar with alternatives that might be more appropriate in a given situation. For example, problems on a single page might be better served with page protection, and problems with page titles or link spam may find the title blacklist and spam blacklist more effective respectively. Because edit filters check every edit in some way, filters that are tripped only rarely are discouraged. "

Exemptions for "urgent situation" -- what/how are these defined?
Discussions may happen postfactum here and filter may be applied before having been thoroughly tested; in this case the corresponding editor is responsible for checking the logs regularly and making sure the filter acts as desired

Hidden filters!
"Non-admins in good standing who wish to review a proposed but hidden filter may message the mailing list for details."
// what is "good standing"?
// what are the arguments for hiding a filter?
// are users still informed if their edit triggers a hidden filter?

"For all filters, including those hidden from public view, a brief description of what the rule targets is displayed in the log, the list of active filters, and in any error messages generated by the filter. "

"Filters should only be hidden where necessary, such as in long-term abuse cases where the targeted user(s) could review a public filter and use that knowledge to circumvent it. Filters should not generally be named after abusive editors, but rather with a simple description of the type of abuse, provided not too much information is given away."

"Be careful not to test sensitive parts of private filters in a public test filter (such as Filter 1): use a private test filter (for example Filter 2) if testing is required."

harassment! mailinglist
"If it would not be desirable to discuss the need for a given edit filter on-wiki, such as where the purpose of the filter is to combat harassment by an abusive banned user who is likely to come across the details of the request, edit filter managers can be emailed directly or on the wikipedia-en-editfilters mailing list at wikipedia-en-editfilters@lists.wikimedia.org."

https://lists.wikimedia.org/mailman/listinfo/wikipedia-en-editfilters
"private mailing list used by English Wikipedia edit filter managers, "
"primarily for discussing hidden filters."
"The mailing list should not be used as a venue for discussions that could reasonably be held on-wiki."


batch testing interface

=================================================================
https://de.wikipedia.org/wiki/Wikipedia:Bearbeitungsfilter

"Der Bearbeitungsfilter (englisch: edit filter; früher: „Missbrauchsfilter“ oder abuse filter) ist ein vielseitig einsetzbares Werkzeug zur Beobachtung und Verhinderung problematischer Bearbeitungen. Dazu gehört die Bekämpfung von Verstößen gegen die Wikipedia-Richtlinien, insbesondere Wikipedia:Vandalismus."

"Derzeit können alle Administratoren Filter bearbeiten."
--> Unterschied zu EN, ne? Da gibts ne Spezielle Gruppe für? (Aber Admins können da Menschen reinstecken, auch sich selber)

Mögliche Auswirkungen
"
* das Verhindern von Edits, die bestimmte Eigenschaften erfüllen
* das Hinweisen des agierenden Benutzers bei solchen Edits
* das reine Aufspüren solcher Edits"

"Für jede aktive Filterregel gilt, dass sie begründet sein muss und die Verhältnismäßigkeit durch die konkret anzunehmenden potentiell schädigenden Bearbeitungen gewahrt ist."

"Gründe für den Einsatz des Bearbeitungsfilter können sein:
* Vandalismus/Sperrumgehungen durch einen Wikipedianer mit wechselnden Benutzerkonten/IP-Adressen oder auf mehreren Seiten, wobei eine konventionelle Artikel- oder Benutzer-Sperre zu viele Nebenwirkungen hätte
* sonst schwer zu findende (unabsichtliche) Verstöße gegen die Wikipedia-Richtlinien
* Erstellung von Wartungslisten (wobei versucht werden sollte, hier eher Bots zu nutzen)"

=================================================================
https://en.wikipedia.org/wiki/Wikipedia:Edit_filter_noticeboard

According to the Edit filter Notice board:
"There are currently 196 enabled filters and 13 stale filters with no hits in the past 30 days (Purge). See also the edit filter graphs." (Stand: 24.11.2018)
"There are currently 198 enabled filters and 11 stale filters with no hits in the past 30 days (Purge). See also the edit filter graphs." (Stand: 25.11.2018, seems to change frequently!)

- discuss current filter behaviour
- suggest filter for deletion, since it's not particularly helpful: " unnecessary, is preventing good edits, or is otherwise problematic,"
  (you can also raise the issue directly with the filter manager who created or enabled the filter)

apart from that: current ongoing discussions on single filters/problems that may require a filter

===============================================================
https://en.wikipedia.org/wiki/Wikipedia:Edit_filter/Requested
-- gibts nur noch auf Deutsch

- suggest new filters

"This page is for people without the abusefilter-modify permission or people without sufficient knowledge about the coding involved to make requests to enact edit filters."

There's a "Bear the following in mind:" checklist
"Filters are applied to all edits. Therefore, problematic changes that apply to a single page are likely not suitable for an edit filter."
- filter, after adding up, make editing slower
- in depth checks should be done by a separate software that users run on their own machines
- no trivial errors should be catched by filters (ala style guidelines)
- there are Titles Blacklist and Link/Spam Blacklist

===============================================================
https://de.wikipedia.org/wiki/Wikipedia:Bearbeitungsfilter/Antr%C3%A4ge
DE für  https://en.wikipedia.org/wiki/Wikipedia:Edit_filter/Requested

viel mehr kram auf der oberseite als bei der englischen; diskussionen zu jeden einzelnen(?) filter sind direkt verlinkt

"Auf dieser Seite können Vorschläge für neue Regeln des Bearbeitungsfilters eingereicht werden. Diskussionen zu Filterregeln (z.B. wegen irrtümlichen Blockaden), deren Nummer nicht bekannt ist, werden ebenfalls hier geführt. Zusätzlich gibt es für jede bereits bestehende Regel eine eigene Diskussionsseite. Eine Übersicht zu diesen einzelnen Seiten gibt der Abschnitt #Liste der Diskussionen zu einzelnen Regeln. Im Archiv werden Diskussionen gesammelt, für die keine Regeln erstellt wurden."

Ein Bsp:
https://de.wikipedia.org/wiki/Wikipedia:Bearbeitungsfilter/271
https://de.wikipedia.org/wiki/Wikipedia:Vandalismusmeldung/Archiv/2018/11/25#Benutzer:Zollwurf_(erl.)
-- ich hab mittlerweile den Eindruck, dass so was evtl auf der Engl. Wikipedia nicht öffentlich wäre?
-- allerdings, ist so ein Problem mit nem Filter zu lösen? Wie kann es sonst gelöst werden?

===============================================================
https://en.wikipedia.org/wiki/Wikipedia:Long-term_abuse

117 active cases [Stand 24.11.2018]
there's a list available at least for the active cases with detailed abuse reports
There's also an archive page of abuse cases: https://en.wikipedia.org/wiki/Wikipedia:Long-term_abuse/Archive (25 entries [Stand 24.11.2018])
And full list of cases: https://en.wikipedia.org/wiki/Wikipedia:Long-term_abuse/Full

"This page summarises a limited number of long term abusers, to assist members of the community who believe they may have cause to report another incident. Note that this page is not a noticeboard. Names should only be added for the most egregious and well-attested cases. Most users here will have been banned, some on multiple occasions. "

"Don't provide too much info
    The text should tread a careful balance between providing useful information and providing enough to obstruct detection. In general such information should only be shared with users of a high level of reputation."

===============================================================
https://de.wikipedia.org/wiki/Wikipedia:WikiProjekt_Vandalismusbek%C3%A4mpfung/Troll-Dokumentationsseiten
DE zu https://en.wikipedia.org/wiki/Wikipedia:Long-term_abuse

"Hier werden Seiten im Benutzernamensraum gesammelt, auf denen die Aktivitäten unterschiedlicher Wikipediastörer charakterisiert oder dokumentiert werden sollen. "
!!!"Eine Definition von störendem Verhalten, z. B. in Bezug auf Art, Umfang oder Dauer der Projektstörung, die eine Bezeichnung als „Wikipediatroll“ rechtfertigt, ist in der Autorengemeinschaft umstritten."

sonst eben wieder listen mit bekannten Fällen; kein Count der aktuell aktiven Fällen wie bei der Englischen Seite

===============================================================
https://en.wikipedia.org/wiki/Special:AbuseFilter/384

bad words in articles and user names filter

===============================================================
https://en.wikipedia.org/wiki/Special:AbuseLog
https://de.wikipedia.org/wiki/Spezial:Missbrauchsfilter-Logbuch
https://es.wikipedia.org/wiki/Especial:RegistroAbusos
https://ca.wikipedia.org/wiki/Especial:Registre_dels_abusos
https://bg.wikipedia.org/wiki/%D0%A1%D0%BF%D0%B5%D1%86%D0%B8%D0%B0%D0%BB%D0%BD%D0%B8:%D0%94%D0%BD%D0%B5%D0%B2%D0%BD%D0%B8%D0%BA_%D0%BD%D0%B0_%D1%84%D0%B8%D0%BB%D1%82%D1%8A%D1%80%D0%B0

can search for all filter triggers in a period of time/by a specific user

auf der Deutschen Seite ist die Filter-Aktion und -Beschreibung aufgelistet, aber die Filternummer ist nicht verlinkt, also kann man nicht direkt den Quellcode ansehen;
Außerdem muss man direkt auf die Seite gehen, um die versuchte Änderungen in der Versionsgeschichte anzugucken

die Spanische Seite ist ähnlich aufgebaut wie die Deutsche.
!!!Da es aber auf Spanisch anscheinend keine "Saved Revisions" (oder wie hieß das nochma) gibt, kann man eigenltich die abusiven Änderungen gar nicht mehr sichten und noch kann man den Filter angucken, der eigentlich getriggered wurde

Katalanisch ist das selbe wie Spanisch, mit dem Unterschied, dass aus irgendeinem Grund (keine Regelmäßigkeit festgestellt), manchmal die Summaries auf Englisch sind, da wird auch ein Diff mit den offending Changes angezeigt.

the Bulgarian page uses the same form as the English one, so source code of the filter as well as diff of the changes can be viewed

===============================================================
https://en.wikipedia.org/wiki/Wikipedia:Edit_filter/False_positives
https://es.wikipedia.org/wiki/Wikipedia:Filtro_de_ediciones/Portal/Archivo/Reporte_de_falsos_positivos/Actual
https://ca.wikipedia.org/wiki/Viquip%C3%A8dia:Filtre_d%27edicions/Falsos_positius

a detailed page with ongoing/recently reported cases

there seems to be no such page for BG and DE --> no possibility to report false positives?

================================================================
Current status (29.11.2018)

EN: There are currently 201 enabled filters, and 12 stale filters with no hits in the past 30 days (Purge).
from https://en.wikipedia.org/wiki/Special:AbuseFilter
DE: 170 enabled, disabled, privat, öffentlich https://de.wikipedia.org/wiki/Spezial:Missbrauchsfilter/?deletedfilters=hide&limit=250&title=Spezial%3AMissbrauchsfilter%2F
ES: 92 https://es.wikipedia.org/wiki/Especial:FiltroAntiAbusos/?deletedfilters=hide&limit=100&title=Especial%3AFiltroAntiAbusos%2F
CA: 24  https://ca.wikipedia.org/wiki/Especial:Filtre_d%27abuses (spannend: man muss sich anmelden um das angucken zu können!)
BG: 24 https://bg.wikipedia.org/wiki/%D0%A1%D0%BF%D0%B5%D1%86%D0%B8%D0%B0%D0%BB%D0%BD%D0%B8:%D0%A4%D0%B8%D0%BB%D1%82%D1%8A%D1%80_%D1%81%D1%80%D0%B5%D1%89%D1%83_%D0%B7%D0%BB%D0%BE%D1%83%D0%BF%D0%BE%D1%82%D1%80%D0%B5%D0%B1%D0%B8

=================================================================
https://en.wikipedia.org/wiki/Special:AbuseFilter

"There are currently 203 enabled filters, and 11 stale filters with no hits in the past 30 days (Purge)."

that's the management interface!
"Welcome to the Edit Filter management interface. Using the Edit Filter, authorized users can configure a wide range of tests, which may help identify and prevent potentially harmful edits and other activities before they are added to the wiki, and the automatic actions to be taken."

"PLEASE be careful. This is potent stuff. Unless it's urgent, always test your filters with no actions enabled first."

weird? the test interface https://en.wikipedia.org/wiki/Special:AbuseFilter/test
says: "For security reasons, only users with the right to view private abuse filters or modify filters may use this interface."
shouldn't all filter editors be able to test??

Collaboration with bots:
"There is a bot reporting users tripping certain filters at WP:AIV and WP:UAA; you can specify the filters here."
https://en.wikipedia.org/wiki/User:DatBot/filters

Sortable table of all filters with following columns:
Filter ID 	Public description 	Actions 	Status 	Last modified 	Visibility 	Hit count
links to single filters, e.g. --> https://en.wikipedia.org/wiki/Special:AbuseFilter/1 (see bellow for detailed filter page)
"Actions" is one of: warn | tag | disallow | throttle | ?? (possibly more, not directly visible)
"Status" is: enabled | disabled
"Last modified" provides a link to diff between versions and the user who did the modification
"Visibility" is: private | public
"Hit count": which period is counted? total number of hits since the filter was enabled? (for all enabled periods, in case it was enabled/disabled multiple times?)

Filter with most hits:
Filter ID 	Public description 	Actions 	Status 	Last modified 	Visibility 	Hit count
61 	New user removing references 	Tag 	Enabled 	12:43, 14 May 2017 by Zzuuzz (talk | contribs) 	Public 	1,593,851 hits

=====================================================================
https://en.wikipedia.org/wiki/Special:AbuseFilter/1

where following information can be viewed:
Filter id; public description; filter hits; statistics; code (conditions); notes (left by filter editors to log changes;); flags ("Hide details of this filter from public view", "enable this filter", "mark as deleted");
links to: last modified (with diff and user who modified it), edit filter's history; "export this filter to another wiki" tool;

Actions to take when matched:
Trigger actions only if the user trips a rate limit
Trigger these actions after giving the user a warning
Prevent the user from performing the action in question
Revoke the user's autoconfirmed status
Tag the edit in contributions lists and page histories

and the filter can be modified if the viewing editor has the right permissions

statistics are info such as "Of the last 1,728 actions, this filter has matched 10 (0.58%). On average, its run time is 0.34 ms, and it consumes 3 conditions of the condition limit." (that's filter id 61) // not sure what the condition limit is
"Of the last 5,616 actions, this filter has matched 0 (0.00%). On average, its run time is 0 ms, and it consumes 0 conditions of the condition limit." (that's filter id 1)

=========================================================================
https://en.wikipedia.org/wiki/Special:AbuseFilter/history/1

Time 	User 	Public filter description 	Flags 	Actions 	Changes

the link with the timestamp links back to the filter editor (see previous page)
user links to the user
changes links to a diff of the current revision with the previous one

=========================================================================
https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2009-03-23/Abuse_Filter

"The AbuseFilter extension, developed by User:Werdna, is now enabled on English Wikipedia. The extension allows all edits to be checked against automatic filters and heuristics, which can be set up to look for patterns of vandalism including page move vandalism and juvenile-type vandalism, as well as common newbie mistakes. When a match is found, the extension can take specified actions, ranging from logging the edit to be checked, giving a warning (e.g. "did you really intend to blank a page?"), to more serious actions such as blocking users."
from https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/2009-03-23/Abuse_Filter
("The Signpost is a monthly community-written and -edited online newspaper covering the English Wikipedia, its sister projects, the Wikimedia Foundation, and the Wikimedia movement at large." https://en.wikipedia.org/wiki/Wikipedia:Wikipedia_Signpost/About)

Note: User:Werdna
https://en.wikipedia.org/wiki/User:Werdna
"I'm Andrew Garrett. I started volunteering in 2005, and I worked at the Wikimedia Foundation from 2009 until I left in 2015 because 7 years is a long time. This is my personal account, so while I'll make mistakes, I intend for actions taken with this account to be in my personal capacity only. My work account was Andrew Garrett."
http://www.andrewjgarrett.com/

========================================================================
https://en.wikipedia.org/w/index.php?title=Wikipedia:Edit_filter&oldid=221158142
Edit_filter page first version, created 23.06.2008, where User:Werdna announced the upcoming MediaWiki Extention he was working on.

"I've been developing an extension which allows privileged users to add very specific restrictions on actions going through on the wiki.

This gives us the opportunity to prevent damage from vandals with very specific modi operandi."

"I submit to the community that this gives us an extraordinary opportunity to disallow some of the worst and most annoying types of vandalism which occur on Wikipedia, and to refocus our efforts into doing other, more productive things than cleaning up after page-move vandalism."

There's a list of things that the filters can filter on

"It is noteworthy here that no rights, even to view information, are granted to all users. This is deliberate. Information about the filters active on Wikipedia would be sensitive information, and, if it were to be publically available, release to those who wish to circumvent them would be inevitable. I currently propose that the right to view filters be almost as well-protected as the right to modify them."

"Of course, this issue of the closed nature of the extension is the extension's main problem, and the one which I foresee the most objections on. "

=========================================================================
https://en.wikipedia.org/wiki/Wikipedia:Edit_filter/Instructions

"This section explains how to create a filter with some preliminary testing, so you don't flood the history page."
- read the docs https://www.mediawiki.org/wiki/Extension:AbuseFilter/Rules_format
- test with debugging tools  https://en.wikipedia.org/wiki/Special:AbuseFilter/tools (visible only for users who are already in the edit filter managers user group)
- test with batch testing interface (dito)
- create logging only filter: https://en.wikipedia.org/wiki/Special:AbuseFilter/new (needs permissions)
- Post a message at WP:EFN (edit filter notice board), so other edit filter managers have a chance to improve it
- Finally, fully enable your filter, e.g. add warning, prevention, tagging, etc.

tips on controlling efficiency/order of operations
lazy evaluation: when 1st negative condition is met, filter terminates execution

"You should always order your filters so that the condition that will knock out the largest number of edits is first. Usually this is a user groups or a user editcount check; in general, the last condition should be the regex that is actually looking for the sort of vandalism you're targeting. "

===========================================================================
https://www.mediawiki.org/wiki/Extension:AbuseFilter

Author(s)
    Andrew Garrett, <-- lead dev
    River Tarnell
    Victor Vasiliev
    Marius Hoch

a media wiki extention written in php;
licensed under GPL 2.0
no further dependencies needed

code repo: https://gerrit.wikimedia.org/g/mediawiki/extensions/AbuseFilter
issue tracker: https://phabricator.wikimedia.org/tag/abusefilter/

"Once the extension has been installed, filters can be created/tested/changed/deleted and the logs can be accessed from the Abuse filter management page Special:AbuseFilter. "

you can import filters from wikipedia

Creates following tables
mysql> describe abuse_filter; (https://www.mediawiki.org/wiki/Extension:AbuseFilter/abuse_filter_table)
+--------------------+---------------------+------+-----+---------+----------------+
| Field              | Type                | Null | Key | Default | Extra          |
+--------------------+---------------------+------+-----+---------+----------------+
| af_id              | bigint(20) unsigned | NO   | PRI | NULL    | auto_increment |
| af_pattern         | blob                | NO   |     | NULL    |                |
| af_user            | bigint(20) unsigned | NO   | MUL | NULL    |                |
| af_user_text       | varbinary(255)      | NO   |     | NULL    |                |
| af_timestamp       | binary(14)          | NO   |     | NULL    |                | // I think that's last modified
| af_enabled         | tinyint(1)          | NO   |     | 1       |                |
| af_comments        | blob                | YES  |     | NULL    |                |
| af_public_comments | tinyblob            | YES  |     | NULL    |                |
| af_hidden          | tinyint(1)          | NO   |     | 0       |                |
| af_hit_count       | bigint(20)          | NO   |     | 0       |                |
| af_throttled       | tinyint(1)          | NO   |     | 0       |                |
| af_deleted         | tinyint(1)          | NO   |     | 0       |                |
| af_actions         | varbinary(255)      | NO   |     |         |                |
| af_global          | tinyint(1)          | NO   |     | 0       |                |
| af_group           | varbinary(64)       | NO   | MUL | default |                |
+--------------------+---------------------+------+-----+---------+----------------+

mysql> describe abuse_filter_log; https://www.mediawiki.org/wiki/Extension:AbuseFilter/abuse_filter_log_table
+------------------+---------------------+------+-----+---------+----------------+
| Field            | Type                | Null | Key | Default | Extra          |
+------------------+---------------------+------+-----+---------+----------------+
| afl_id           | bigint(20) unsigned | NO   | PRI | NULL    | auto_increment |
| afl_filter       | varbinary(64)       | NO   | MUL | NULL    |                |
| afl_user         | bigint(20) unsigned | NO   | MUL | NULL    |                | \\User ID of the author of the action.
| afl_user_text    | varbinary(255)      | NO   |     | NULL    |                | \\User name of the author of the action.
| afl_ip           | varbinary(255)      | NO   | MUL | NULL    |                |
| afl_action       | varbinary(255)      | NO   |     | NULL    |                | \\The action which triggered the filter. Values can include the following values: edit, delete, createaccount, move, upload, autocreateaccount, stashupload
| afl_actions      | varbinary(255)      | NO   |     | NULL    |                | \\What the filter made about the action
| afl_var_dump     | blob                | NO   |     | NULL    |                | \\Value of the variables of the filter that matched the edit, stored as a serialized PHP array.
| afl_timestamp    | binary(14)          | NO   | MUL | NULL    |                |
| afl_namespace    | tinyint(4)          | NO   | MUL | NULL    |                | \\Target Namespace of the filtered action.
| afl_title        | varbinary(255)      | NO   |     | NULL    |                | \\Target title of the filter action.
| afl_wiki         | varbinary(64)       | YES  | MUL | NULL    |                |
| afl_deleted      | tinyint(1)          | NO   |     | 0       |                | \\"Whether the AbuseLog entry was suppressed. 1 if suppressed, 0 otherwise.
" ohm, that means, that if 1, the rest of the line would be empty?
| afl_patrolled_by | int(10) unsigned    | YES  |     | NULL    |                | \\unused
| afl_rev_id       | int(10) unsigned    | YES  | MUL | NULL    |                | \\Foreign key to revision.rev_id, only populated for saved edits in order to show a diff link. I've got the feeling, it is also unused, for filter id 23 it is empty for all log entries
| afl_log_id       | int(10) unsigned    | YES  | MUL | NULL    |                | \\unused
+------------------+---------------------+------+-----+---------+----------------+
16 rows in set (0.00 sec)

mysql> describe abuse_filter_history; (from https://www.mediawiki.org/wiki/Extension:AbuseFilter/abuse_filter_history_table)
+---------------------+---------------------+------+-----+---------+----------------+
| Field               | Type                | Null | Key | Default | Extra          |
+---------------------+---------------------+------+-----+---------+----------------+
| afh_id              | bigint(20) unsigned | NO   | PRI | NULL    | auto_increment |
| afh_filter          | bigint(20) unsigned | NO   | MUL | NULL    |                |
| afh_user            | bigint(20) unsigned | NO   | MUL | NULL    |                |
| afh_user_text       | varbinary(255)      | NO   | MUL | NULL    |                |
| afh_timestamp       | binary(14)          | NO   | MUL | NULL    |                |
| afh_pattern         | blob                | NO   |     | NULL    |                |
| afh_comments        | blob                | NO   |     | NULL    |                |
| afh_flags           | tinyblob            | NO   |     | NULL    |                |
| afh_public_comments | tinyblob            | YES  |     | NULL    |                |
| afh_actions         | blob                | YES  |     | NULL    |                |
| afh_deleted         | tinyint(1)          | NO   |     | 0       |                |
| afh_changed_fields  | varbinary(255)      | NO   |     |         |                |
| afh_group           | varbinary(64)       | YES  |     | NULL    |                |
+---------------------+---------------------+------+-----+---------+----------------+
13 rows in set (0.00 sec)

Note! no public view of table abuse_filter_history at the moment

mysql> describe abuse_filter_action; (from https://www.mediawiki.org/wiki/Extension:AbuseFilter/abuse_filter_action_table)
+-----------------+---------------------+------+-----+---------+-------+
| Field           | Type                | Null | Key | Default | Extra |
+-----------------+---------------------+------+-----+---------+-------+
| afa_filter      | bigint(20) unsigned | NO   | PRI | NULL    |       |
| afa_consequence | varbinary(255)      | NO   | PRI | NULL    |       |
| afa_parameters  | tinyblob            | NO   |     | NULL    |       |
+-----------------+---------------------+------+-----+---------+-------+
3 rows in set (0.00 sec)

Seems to contain data for currently enabled filters only;
Question: how do we find data for disabled filters?

# API calls

## List information about filters:
https://en.wikipedia.org/w/api.php?action=query&list=abusefilters&abfshow=!private&abfprop=id%7Chits
or in the sandbox:
https://en.wikipedia.org/wiki/Special:ApiSandbox#action=query&list=abusefilters&abfshow=!private&abfprop=id%7Chits

Parameters

    abfstartid: The filter id to start enumerating from
    abfendid: The filter id to stop enumerating at
    abfdir: The direction in which to enumerate (older, newer)
    abfshow: Show only filters which meet these criteria (enabled|!enabled|deleted|!deleted|private|!private)
    abflimit: The maximum number of filters to list
    abfprop: Which properties to get (id|description|pattern|actions|hits|comments|lasteditor|lastedittime|status|private)

When filters are private, some of the properties specified with abfprop will be missing unless you have the appropriate user rights.

## List instances where actions triggered an abuse filter.
https://en.wikipedia.org/w/api.php?action=query&list=abuselog&afluser=SineBot&aflprop=ids
or in the sandbox:
https://en.wikipedia.org/wiki/Special:ApiSandbox#action=query&list=abuselog&afluser=SineBot&aflprop=ids

Parameters

    aflstart: The timestamp to start enumerating from
    aflend: The timestamp to stop enumerating at
    afldir: The direction in which to enumerate (older, newer)
    afluser: Show only entries where the action was attempted by a given user or IP address.
    afltitle: Show only entries where the action involved a given page.
    aflfilter: Show only entries that triggered a given filter ID
    afllimit: The maximum number of entries to list
    aflprop: Which properties to get (ids|user|title|action|result|timestamp|details)

===========================================================================
https://www.mediawiki.org/wiki/Extension:AbuseFilter/Rules_format

Manual on writing filter rules;

===========================================================================
https://phabricator.wikimedia.org/tag/abusefilter/

keep in mind in case of problems

===========================================================================
Google search for something on the quarry.wmflabs.org page

site:quarry.wmflabs.org all tables

===========================================================================
https://en.wikipedia.org/wiki/Wikipedia:Namespace

Namespaces
Subject namespaces 	Talk namespaces
0 	(Main/Article) 	Talk 	1
2 	User 	        User talk 	3
4 	Wikipedia 	    Wikipedia talk 	5
6 	File 	        File talk 	7
8 	MediaWiki 	    MediaWiki talk 	9
10 	Template 	    Template talk 	11
12 	Help 	        Help talk 	13
14 	Category 	    Category talk 	15
100 	Portal 	    Portal talk 	101
108 	Book 	    Book talk 	109
118 	Draft 	    Draft talk 	119
710 	TimedText 	TimedText talk 	711
828 	Module 	    Module talk 	829
2300 	Gadget 	    Gadget talk 	2301
2302 	Gadget definition 	Gadget definition talk 	2303
Virtual namespaces
-1 	Special
-2 	Media

============================================================================
https://en.wikipedia.org/wiki/Wikipedia:Vandalism

"This is not a noticeboard for vandalism. Report vandalism from specific users at Wikipedia:Administrator intervention against vandalism, or Wikipedia:Requests for page protection for specific pages.
Not to be confused with Wikipedia:Disruptive editing."

"This page documents an English Wikipedia policy."

"This page in a nutshell: Intentionally making abusive edits to Wikipedia will result in a block."

DEF Vandalism:
"On Wikipedia, vandalism has a very specific meaning: editing (or other behavior) deliberately intended to obstruct or defeat the project's purpose, which is to create a free encyclopedia, in a variety of languages, presenting the sum of all human knowledge."
"The malicious removal of encyclopedic content, or the changing of such content beyond all recognition, without any regard to our core content policies of neutral point of view (which does not mean no point of view), verifiability and no original research, is a deliberate attempt to damage Wikipedia. There, of course, exist more juvenile forms of vandalism, such as adding irrelevant obscenities or crude humor to a page, illegitimately blanking pages, and inserting obvious nonsense into a page. Abusive creation or usage of user accounts and IP addresses may also constitute vandalism."

Consequences of vandalism, vandalism management
"Vandalism is prohibited. While editors are encouraged to warn and educate vandals, warnings are by no means a prerequisite for blocking a vandal (although administrators usually only block when multiple warnings have been issued). "

"Even if misguided, willfully against consensus, or disruptive, any good-faith effort to improve the encyclopedia is not vandalism."
"For example, edit warring over how exactly to present encyclopedic content is not vandalism." !!!
"Careful consideration may be required to differentiate between edits that are beneficial, edits that are detrimental but well-intentioned, and edits that are vandalism."
"If it is clear that the editor in question is intending to improve Wikipedia, those edits are not vandalism, even if they violate some other core policy of Wikipedia."
"When editors are editing in good faith, mislabeling their edits as vandalism makes them less likely to respond to corrective advice or to engage collaboratively during a disagreement,"

Handling
"Upon discovering vandalism, revert such edits, using the undo function or an anti-vandalism tool. Once the vandalism is undone, warn the vandalizing editor. Notify administrators at the vandalism noticeboard of editors who continue to vandalize after multiple warnings, and administrators should intervene to preserve content and prevent further disruption by blocking such editors. Users whose main or sole purpose is clearly vandalism may be blocked indefinitely without warning."

"examples of suspicious edits are those performed by IP addresses, red linked, or obviously improvised usernames"

One of the strategies to spot vandalism is "Watching for edits tagged by the abuse filter. However, many tagged edits are legitimate, so they should not be blindly reverted. That is, do not revert without at least reading the edit."

"Warn the vandal. Access the vandal's talk page and warn them. A simple note explaining the problem with their editing is sufficient. If desired, a series of warning templates exist to simplify the process of warning users, but these templates are not required. These templates include

    Level one: {{subst:uw-vandalism1}} This is a gentle caution regarding unconstructive edits; it encourages new editors to use a sandbox for test edits. This is the mildest warning.
    Level two: {{subst:uw-vandalism2}} This warning is also fairly mild, though it explicitly uses the word 'vandalism' and links to this Wikipedia policy.
    Level three: {{subst:uw-vandalism3}} This warning is sterner. It is the first to warn that further disruptive editing or vandalism may lead to a block.
    Level four: {{subst:uw-vandalism4}} This is the sharpest vandalism warning template, and indicates that any further disruptive editing may lead to a block without warning."

Types of vandalism:
"
* Abuse of tags: Bad-faith placing of non-content tags such as {{afd}}, {{db}}, {{sprotected}}, or other tags on pages that do not meet such criteria. This includes baseless removal of {{policy}} and related tags.

* Account creation, malicious: Creating accounts with usernames that contain deliberately offensive or disruptive terms is considered vandalism, whether the account is used or not. For Wikipedia's policy on what is considered inappropriate for a username, see Wikipedia:Username policy. See also Wikipedia:Sock puppetry.

* Avoidant vandalism: Removing {{afd}}, {{copyvio}} and other related tags in order to conceal deletion candidates or avert deletion of such content. However, this is often mistakenly done by new users who are unfamiliar with AfD procedures and such users should be given the benefit of the doubt and pointed to the proper page to discuss the issue.

* Blanking, illegitimate
For legitimate cases of blanking articles, see Wikipedia:Redirect § Redirects that replace previous articles.

Removing encyclopedic content without any reason, or replacing such content with nonsense. Content removal is not considered to be vandalism when the reason for the removal of the content is readily apparent by examination of the content itself, or where a non-frivolous explanation for the removal of apparently legitimate content is provided, linked to, or referenced in an edit summary.

Blanking that could be legitimate includes blanking all or part of a biography of a living person. Wikipedia is especially concerned about providing accurate and unbiased information on the living; blanking may be an effort to remove inaccurate or biased material. Due to the possibility of unexplained good-faith content removal, {{uw-test1}} or {{uw-delete1}}, as appropriate, should be used as initial warnings for content removals without more descriptive edit summaries.

* Copyrighted material, repeated uploading of: Uploading or using material on Wikipedia in ways which violate Wikipedia's copyright policies after having been warned is vandalism. Because users may be unaware that the information is copyrighted, or of Wikipedia policies on how such material may and may not be used, such action only becomes vandalism if it continues after the copyrighted nature of the material and relevant policy restricting its use have been communicated to the user.

* Edit summary vandalism: Making offensive edit summaries in an attempt to leave a mark that cannot be easily expunged from the record (edit summaries cannot simply be "reverted" and require administrative action if they have to be removed from a page's history). Often combined with malicious account creation.

* Format vandalism: Changing the formatting of a page unreasonably and maliciously. But many times, editors might just make an unintended mistake or are testing how the wikicode works. Sometimes it might be a bug in the Wikipedia software. Some changes to the format are not vandalism, but rather either good faith edits of editors who don't know the guidelines or simply a different opinion on how the format should look, in which case it is just a disputed edit.

* Gaming the system: Deliberate attempts to circumvent enforcement of Wikipedia policies, guidelines, and procedures by causing bad faith edits to go unnoticed. Includes marking bad faith edits as minor to get less scrutiny, making a minor edit following a bad faith edit so it won't appear on all watchlists, recreating previously deleted bad faith creations under a new title, use of the {{construction}} tag to prevent deletion of a page that would otherwise be a clear candidate for deletion, or use of sock puppets.

* Hidden vandalism: Any form of vandalism that makes use of embedded text, which is not visible to the final rendering of the article but visible during editing. This includes link vandalism, or placing malicious, offensive, or otherwise disruptive or irrelevant messages or spam in hidden comments for editors to see.

* Hoaxing vandalism: Deliberately adding falsities to articles, particularly to biographies of living people, with hoax information is considered vandalism.

* Image vandalism: Uploading shock images, inappropriately placing explicit images on pages, or simply using any image in a way that is disruptive. Please note though that Wikipedia is not censored for the protection of minors and that explicit images may be uploaded and/or placed on pages for legitimate reasons (that is, if they have encyclopedic value).

* Link vandalism: Adding or changing internal or external links on a page to disruptive, irrelevant, or inappropriate targets while disguising them with mislabeling.

* Page creation, illegitimate: Creating new pages with the sole intent of malicious behavior. It also includes personal attack pages (articles written to disparage the subject), hoaxes and other intentionally inaccurate pages. There are many other types of pages that merit deletion, even speedy deletion, but which are not vandalism. New users sometimes create test pages containing nonsense or even autobiographies, and doing so is not vandalism; such pages can also be moved to become their sandbox or userpage. Pages on non-notable topics are not vandalism. Blatant advertising pages, and blatant POV pushes, are not vandalism, but frequently happen and often lead to editors being blocked. It's important that people creating inappropriate pages be given appropriate communication; even if they aren't willing to edit within our rules, they are more likely to go away quietly if they understand why their page has been deleted.

* Page lengthening: Adding very large (measured by the number of bytes) amounts of bad-faith content to a page so as to make the page's load time abnormally long or even make the page impossible to load on some computers without the browser or machine crashing. Adding large amounts of good-faith content is not vandalism, though prior to doing so, one should consider if splitting a long page may be appropriate (see Wikipedia:Article size).

* Page-move vandalism: Changing the names of pages to disruptive, irrelevant, or otherwise inappropriate names. Only autoconfirmed or confirmed users can move pages.

* Silly vandalism: Adding profanity, graffiti, or patent nonsense to pages; creating nonsensical and obviously unencyclopedic pages, etc. It is one of the most common forms of vandalism. However, the addition of random characters to pages is often characteristic of an editing test and, though impermissible, may not be malicious.

* Sneaky vandalism: Vandalism that is harder to spot, or that otherwise circumvents detection, including adding plausible misinformation to articles (such as minor alteration of facts or additions of plausible-sounding hoaxes), hiding vandalism (such as by making two bad edits and only reverting one), simultaneously using multiple accounts or IP addresses to vandalize, abuse of maintenance and deletion templates, or reverting legitimate edits with the intent of hindering the improvement of pages. Impersonating other users by signing an edit with a different username or IP address also constitutes sneaky vandalism, but take care not to confuse this with appropriately correcting an unsigned edit made by another user. Some vandals even follow their vandalism with an edit that states "Rv vandalism" in the edit summary in order to give the appearance the vandalism was reverted.

* Spam external linking: Adding or continuing to add spam external links is vandalism if the activity continues after a warning. A spam external link is one added to a page mainly for the purpose of promoting a website, product or a user's interests rather than to improve the page editorially.

* Stockbroking vandalism: Adding information to pages about quoted companies concerning forthcoming mergers, announcements, and the like. The vandal's intent is to provide credibility to their attempt to promote shares.

* Talk page vandalism: Illegitimately deleting or editing other users' comments. However, it is acceptable to blank comments constituting vandalism, internal spam, or harassment or a personal attack. It is also acceptable to identify an unsigned comment. Users are also permitted to remove comments from their own user talk pages. A policy of prohibiting users from removing warnings from their own talk pages was considered and rejected on the grounds that it would create more issues than it would solve.

* Template vandalism: Modifying the wiki language or text of a template in a harmful or disruptive manner. This is especially serious, because it will negatively impact the appearance of multiple pages. Some templates appear on hundreds or thousands of pages, so they are permanently protected from editing to prevent vandalism.

* User and user talk page vandalism: Unwelcome, illegitimate edits to another person's user page may be considered vandalism. User pages are regarded as within the control of their respective users and generally should not be edited without permission of the user to whom they belong. See WP:UP#OWN. Related is Wikipedia:No personal attacks.

* Vandalbots: A script or "robot" that attempts to vandalize or add spam to a mass of pages."

This is not vandalism:
- boldly editing
- copyright violation
- disruptive editing or stubbornness --> edit warring
- edit summary omission
- editing tests by experimenting users: "Such edits, while prohibited, are treated differently from vandalism"
- harassment or personal attacks: "Personal attacks and harassment are not allowed. While some harassment is also vandalism, such as user page vandalism, or inserting a personal attack into an article, harassment in itself is not vandalism and should be handled differently."
- Incorrect wiki markup and style
- lack of understanding of the purpose of wikipedia: "editing it as if it were a different medium—such as a forum or blog—in a way that it appears as unproductive editing or borderline vandalism to experienced users."
- misinformation, accidental
- NPOV contraventions (Neutral point of view)
- nonsense, accidental: "sometimes honest editors may not have expressed themselves correctly (e.g. there may be an error in the syntax, particularly for Wikipedians who use English as a second language)."
- Policy and guideline pages, good-faith changes to: "If people misjudge consensus, it would not be considered vandalism;"
- Reversion or removal of unencyclopedic material, or of edits covered under the biographies of living persons policy: "Even factually correct material may not belong on Wikipedia, and removing such content when it is not in line with Wikipedia's standards is not vandalism."
- Deletion nominations: "Good-faith nominations of articles (or templates, non-article pages, etc) are not vandalism."

=====================================================================
https://en.wikipedia.org/wiki/Wikipedia:Administrator_intervention_against_vandalism

Notice board;
"This page is intended only for reports about active, obvious, and persistent vandals and spammers."
"Don't forget that blocking is a last resort;"

=====================================================================
https://en.wikipedia.org/wiki/Wikipedia:Disruptive_editing

"Disruptive editing is not vandalism, though vandalism is disruptive."
"Disruptive editing is not always intentional. Editors may be accidentally disruptive because they don't understand how to correctly edit, or because they lack the social skills or competence necessary to work collaboratively "
Okay what are disruptive edits that are not vandalism? (apart from edit wars)

"sometimes attracts people who seek to exploit the site as a platform for pushing a single point of view, original research, advocacy, or self-promotion."
"not verifiable through reliable sources or insisting on giving undue weight to a minority view."

"Collectively, disruptive editors harm Wikipedia by degrading its reliability as a reference source and by exhausting the patience of productive editors who may quit the project in frustration when a disruptive editor continues with impunity."

examples of disruptive editing:
"Engages in "disruptive cite-tagging"; adds unjustified {{citation needed}} tags to an article when the content tagged is already sourced, uses such tags to suggest that properly sourced article content is questionable."
"Rejects or ignores community input: resists moderation and/or requests for comment, continuing to edit in pursuit of a certain point despite an opposing consensus from impartial editors."

=====================================================================
https://en.wikipedia.org/wiki/Wikipedia:WikiBullying

"This is an explanatory supplement to the Wikipedia:Civility and Wikipedia:Ownership of articles policies.
This page is intended to provide additional information about concepts in the page(s) it supplements. This page is not one of Wikipedia's policies or guidelines, as it has not been thoroughly vetted by the community."

"WikiBullying is using Wikipedia to threaten and/or intimidate other people, whether they are Wikipedia editors or not."
"If you feel that you are being bullied or another user has threatened you with bodily harm, it is important that you report them immediately to the Incidents page on the Administrator's Noticeboard so the matter can be properly dealt with."
"All complaints about bullying, even those which turn out to be unjustified should be treated with seriousness and respect, and any WP:BOOMERANG on individuals who have complained they are being bullied is contrary to the principles of respect for thoughtful intellectual discourse that Wikipedia represents. No one should ever fear coming forward to make the community aware of a bullying concern."

"There are essentially two forms of bullying on Wikipedia: attacks against the individual editor by targeting a single user, or giving the perception of power aimed at the entire Wikipedia community at large."

"Forms of WikiBullying:

    1.1 Asserting ownership: "No article on Wikipedia is owned by any editor. Any text that is added to Wikipedia is freely licensed under WP:CC-BY-SA and other users are free to add, remove or modify it at will, provided that such editing is done responsibly."
    1.2 POV Railroading: "Point of View (POV) railroading refers to the use of bullying tactics to discredit an editor with an opposing viewpoint or eliminate them from a discussion."
    1.3 False accusations: "False accusations are a common form of bullying on Wikipedia, although people do sometimes make honest mistakes. Accusations of misconduct made without evidence are considered a serious personal attack."
    1.4 Misrepresentation: "Quoting others out of context and other forms of straw man argument are against the civility policy. Again, try to find out if there has been a misunderstanding."
    1.5 Making "no-edit" orders contrary to policy: "Another form of wikibullying is to issue no-edit orders which are not backed by current policies (or guidelines). A "no-edit" order is a message sent to a single editor (who is not banned) or to the Wikipedia community not to edit at all or in a particular manner, or not to edit a particular page or part of a page at all or in a particular manner. These messages can be sent to a user's talk page, placed on an article's talk page, or in hidden text that would not be missed if an editor attempts to edit the article or section. No editor may unilaterally take charge over an article or part of an article by sending no-edit orders.

There are some no-edit orders that are acceptable. For example, if a consensus has already been formed regarding a topic, and a single editor has constantly stubbornly defied the ruling, politely discussing this one-on-one on the user's talk page is acceptable."
    1.6 Wikihounding: "Wikihounding is the singling out of one or more editors, and joining discussions on multiple pages or topics they may edit or multiple debates where they contribute, to repeatedly confront or inhibit their work. This is with an apparent aim of creating irritation, annoyance or distress to the other editor. Wikihounding usually involves following the target from place to place on Wikipedia."
    1.7 Use of hidden text: "Some unacceptable uses are:

    Telling all other editors not to edit the page
    Telling others not to remove a section of the article, as if the section were written in stone
    Telling others that a page should not be proposed for deletion, when this may be doubted by others
    Writing new guidelines that apply specifically to the page and branding them as "policy." In the past, policies that have been proposed for a single article have failed to attain a consensus."
    1.8 Real life threats: "The Wikimedia Foundation, if need be, will investigate or arrange for law enforcement to investigate threats of violence."
"

============================================================
https://en.wikipedia.org/wiki/Wikipedia:Requests_for_page_protection

"Full protection is used to stop edit warring between multiple users or to prevent vandalism to high-risk templates; semi-protection and pending changes are usually used only to prevent IP and new user vandalism (see the rough guide to semi-protection); and move protection is used to stop pagemove revert wars. Extended confirmed protection is used where semi-protection has proved insufficient (see the rough guide to extended confirmed protection)."

=============================================================
https://en.wikipedia.org/wiki/Wikipedia:Offensive_material

"In original Wikipedia content, a vulgarity or obscenity should either appear in its full form or not at all;"

"A cornerstone of Wikipedia policy is that the project is not censored. Wikipedia editors should not remove material solely because it may be offensive, unpleasant, or unsuitable for some readers. However, this does not mean that Wikipedia should include material simply because it is offensive, nor does it mean that offensive content is exempted from regular inclusion guidelines. "

=============================================================
https://en.wikipedia.org/wiki/Wikipedia:Neutral_point_of_view

"This page in a nutshell: Articles must not take sides, but should explain the sides, fairly and without editorial bias. This applies to both what you say and how you say it."
"This policy is non-negotiable, and the principles upon which it is based cannot be superseded by other policies or guidelines, nor by editor consensus. "

"Achieving what the Wikipedia community understands as neutrality means carefully and critically analyzing a variety of reliable sources and then attempting to convey to the reader the information contained in them fairly, proportionately, and as far as possible without editorial bias"
"Wikipedia aims to describe disputes, but not engage in them."
" Editors, while naturally having their own points of view, should strive in good faith to provide complete information, and not to promote one particular point of view over another. As such, the neutral point of view does not mean exclusion of certain points of view, but including all verifiable points of view which have sufficient due weight."

"As a general rule, do not remove sourced information from the encyclopedia solely on the grounds that it seems biased. Instead, try to rewrite the passage or section to achieve a more neutral tone."

"Remove material only where you have a good reason to believe it misinforms or misleads readers in ways that cannot be addressed by rewriting the passage."

"The best name to use for a topic may depend on the context in which it is mentioned; it may be appropriate to mention alternative names and the controversies over their use, particularly when the topic in question is the main topic being discussed."

"Try not to quote directly from participants engaged in a heated dispute; instead, summarize and present the arguments in an impartial tone."

==============================================================
Filters manual tags evaluation

Following filter categories have been identified (sometimes, a filter was labeled with more than one tag):

- Vandalism
  - hoaxing
  - silly vandalism (e.g. repeating characters, inserting swear words)
  - spam
  - sockpuppetry
  - long term abuse
  - harassment/personal attacks
    - doxxing
    - impersonation
  - trolling
  - copyright violation

  Labeled along the vandalism typology (check above)
  - link vandalism
  - abuse of tags
  - username vandalism
  - image vandalism
  - avoidant vandalism
  - talk page vandalism
  - page move vandalism
  - template vandalism
  - vandalbots

  Kind of similar:
  - seo
  - stockbroker vandalism
  - biased pov
  - self promotion
  - conflict of interest

Inbetween
- edit warring
- political controversy
- politically/religiously motivated hate

- Good faith
  - bad style ("unencyclopedic edits" e.g. citing a blog or mentioning a hypothetical future album release)
  - lazyness


- Maintenance
  - bugs
  - wiki policy (compliance therewith)
  - test filters


A lot of filters are disabled/deleted bc:
* they hit too many false positives
* they were implemented to target specific incidents and these vandalism attempts stopped
* they were tested and merged into other filters
* there were too few hits and the conditions were too expensive

Multiple filters have the comment "let's see whether this hits something", which brings us to the conclusion that edit filter editors have the right and do implement filters they consider necessary

=============================================================
Grounded Theory~\cite{Charmaz2006}

"This book provides \textit{a} way of doing grounded theory" (p.9)

Preface
"At each phase of the research journey, \textit{your} reasings of your work guide your next moves."(p.xi)
"In short, the finished work is a construction–yours." (p.xi)

Chapter 1
"we build levels of abstraction directly from the data" (p.3)

"Glaser and Strauss aimed to move qualitative inquiry beyond descriptive studies into the reals of explanatory theoretical frameworks,"(p.6)

Criteria:
"a completed grounded theory met the following criteria: a close fit with the data, usefulness, conceptual density, durability over time, modifiability, and explanatory power." (p.6)

"assumed that process, not structure, was fundamental to human existence" (p.7)
"A process consists of unfolding temporal sequences that may have identifiable markers with clear beginnings and endings and benchmarks in between. [...] Thus, single events become linked as part of a larger whole." (p.10)

"we are part of the world we study and the data we collect. We \textit{construct} our grounded theories through our past and present involvements and interactions with people, perspectives, and research practices."(p.10)
"My approach explicitely assumes that any theoretical rendering offers and \textit{interpretive} portrayal of the studied world, not an exact picture of it." (p.10)

"I advocate gathering rich–detailed and full–data and placing them in their relevant situational and social contexts." (p.10-11) // cooking data with care

Chapter 2:
"What do you want to study? Which research problem might you pursue? [...] How do you use methods to gather rich data?" (p.13)
"Obtaining rich data means seeking 'thick' description (Geertz, 1973)" (p.14)

"we first aim to see this world as our research participants do–from the inside."(p.14)
"You might learn that what outsiders assume abouth the world you study may be limited, imprecise, mistaen, or egregiously wrong."(p.14)

"\texit{How} you collect data affects \texit{which} phenomena yo will see, \textit{how}, \textit{where}, and \textit{when} you will view them, and \textit{what} sense you will make of them." (p.15)

"We are not scientific obeservers who can dismiss scrutiny of our values by claiming scientific neutrality and authority." (p.15)

"grounded theorists often begin their studies with certain research interests and a set of general concepts" (p.16)
"need to remain as open as possible to whatever we see" (p.17)
"We do not force preconceived ideas and theories directly upon our data." (p.17)
"The quality–and credibility–of your study starts with the data." (p.18)
"Skimpy data may give you a wonderful start but do not add up to a detailed study or a nuanced grounded theory." (p.18)
"What kind of data stands as rich and sufficient?:
* Have I collected enough background data about persons, processes, and settings to have ready recall and to understand and portray the full range of contexts of the study? // what actors are there: admins; editors (good faith/vandals); edit filter managers (how do I become a member of this group?); people requesting an edit filter
* Have I gained detailed descriptions of a range of participants' views and actions? // I have got traces.. ; TODO maybe look for which filters have a discussion/filter request/sock puppet investigation linked to them in the comments; maybe also conduct IVs?
* Do the data reveal what lies beneath the surface?
* Are the data sufficient to reveal changes over time? // TODO: get hold of the log table!!!!
* Have I gained multiple views of participants' range of actions? // + filter actions! (filters are also actors according to ANT)
* Have I gathered data that enable me to develop analytic categories?
* What kinds of comparisons can I make between data? How do these comparisons generate and inform my ideas?
" (p.18-19)

"We demonstrate our respect by making concerted efforts to learn about their views and actions and to try to understand their lives from their perspectives." (p.19)
"we must test our assumptions about the worlds we study, not unwittingly reproduce these assumptions."(p.19)
"It means discovering what our research participants take for granted or do not state as well as what they say and do."(p.19)
"We try to understand but do not ncessarily adopt or reproduce their views as our own."(p.19)

starting questions:
"
* What's happening here?
* What are the basic social processes?
* What are the basic social psychological processes" (p.20)

"Everything may seem significant–or trivial."(p.20)
TODO: Look for this in potential IVs:
"
* From whose point of view is a given process fundamental? From whose is it marginal?
* How do the observed social processes emerge? How do participants' actions construct them?
* Who exerts control over these processes? Under what conditions?
* What meanings do different participants attribute to the process? How do they talk about it? What do they emphasize? What do they leave out?
* How and when do their meanings and actions concerning the process change?
"(p.20)
"Do they provide an idealized picture wrapped in a public relations rhetoric" (p.20)
"When does a basic social process become visible or change?"(p.20)

"Actions may defy stated intentions. Different participants have different vantage points–and, sometimes, competing agendas. Do they realize they hold competing agendas? How do they act on them? When, if ever, does conflict emerge?"

TODO: Look for people who have triggered smth repeatedly. Could it be good faith? Or were they testing? What happened afterwards?

Field notes in GT:
"
* record individual and collective action
* contain full, detailed notes with anecdotes and observations
* emphasize significant processes occurring in the setting
* address what participants define as interesting and/or problematic
* attend to participants' language use
* place actors and actions in scenes and contexts
* become progressively focused on key analytic ideas
" (p.22)

TODO: show the actions and process that construct the topic

"show how people move through the organization–or are moved through it" (p.23)
"seeing data everywhere and nowhere" (p.23)

GT:
1) compare data from the beinning of the research
2) compare data with emerging categories
3) demonstrate relations between concepts and categories (p.23)

TODO: answer following questions (p.24)
"
* What is the setting of action? When and how does action take place?
* What is going on? What is the overall activity being studied, the relatively long-term behavior about which participants organize themselves? What specific acts comprise this activity? --> maintaining a community-sources encyclopedia?
* What is the distribution of participants over space and time in these locales?
* How are actors [research participants] organized? What organizations effect, oversee, regulate or promote this activity?
* How are members stratified? Who is ostensibly in charge? Does being in charge vary by activity? How is membership achieved and maintained?
* What do actory pay attention to? What is important, preoccupying, critical?
* What do they pointedly ignore that other persons might pay attention to?
* What symbols do actors invoke to understand their worlds, the participants and processes whithin them, and the objects and events they encounter? What names do they attach to objects, events, persons, roles, settings, equipment?
* What practices, skills, strategems, methods of operation do actors employ?
* Which theories, motives, excuses, justifications or other explanations do actors use in accounting for their participation? How do they explain to each other, not to outside investigators, what they do and why they do it?
* What goals do actors seek? When, form their perspective, is an act well or poorly done? How do they judge action–by what standards, developed and applied by whom?
* What rewards do various actors gain from their participation?"

"intensive intervie fosters eliciting each participant's interpretation of his or her experience"(p.25)

"Researchers treat extant texts \textit{as} data to address their research questions although these texts were produced for other–often very different–purposes." (p.35)
"As acounts, texts tell something of intent and have intended–and perhaps unintended–audiences."(p.35)

"interview respondents may wish to appear affable, intelligent, or politically correct and thus shape their responses accordingly" (p.36)

"search for reasons for disparities between observed realities and written responses"(p.36)

additional types of data we can use:
public records, government reports, organizational documents, mass media, literature, autobiographies, personal correspondence, Internet discussions, and earlier qualitative materials from data banks.

TODO: Answer for myself:
"
* What are the parameters of the information?
* On what and whose facts does this information rest?
* What does the information mean to various participants or actors in the scene?
* What does the information leave out?
* Who has access to facts, records, or sources of the information?
* Who is the inteded audience for the information?
* Who benefits from shaping and/or interpreting this information in a particular way?
* How, if at, all does the information affect actions?
"(p.37-38)

"To the extent possible, we need to situate texts in their contexts." (p.39)
"Where do the data come from? Who participated in shaping them? What did the authors intend? Have participants provided sufficient information for us to make a plausible interpretation? And do we have sufficient knowledge of the relevant worlds to read their words with any understanding?"(p.39)
"Much textual analysis is without context, or worse, out of context. [...] Providing a description of the times, actors, and issues gives you a start. Multiple methods help, such as intervieweing key participants, and using several types of documents also helps." (p.39)

TODO: Questions to ask of a text (p.39-40):
"
* How was the text produced? By whom?
* What is the ostensible purpose of the text? Might the text serve other unstated or assumed purposes? Which ones?
* How does the text represent what its author(s) assumed to exist? Which meanings are embedded within it? How do those meanings reflect a particular social, historica, and perhaps organizational context?
* What is the structure of the text?
* How does its structure shape what is said? Which categories can you discern in its structure? What can you glean from these categories? Do the categories change in sequential texts over time? How so?
* Which contextual meanings does the text imply?
* How does its content construct images of reality?
* Which realities does the text claim to represent? How does it represent them?
* What, if any, unintended information and meanings might you see in the text?
* How is language used?
* Which rules govern the constructuion of the text? How can you discern them in the narrative? How do these rules reflect both tacit assumptions and explicit meanings? How might they be related to other data on the same topic?
* When and how do telling points emerge in the text?
* What kinds of comparisons can you make between texts? Between different texts on the same topic? Similar texts at different times such as organizational annual reports? Between different authors who address the same questions?
* Who benefits from the text? Why?
"
# Coding in GT

"Grounded theory coding consists of at least two phases: initial and
focused coding." (p.42)

"From time to time, we may adopt our participants' telling
terms as in vivo codes."(p.42)

"During initial coding we study fragments of data-
words, lines, segments, and incidents-closely for their analytic
import."
"While engaging in focused coding, we select
what seem to be the most useful initial codes and test them against
extensive data."(p.42)

"Coding means naming segments of data with a label that simultaneously categorizes, summarizes, and accounts for each piece of data" (p.43)
"first step in moving beyond concrete statements in the data to making analytic interpretations." (p.43)

"codes stick closely to the data, show actions, and indicate how
dilemmas surrounding disclosure arise." (p.45)

"Coding is the pivotal link between collect-
ing data and developing an emergent theory
to explain these data." (p.46)

"The logic of grounded theory coding differs from quantitative logic that
applies preconceived categories or codes to the data."(p.46)

"Language plays a crucial role"(p.46)
"Specific use of language reflects views and values." (p.47)

"Coding impels us to make our participants' language
problematic to render an analysis of it. Coding should inspire us to examine
hidden assumptions in our own use of language as well as that of our participants." (p.47)

"we try to understand participants' views and actions from their perspectives." (p.47)

Initial coding questions:
"• 'What is this data a study of?' (Glaser, 1978: 57; Glaser & Strauss, 1967)
• What does the data suggest? Pronounce?
• From whose point of view?
• What theoretical category does this specific datum indicate? (Glaser, 1978)" (p.47)

"Try to see actions in each segment of data rather than applying preexisting categories to the data." (p.47)
"Attempt to code with words that reflect action." (p.47-48)

"Initial grounded theory coding can prompt you to see areas in which you lack needed data." (p.48)

active coding -> use gerunds
"We gain a strong sense of action and sequence with gerunds." (p.49)

"If you ignore, gloss over, or leap beyond participants'
meanings and actions, your grounded theory will likely reflect an outsider's,
rather than an insider's view." (p.49)
"Outsiders often import an alien professional lan-
guage to describe the phenomenon." (p.49)

"Make your codes fit the data
you have rather than forcing the data to fit them." (p.49)

To do while coding:
"
Remain open
Stay close to the data
Keep your codes simple and precise
Construct short codes
Preserve actions
Compare data with data
Move quickly through the data.
"

"Fresh data and line-by-line coding prompt you to remain open to the data
and to see nuances in it"(p.50)

Being critical:
"Line-by-line coding frees you from becoming so immersed in your respon-
dents' worldviews that you accept them without question. Then you fail to look
at your data critically and analytically. Being critical about your data does not
necessarily mean being critical of your research participants. Instead, being
critical forces asking yourself questions about your data." (p. 51)

in vivo codes: "codes of participants' special terms"(p.55)
"useful analytic point of departure" (p.55)
"preserve participants' meanings of their views and actions" (p.55)

3 kinds of useful in vivo codes:
"
* Those general terms everyone 'knows' that flag condensed but significant
meanings
* A participant's innovative term that captures meanings or experience
* Insider shorthand terms specific to a particular group that reflect their
perspective.
" (p.55)

"Pursue telling terms" (p.57)

Focused Coding:
"using the most significant and/or frequent earlier codes to sift through large amounts of data"
"which initial codes make the most analytic sense" (p.57)

Axial Coding:
"relates categories to subcategories, specifies the properties and dimensions of a category" (p.60)

Theoretical coding:
"Glaser (1978: 72) introduced theoretical
codes as conceptualizing 'how the substantive codes may relate to each other
as hypotheses to be integrated into a theory.' In short, theoretical codes specify
possible relationships between categories you have developed in your focused
coding."(p.63)

"When your analysis indicates, use theoretical codes to help you clarify and
sharpen your analysis but avoid imposing a forced framework on it with them."
"interrogate yourself about whether these theoretical codes interpret all the data" (p.66)

"each preconceived idea should earn
its way into your analysis-including your own ideas from previous studies"(p.68)

TODO: Check against my analysis:
"Be careful about applying a language of intention,
motivation, or strategies unless the data support your assertions. You cannot assume
what is in someone' s mind-particularly if he or she does not tell you."(p.68)

"Take an examined stance about whose point of view your codes reflect,"(p.69)

================================================================
https://en.wikipedia.org/w/api.php?action=help&modules=main

action

    Which action to perform.

    abusefiltercheckmatch
        Check to see if an AbuseFilter matches a set of variables, an edit, or a logged AbuseFilter event.
    abusefilterchecksyntax
        Check syntax of an AbuseFilter filter.
    abusefilterevalexpression
        Evaluates an AbuseFilter expression.
    abusefilterunblockautopromote
        Unblocks a user from receiving autopromotions due to an abusefilter consequence.

================================================================
https://en.wikipedia.org/wiki/Wikipedia:Database_download


================================================================
https://stats.wikimedia.org/v2

To generate stats for different wiki projects

=====================================================================
Claudia: * A focus on the Good faith policies/guidelines is a historical development. After the huge surge in edits Wikipedia experienced starting 2005 the community needed a means to handle these (and the proportional amount of vandalism). They opted for automatisation. Automated system branded a lot of good faith edits as vandalism, which drove new comers away. A policy focus on good faith is part of the intentions to fix this.

====================================================================
\cite{Lessig2006}

"A locked door is not a command “do not enter”
backed up with the threat of punishment by the state. A locked door is a
physical constraint on the liberty of someone to enter some space.
My claim is that this form of regulation will become increasingly com-
mon in cyberspace."(p.82)

"Code is a regulator in cyberspace because it defines the terms upon which cyberspace is offered." (p.84)

"Communities,
exchange, and conversation all flourish in a certain type of space; they are
extinguished in a different type of space." (p.85)
"Spaces have values. 6 They manifest these values through the practices or lives
that they enable or disable."(p.85)

narrow bandwidth and text-centered communication
"Most think of this fact about the early Net as a limitation. Technically, it
was. But this technical description does not exhaust its normative description
as an architecture that made possible a certain kind of life. From this perspec-
tive, limitations can be features; they can enable as well as disable. And this
particular limitation enabled classes of people who were disabled in real-
space life." (p.86)

"Every time AOL decides that it
wants to regulate a certain kind of behavior, it must select from among at least
four modalities—rules, norms, prices, or architecture." (p.94)

"Norms become different when ballots can overrule them," (p.102)

// TODO vgl Wikipedia!
"politics [is] implemented through technology.” 41 Differ-
ences in the code constitute them differently, but some code makes community
thicker than others. Where community is thick, norms can regulate."(p.102)

"End-to-end is a para-
digm for technology that embeds values. Which architecture we encourage is
a choice about which policy we encourage." (p.112)
// TODO: What values are embedded in Wikipedia's architecture? In the architecture of the edit filter system?

"In places where community is not fully self-enforcing, norms are supple-
mented by rules imposed either through code or by the relevant sovereign." (p.113)

"They have the right to
exit, but in the sense that Soviet citizens had the right to exit—namely, with
none of the assets they had built in their particular world." (p.113)

quoting Seconf Life CEO's
"[O]ur feeling is . . . that we should aggressively move into code anything we can,
because of the enhanced scalability it gives us. And we should execute policy out-
side of code only when absolutely necessary or unfeasible." (p.114)
"If Second Life can use code to better control behavior, what about first-life?" (p.114)

"Individuals can act
to resist the force of code directly. Or individuals can act to resist the force of
code through code." (p.118)

quoting Tim Wu
"The reason that code matters for law at all is its capability to define behavior on
a mass scale." (p.119)
"In this second sense, code functions “as an anti-regulatory mechanism: a
tool to minimize the costs of law that certain groups will use to their advan-
tage.” " (p.119)

Chapter 7: things are regulated by; laws, norms, market, technology

"Norms constrain through the
stigma that a community imposes; markets constrain through the price that
they exact; architectures constrain through the physical burdens they impose;
and law constrains through the punishment it threatens." (p.124)

Indirection and accountability
"Indirection misdirects responsibility. When a government uses other
structures of constraint to effect a constraint it could impose directly, it mud-
dies the responsibility for that constraint and so undermines political
accountability." (p.133)

=====================================================================

Toolforge links:
https://wikitech.wikimedia.org/wiki/Help:Toolforge#Troubleshooting_2
https://wikitech.wikimedia.org/wiki/Help:Toolforge#Contact
https://wikitech.wikimedia.org/wiki/Analytics#Contact
https://www.mediawiki.org/wiki/Toolserver:Main_Page

Abuse Filter Project on Phabricator
https://phabricator.wikimedia.org/project/view/217/

Abuse Filter git repo
https://gerrit.wikimedia.org/r/#/c/mediawiki/extensions/AbuseFilter/+/489705/3/includes/AbuseFilter.php

Evidence abuse filter history still exists
https://en.wikipedia.org/wiki/Special:AbuseFilter/history

====================================================================

Get hold of abuse_filter_history

https://gerrit.wikimedia.org/r/plugins/gitiles/operations/puppet/+/refs/heads/production/modules/profile/templates/labs/db/views/maintain-views.yaml
https://gerrit.wikimedia.org/r/plugins/gitiles/mediawiki/extensions/AbuseFilter/+/refs/heads/master/abusefilter.tables.sql#63
https://phabricator.wikimedia.org/T123978

"
there'll be such a table, I don't think there's a view for it to expose it to the public
it's not in https://gerrit.wikimedia.org/r/plugins/gitiles/operations/puppet/+/refs/heads/production/modules/profile/templates/labs/db/views/maintain-views.yaml
I don't know enough about AbuseFilter to know how easy it would be to write a view for this table
https://gerrit.wikimedia.org/r/plugins/gitiles/mediawiki/extensions/AbuseFilter/+/refs/heads/master/abusefilter.tables.sql#63
you'd probably need to respect afh_deleted, and join against abuse_filter to check filter visibility?
in order to expose a view?
aha: https://phabricator.wikimedia.org/T123978
yes
"

===================================================
https://en.wikipedia.org/wiki/Wikipedia:No_original_research

"Wikipedia articles must not contain original research. The phrase "original research" (OR) is used on Wikipedia to refer to material—such as facts, allegations, and ideas—for which no reliable, published sources exist.[a]"
"(This policy of no original research does not apply to talk pages and other pages which evaluate article content and sources, such as deletion discussions or policy noticeboards.) "
"The prohibition against OR means that all material added to articles must be attributable to a reliable, published source, even if not actually attributed.[a]"
