\chapter{Discussion}
\label{chap:discussion}

I started this inquiry with following questions:\\ %TODO  either number the questions with Qx from the beginning and use it consistently or leave it be altogether
Q1: What is the role of edit filters among existing algorithmic quality-control mechanisms on Wikipedia (bots, semi-automated tools, ORES, humans)?\\
%-- chapter 4 (and 2)
Q2: Edit filters are a classical rule-based system. Why are they still active today when more sophisticated ML approaches exist?\\
%-- chapter 6 (discussion)
Q3: Which type of tasks do filters take over?\\ %-- chapter 5
Q4: How have these tasks evolved over time (are they changes in the type, number, etc.)? %-- chapter 5 (can be significantly expanded)

In what follows, I go over each of them and summarise the findings.

\section{Q1 What is the role of edit filters among existing quality-control mechanisms on Wikipedia (bots, semi-automated tools, ORES, humans)?}

When edit filters were introduced in 2009, various other mechanisms that took care of quality control on Wikipedia had already been in place for some time.
However, the community felt the need for an agent (mechanism, syn) preventing obvious but pervasive and difficult to clean up vandalism as early as possible.
This was supposed to take workload off the other mechanisms along the quality control process (syn) (see figure~\ref{funnel}), especially off human editors who could then use their time more productively elsewhere, namely to check less obvious (syn) cases.
%TODO is there another important findind from chapter 4's conclusion that is missing here?

It seems obvious/natural/... to compare the edit filters, being a competely automated mechanism, with bots.
What did the filters accomplish differently?
% before vs after
A key distinction is that while bots check already published edits which they eventually may decide to revert, filters are triggered before an edit ever published.
One may argue that nowadays this is not a significant difference.
Whether a disruptive edit is outright disallowed or caught 2 seconds after its publication by ClueBot NG doesn't have a tremendous impact on the readers:
the vast majority of them will never see the edit either way.
Still, there are various examples of hoaxes that didn't survive long on Wikipedia but the couple of seconds before they were reverted were sufficient for the corrupted version to be indexed by various/multiple/... news aggregators and search engines. %TODO find them!

% The infrastructure question: Part of the software vs externally run
Another difference between bots and filters underlined several times in community discussions was that as a MediaWiki extension edit filters are part of the core software whereas bots are running on external infrastructure which makes them both slower and generally less reliable.
(Compare Geiger's account about running a bot on a workstation in his apartment which he simply pulled the plug on when he was moving out~\cite{Geiger2014}.)
Nowadays, we can ask ourselves whether this is still of significance:
A lot of bots are run on Toolforge~\cite{Wikimedia:Toolforge}, a cloud service providing a hosting environment for a variety of applications (bots, analytics, etc.) run by volunteers who work on Wikimedia projects.
The service is maintained by the Wikimedia Foundation the same way the Wikipedia servers are, so in consequence just as reliable and available as the encyclopedia itself.
The argument that someone powered off the basement computer on which they were running bot X is just not as relevant anymore.

% general discussion on "platform" and what the metaphor hides? (e.g. bot develorpers' frustration that their work is rendered invisible?)

% more on bots vs filters
% collaboration possible on filters?
% who edits filters (edit filter managers, above all trusted admins) and who edits bots (in theory anyone approved by the BAG)
Above all the distinction of bots vs filters: what tasks are handled by which mechanism and why? slides (syn!) into the foreground over and over aagain.
After all the investigations I would venture the claim that from end result perspective it probably doesn't make a terrible difference at all.
As mentioned in the paragraph above, whether malicious content is directly disallowed or reverted 2 seconds later (in which time probably who 3 user have seen it, or not) is hardly a qualitative difference for Wikipedia's readers. %TODO (although I'm making a slightly different point in the paragraph above, clean up!)
I would argue though that there are other stakeholders for whom the choice of mechanism makes a bigger difference:
the operators of the quality control mechanisms and the users whose edits are being targeted.
The difference (syn!) for edit filter managers vs bot developers is that the architecture of the edit filter plugin supposedly fosters collaboration which results in a better system (compare with the famous ``given enough eyeballs, all bugs are shallow''~\cite{Raymond1999}).
Any edit filter manager can modify a filter causing problems and the development of a single filter is mostly a collaborative (syn!) process.
Just a view on the history of most filters reveals that they have been updated multiple times by various users.
In contrast, bots' source code is often not publicly available and they are mostly run by one operator only, so no real peer review of the code is practiced and the community has time and again complained of unresponsive bot operators in emergency cases.
(On the other hand, more and more bots are based on code from various bot development frameworks such as pywikibot~\cite{pywikibot}, so this is not completely valid either.)
On the other hand, it seems far more difficult/restrictive to become an edit filter manager: there are only very few of them, the vast majority admins or in exceptional cases very trusted users.
A bot operator on the other hand (syn) only needs an approval by the BAG and can get going.

The choice of mechanism also makes a difference for the editor whose edits have been deemed disruptive.
Filters assuming good faith seek communication with the editor by issuing warnings which provide some feedback for the editor and allow them to modify their edit (hopefully in a constructive fashion) and publish it again.
Bots on the other hand (syn) simply revert everything their algorithms find malicious. %TODO in theory a bot can be programmed to leave a message on the user's talk page. It is still a revert first-ask questions later approach
In case of good faith edits, this would mean that an editor wishing to dispute this decision should open a discussion (on the bot's talk page?) and research has shown that attempts to initiate discussions with (semi-)automated quality control agents have in general quite poor response rates ~\cite{HalGeiMorRied2013}.
% that's positive! editors get immmediate feedback and can adjust their (good faith) edit and publish it! which is psychologically better than publish something and have it reverted in 2 days

%TODO Fazit?

\section{Q2: Edit filters are a classical rule-based system. Why are they still active today when more sophisticated ML approaches exist?}
%* What can we filter with a REGEX? And what not? Are regexes the suitable technology for the means the community is trying to achieve?

Research has long demonstrated higher precision and better results of machine learning methods. %TODO find quotes!
Several explanations of this phenomenon come to mind.
For one, Wikipedia's edit filters are an established system which works and does its work reasonably well, so there is no need (syn) to change it. (``never touch a running system)
It has been organically weaven in Wikipedia's quality control ecosystem with historical needs to which it responded and people at the time believing the mechanism to be the right solution to the problem they had.
We could ask why was it introduced in the first place when there were already other mechanisms (and possibly already the first ML based bots) %TODO check timeline
A very plausible explanation here is that since Wikipedia is a volunteer project a lot of stuff probably happens because at some precise moment there are particular people who are familiar with some concrete technologies so they construct a solution using the technologies they are good at using (or want to use).

Another interesting reflection is that rule based systems are arguably easier to implement and above all to understand by humans which is why they still enjoy popularity today.
On the one hand, overall less technical knowledge is needed in orderto implement a single filter:
An edit filter manager has to ``merely'' understand regular expressions.
Bot development on the other hand (syn!) is a little more challenging:
A developer needs resonable knowledge of at least one programming language and on top of that has to make themself familiar with stuff like the Wikimedia API, ....
Moreover, since regular expressions are still somewhat human readable and understandable (syn!) in contrast to a lot of popular machine learning algorithms, it is easier to hold rule based systems and their developers accountable.

Filters are a simple mechanism (simple to implement) that swiftly takes care of cases that are simple to recognise as undesirable.
ML needs training data (expensive), it's not simple to implement.


\begin{comment}
maybe it's a historical phenomenon (in many regards):
* perhaps there were differences that are not essential anymore, such as:
  * on which infrastructure does it run (part of the core software vs own computers of the bot operators)
  * filters are triggered *before* an edit is even published, whereas bots (and tools) can revert an edit post factum. Is this really an important difference in times when bots need a couple of seconds to revert an edit?
* perhaps the extension was implemented because someone was capable of implementing and working well with this type of systems so they just went and did it (do-ocracy; Wikipedia as a collaborative volunteer project);
* perhaps it still exists in times of fancier machine learning based tools (or bots) because rule-based systems are more transparent/easily understandable for humans and writing a regex is simpler than coding a bot.
* hypothesis: it is easier to set up a filter than program a bot. Setting up a filter requires "only" understanding of regular expressions. Programming a bot requires knowledge of a programming language and understanding of the API.
\end{comment}

%TODO do something with this
\begin{comment}
Interestingly, there was a guideline somewhere stating that no trivial formatting mistakes should trip filters\cite{Wikipedia:EditFilterRequested}
%TODO (what exactly are trivial formatting mistakes? starting every paragraph with a small letter; or is this orthography and trivial formatting mistakes references only Wiki syntax? I think though they are similar in scale and impact)
I actually think, a bot fixing this would be more appropriate.
## Open questions

If discerning motivation is difficult, and, we want to achieve different results, depending on the motivation, that lead us to the question whether filtering is the proper mechanism to deal with disruptive edits.

\end{comment}

\section{Q3: Which type of tasks do filters take over?}

\section{Q4: How have these tasks evolved over time (are they changes in the type, number, etc.)?}



% censorship infrastructure concerns: maybe discuss in the conclusion

% think about what values we embed in what systems and how; --> Lessig
\begin{comment}
Alternative approaches to community management:
compare with Surviving the Eternal September paper~\cite{KieMonHill2016}
"importance of strong
systems of norm enforcement made possible by leadership,
community engagement, and technology."

"emphasizing decentralized moderation" //all community members help enforce the norms
"ensuring enough leadership capacity is available
when an influx of newcomers is anticipated."
"Designers may
benefit by focusing on tools to let existing leaders bring others
on board and help them clearly communicate norms."
"designers should support an ecosystem of accessible and ap-
propriate moderator tools."

\end{comment}

% TODO also comment on negative results! (what negative results do I have?)

% TODO comment on: so what's the role of the filters, why were they introduced (to get over with obvious persistent vandalism which was difficult to clean up, most probably automated) -- are they fulfilling this purpose?

%***************************************
\cite{GeiRib2010}

"these tools make certain pathways of action easier for vandal
fighters and others harder"

"Ultimately, these tools take their users
through stan dardized scripts of action in which it always
possible to act otherwise, but such deviations demand
inventiveness and time."

\begin{comment}
Use as an argument in favor of filter came to be this way organically
\url{http://www.aaronsw.com/weblog/whorunswikipedia}
"But what’s less well-known is that it’s also the site that anyone can run. The vandals aren’t stopped because someone is in charge of stopping them; it was simply something people started doing. And it’s not just vandalism: a “welcoming committee” says hi to every new user, a “cleanup taskforce” goes around doing factchecking. The site’s rules are made by rough consensus. Even the servers are largely run this way — a group of volunteer sysadmins hang out on IRC, keeping an eye on things. Until quite recently, the Foundation that supposedly runs Wikipedia had no actual employees.
This is so unusual, we don’t even have a word for it. It’s tempting to say “democracy”, but that’s woefully inadequate. Wikipedia doesn’t hold a vote and elect someone to be in charge of vandal-fighting. Indeed, “Wikipedia” doesn’t do anything at all. Someone simply sees that there are vandals to be fought and steps up to do the job."
//yeah, I'd call it "do-ocracy"

\end{comment}

%***************************************
\begin{comment}

* Till now the whole inquiry is largely descriptive. It's fine the status quo is captured but then we should go a step further and ask "so what"? What do we have from that? Explain the data
  * maybe we won't be able to explain a lot of it and we can open it further as interesting questions to be looked into by ethnographers


* think about what values we embed in what systems and how; --> Lessig

Difference bot/filter: filters are part of the "platform". (vgl also ~\cite{Geiger2014} and criticism towards the view of a hollistic platform)
They are a MediaWiki extension, which means they are run on official Wikimedia infrastructure. (vgl \cite{Geiger2014} and "bespoke code")
This makes them more robust and bestow them another kind of status.
Bots on the other hand are what Stuart Geiger calls "bespoke code": they are auxiliary programms developed, mantained and run by single community members, typically (at least historically?) not on Wikimedia's infrastructure, but instead on private computers or third party servers.
Is this difference really significant nowadays though? A lot of bots are run on the toolserver which makes the "not server-side" distinction really difficult.
The toolserver is yet another infrastructure run and maintained by the Wikimedia foundation.
So arguments such as reduced reliability through running on a private machine in a person's living room become kind of obsolete.

\cite{Geiger2014}
"What if, from the beginning, I had decided to run my bot on the toolserver, a
shared server funded and maintained by a group of German Wikipedians for all kinds of pur-
poses, including bots? If so, the bot may have run the same code in the same way, producing
the same effects in Wikipedia, but it would have been a different thing entirely."
"when life got in the way, it was something I literally pulled the plug on
without so much as a second thought."

* another difference bots/filters, it's easier to ddos the bot infrastructure, than the filters: buy a cluster and edit till the revert table overflows -- mh. I can also edit till the AbuseLog overflows...

* why get certain filters (and not others?)
* do filters solve effectively the task they were conjured up to life to fulfil?
* what kinds of biases/problems are there?

Claudia: * A focus on the Good faith policies/guidelines is a historical development. After the huge surge in edits Wikipedia experienced starting 2005 the community needed a means to handle these (and the proportional amount of vandalism). They opted for automatisation. Automated system branded a lot of good faith edits as vandalism, which drove new comers away. A policy focus on good faith is part of the intentions to fix this.

 could be that the high hit count was made by false positives, which will have led to disabling the filter (TODO: that's a very interesting question actually; how do we know the high number of hits were actually leggit problems the filter wanted to catch and no false positives?)
--  we can't really? unless we study the edits themselves; I did this exemplarily for edits from the peak period in 2016; they were not false positives but a big spam wave.
\end{comment}

% Ethical discussion: open science vs thick descriptions which put individuals on the spot

%***************************************

\section{Limitations}

This work presents a first attempt at analysing Wikipedia's edit filter system.
Several limitations of this study come to mind.
Firstly, it focuses on English Wikipedia only.
This presents (syn!) an excellent starting point for analysis of the edit filter system, since this was also the first language version to which the mechanism was introduced.
However, valuable lessons can be learnt (about the communities, models of governance, usefulness of filters, etc.) from comparing edit filter use across different language versions.
Just recall, how for instance the role of edit filter managers doesn't exist in certain language versions (comapare chapter~\ref{}) and instead it is administrators who have an \emph{abusefilter-modify} permission next to their other rights.

Secondly, unfortunately, conducting a classical ethnographic analysis was not possible.
It would have been particularly insightful to talk to edit filter managers (above all such who are simultaneously also bot operators) and developers of the extension, as well as regular editors who have tripped a filter.
This is partially due to the fact that we employ a computer science perspective on the question and partially due to limited time.
I really only used ``found data'' (compare~\ref{sec:trace-ethnography}) (well I also attempted to interpret the found data and link it) and future studies can and should use the first insights of the current research as interview prompts

Thirdly, the manual filter classification was undertaken by one person only (me), so biases of this person have certainly shaped the labels.

Fourth, edit filter history table was not available, so no hollistic quantitative analysis of the filters' development over time was possible.

Fifth, no access to the details of hidden filters, so no insights into the areas they target (although couple of educated guesses: bunch of persistent long term vandal, who often employ sockpuppets; harassment/personal attack cases hidden to protect the affected persons)

%Data
Following other pages looked interesting or related, but were left out, mainly because of insufficient time.
(Is there a better reasoning why I looked at the pages I looked at specifically, while left particularly these other pages for later?)

%TODO Discuss ethical concerns of thickening traces and its clash with open science aspiration here?
%********************
% Filters vs bots
% Investigation of edit filter managers who are also bot operators: what do they implement when?
\begin{comment}
Question:
Oftentimes edit filter managers are also bot operators; how would they decide when to implement a filter and when a bot?
%TODO: ask people! (on IRC?)
I've compiled a list of edit filter managers who are simultaneously also bot operators;
I've further assembled the bots they run and made notes on the bots that seem to be relevant to vandalism prevention/quality assurance
I'm currently trying to determine from document traces what filter contributions the corresponding edit filter managers had and whether they are working on filters similar to the bots they operate.
Insight is currently minimal, since abuse\_filter\_history table is not available and we can only determine what filters an edit filter manager has worked on from limited traces such as: last modifier of the filter from abuse\_filter table; editors who signed their comments from abuse\_filter table; probably some noticeboards or talk page archives, but I haven't looked into these so far.
\end{comment}

%************************************************************************

\section{Directions for future studies}
\label{sec:further-studies}

Throughout the thesis, a variety of intriguing questions arose which couldn't be addressed due to various reasons, above all–insufficient time.
Here, a comprehensive list of all these pointers for possible future research is provided.

\begin{enumerate}
    \item \textbf{How have edit filters's tasks evolved over time?}: Unfortunately, no detailed historical analysis of the filters was possible, since the database table storing changes to individual filters (\emph{abuse\_filter\_history}) is not currently replicated (see section~\ref{sec:overview-data}). A patch aiming to renew the replication of the table is currently under review~\cite{gerrit-tables-replication}. When a dump becomes available, an extensive analysis (sym) of filter creation and activation patterns, together with .. will be possible (syn).
        (Actually there is some historical stuff: e.g. temporal overview of hits, broken down by filter action... Beware however, it is the *current* filter action they were plotted with and it is very possible that the corresponding filters had a different action switched on some time ago. %TODO check whether that's actually true
        (or another visibility level, different regex pattern which would've resulted in a different manual tag)
    \item \textbf{What are the differences between how filters are governed on EN Wikipedia compared to other language versions?}: Different Wikipedia language versions each have a local community behind them. %TODO quote?
        These communities vary widely in their modes of organisation, ..., and values. It would be definitely fascinating to explore differences between filter governance (and what typed of filters are applied) between the different languages.
    \item \textbf{Are edit filters a suitable mechanism for fighting harassment?}: Online harassment has been an increasingly important topic since.. %TODO quote ExMachina paper?
        It is also a problem recognised and addressed by Wikimedia/the Wikipedian community %TODO see 2015 Harassment survey; is there a newer one?
        According to the edit filter noticeboard archives~\cite{Wikipedia:EditFilterNoticeboardHarassment} there have been some attempts to combat harassment by means of filters.
        An evaluation of the usefulness and success of the mechanism at this task would be really interesting.
    \item \textbf{When an editor (edit filter manager who is also a bot operator) will implement a bot and when a filter} - ethnographic inquiry
    \item \textbf{Repercussions on affected editors}: What are the consequences of edit filters on editors whose edits are filtered? Frustration? Allienation? Do they understand what is going on? Or are for example edit filter warnings helpful and the editors appreciate the hints they have been given and use them to improve their collaboration?
\begin{comment}
%TODO where to put this?
Users are urged to use the term "vandalism" carefully, since it tends to offend and drive people away.
("When editors are editing in good faith, mislabeling their edits as vandalism makes them less likely to respond to corrective advice or to engage collaboratively during a disagreement,"~\cite{Wikipedia:Vandalism})
There are also various complaints/comments by users bewildered that their edits appear on an ``abuse log''
\end{comment}
    \item \textbf{Is it possible to study the regex patterns in a more systematic fashion? What is to be learnt from this?} For example, it comes to attention that a lot of filters target new users: ``!(""confirmed"" in user\_groups)'' is their first condition%is this really interesting?
    \item \textbf{(How) has the notion of ``vandalism'' on Wikipedia evolved over time?}: By comparing older and newer filters, or respectively updates in filter patterns we could investigate whether there is a qualitative change in the interpretation of the ``vandalism'' notion on Wikipedia.
    \item \textbf{False Positives?}: were filters shut down, bc they matched more False positives than they had real value?
    \item \textbf{What are the urgent situations in which edit filter managers are given the freedom to act as they see fit and ignore best practices of filter adoption (i.e. switch on a filter in log only mode first and announce it on the notice board so others can have a look)? Who determines they are urgent?}: I think these cases should be scrutinised extra carefully since ``urgent situations'' have historically always been an excuse for cuts in civil liberties.
%* is there a qualitative difference between complaints of bots and complaints of filters?
    \item \textbf{Is there a qualitative difference between the tasks/patterns of public and hidden filters?}: We know of one general guideline/rule of a thumb (cite!) according to that general filters are to be public while filters targeting particular users are hidden. Is there something more to be learnt from an actual examination of hidden filters? One will have to request access to them for research purposes, sign an NDA, etc.
    \item \textbf{Do edit filter managers specialize on particular types of filters (e.g. vandalism vs good faith?)} \emph{abuse\_filter\_history } table is needed for this
    \item \textbf{What proportion of quality control work do filters take over?}: compare filter hits with number of all edits and reverts via other quality control mechanisms
    \item \textbf{Do edit filter managers stick to the edit filter guidelines?}: e.g. no trivial problems (such as spelling mistakes) should trigger filters; problems with specific pages are generally better taken care of by protecting the page and problematic title by the title blacklist; general filters shouldn't be hidden
\end{enumerate}
