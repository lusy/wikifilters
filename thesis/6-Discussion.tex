\chapter{Discussion and Limitations}
\label{chap:discussion}

The purpose of this chapter is to reflect upon what we have learnt so far and describe/outline some limitations of the present study.

\section{Discussion}

% Why are there still rule-based systems in ML time?
One urgent question that is still open is:
Why are there still very well established up-and-running rule based systems in times of fancy(syn) machine learning algorithms?
Research has long demonstrated higher precision and better results of machine learning methods. %TODO find quotes!
Several explanations of this phenomenon come to mind.
For one, Wikipedia's edit filters are an established system which works and does its work reasonably well, so there is no need (syn) to change it. (``never touch a running system)
It has been organically weaven in Wikipedia's quality control ecosystem with historical needs to which it responded and people at the time believing the mechanism to be the right solution to the problem they had.
We could ask why was it introduced in the first place when there were already other mechanisms (and possibly already the first ML based bots) %TODO check timeline
A very plausible explanation here is that since Wikipedia is a volunteer project a lot of stuff probably happens because at some precise moment there are particular people who are familiar with some concrete technologies so they construct a solution using the technologies they are good at using (or want to use).

Another interesting reflection is that rule based systems are arguably easier to implement and above all to understand by humans which is why they still enjoy popularity today.
On the one hand, overall less technical knowledge is needed in orderto implement a single filter:
An edit filter manager has to ``merely'' understand regular expressions.
Bot development on the other hand (syn!) is a little more challenging:
A developer needs resonable knowledge of at least one programming language and on top of that has to make themself familiar with stuff like the Wikimedia API, ....
Moreover, since regular expressions are still somewhat human readable and understandable (syn!) in contrast to a lot of popular machine learning algorithms, it is easier to hold rule based systems and their developers accountable.

\begin{comment}
maybe it's a historical phenomenon (in many regards):
* perhaps there were differences that are not essential anymore, such as:
  * on which infrastructure does it run (part of the core software vs own computers of the bot operators)
  * filters are triggered *before* an edit is even published, whereas bots (and tools) can revert an edit post factum. Is this really an important difference in times when bots need a couple of seconds to revert an edit?
* perhaps the extension was implemented because someone was capable of implementing and working well with this type of systems so they just went and did it (do-ocracy; Wikipedia as a collaborative volunteer project);
* perhaps it still exists in times of fancier machine learning based tools (or bots) because rule-based systems are more transparent/easily understandable for humans and writing a regex is simpler than coding a bot.
* hypothesis: it is easier to set up a filter than program a bot. Setting up a filter requires "only" understanding of regular expressions. Programming a bot requires knowledge of a programming language and understanding of the API.
\end{comment}

% The infrastructure question: Part of the software vs externally run
One difference between bots and filters underlined several times was that as a MediaWiki extension edit filters are part of the core software whereas bots are running on external infrastructure which makes them generally less reliable.
Nowadays, we can ask ourselves whether this is a significant difference (syn!) anymore:
a lot of bots are run on the toolserver which is also provided and maintained by the Wikimedia Foundation (the same people/organisation who run the Wikipedia servers), so in consequence just as reliable and available as the encyclopedia itself.
The argument that someone powered off the basement computer on which they were running bot X is just not as relevant anymore.

% general discussion on "platform" and what the metaphor hides? (e.g. bot develorpers' frustration that their work is rendered invisible?)

% before vs after
A key difference is also that while bots check already published edits which they eventually may decide to revert, filters are triggered before an edit ever published.
One may argue that nowadays this is not a significant difference.
Whether a disruptive edit is outright disallowed or caught 2 seconds after its publication by ClueBot NG doesn't have a tremendous impact on the readers:
the vast majority of them will never see the edit either way.
% so?

% more on bots vs filters
Above all the distinction of bots vs filters: what tasks are handled by which mechanism and why? slides (syn!) into the foreground over and over aagain.
After all the investigations I would venture the claim that from end result perspective it probably doesn't make a terrible difference at all.
As mentioned in the paragraph above, whether malicious content is directly disallowed or reverted 2 seconds later (in which time probably who 3 user have seen it, or not) is hardly a qualitative difference for Wikipedia's readers.
I would argue though that there are other stakeholders for whom the choice of mechanism makes a bigger difference:
the operators of the quality control mechanisms and the users whose edits are being targeted.
The difference (syn!) for edit filter managers vs bot developers is that the architecture of the edit filter plugin fosters collaboration which results in a better system (with more eyeballs all bugs are.. ???)
Any edit filter manager can modify a filter causing problems and the development of a single filter is mostly a collaborative (syn!) process.
Just a view on the history of most filters reveal that they have been updated multiple times by various users.
In contrast, bots' source code is often not publicly available and they mostly run by one operator only, so no real peer review of the code is practiced and the community has time and again complained of unresponsive bot operators in emergency cases.

The choice of mechanism makes a difference for the editor whose edits have been classified as disruptive as well.
Filters assuming good faith seek communication with the editor by issueing warnings which provide some feedback for the editor and allow them to modify their edit (hopefully in a constructive fashion) and publish it again.
Bots on the other hand simply revert everything their algorithms find malicious.
In case of good faith edits, this would mean that an editor wishing to dispute this decision should open a discussion (on the bot's talk page?) and research has shown that attempts to initiate discussions with (semi-)automated quality control agents have in general quite poor response rates % TODO quote
% that's positive! editors get immmediate feedback and can adjust their (good faith) edit and publish it! which is psychologically better than publish something and have it reverted in 2 days

% censorship infrastructure concerns: maybe discuss in the conclusion

% think about what values we embed in what systems and how; --> Lessig
\begin{comment}
Alternative approaches to community management:
compare with Surviving the Eternal September paper~\cite{KieMonHill2016}
"importance of strong
systems of norm enforcement made possible by leadership,
community engagement, and technology."

"emphasizing decentralized moderation" //all community members help enforce the norms
"ensuring enough leadership capacity is available
when an influx of newcomers is anticipated."
"Designers may
benefit by focusing on tools to let existing leaders bring others
on board and help them clearly communicate norms."
"designers should support an ecosystem of accessible and ap-
propriate moderator tools."

\end{comment}

% TODO also comment on negative results! (what negative results do I have?)

%***************************************
\cite{GeiRib2010}

"these tools make certain pathways of action easier for vandal
fighters and others harder"

"Ultimately, these tools take their users
through stan dardized scripts of action in which it always
possible to act otherwise, but such deviations demand
inventiveness and time."

\begin{comment}
Use as an argument in favor of filter came to be this way organically
\url{http://www.aaronsw.com/weblog/whorunswikipedia}
"But what’s less well-known is that it’s also the site that anyone can run. The vandals aren’t stopped because someone is in charge of stopping them; it was simply something people started doing. And it’s not just vandalism: a “welcoming committee” says hi to every new user, a “cleanup taskforce” goes around doing factchecking. The site’s rules are made by rough consensus. Even the servers are largely run this way — a group of volunteer sysadmins hang out on IRC, keeping an eye on things. Until quite recently, the Foundation that supposedly runs Wikipedia had no actual employees.
This is so unusual, we don’t even have a word for it. It’s tempting to say “democracy”, but that’s woefully inadequate. Wikipedia doesn’t hold a vote and elect someone to be in charge of vandal-fighting. Indeed, “Wikipedia” doesn’t do anything at all. Someone simply sees that there are vandals to be fought and steps up to do the job."
//yeah, I'd call it "do-ocracy"

\end{comment}

%***************************************
\begin{comment}

* Till now the whole inquiry is largely descriptive. It's fine the status quo is captured but then we should go a step further and ask "so what"? What do we have from that? Explain the data
  * maybe we won't be able to explain a lot of it and we can open it further as interesting questions to be looked into by ethnographers


* think about what values we embed in what systems and how; --> Lessig

Difference bot/filter: filters are part of the "platform". (vgl also ~\cite{Geiger2014} and criticism towards the view of a hollistic platform)
They are a MediaWiki extension, which means they are run on official Wikimedia infrastructure. (vgl \cite{Geiger2014} and "bespoke code")
This makes them more robust and bestow them another kind of status.
Bots on the other hand are what Stuart Geiger calls "bespoke code": they are auxiliary programms developed, mantained and run by single community members, typically (at least historically?) not on Wikimedia's infrastructure, but instead on private computers or third party servers.
Is this difference really significant nowadays though? A lot of bots are run on the toolserver which makes the "not server-side" distinction really difficult.
The toolserver is yet another infrastructure run and maintained by the Wikimedia foundation.
So arguments such as reduced reliability through running on a private machine in a person's living room become kind of obsolete.

\cite{Geiger2014}
"What if, from the beginning, I had decided to run my bot on the toolserver, a
shared server funded and maintained by a group of German Wikipedians for all kinds of pur-
poses, including bots? If so, the bot may have run the same code in the same way, producing
the same effects in Wikipedia, but it would have been a different thing entirely."
"when life got in the way, it was something I literally pulled the plug on
without so much as a second thought."

* another difference bots/filters, it's easier to ddos the bot infrastructure, than the filters: buy a cluster and edit till the revert table overflows -- mh. I can also edit till the AbuseLog overflows...

* why get certain filters (and not others?)
* do filters solve effectively the task they were conjured up to life to fulfil?
* what kinds of biases/problems are there?

Claudia: * A focus on the Good faith policies/guidelines is a historical development. After the huge surge in edits Wikipedia experienced starting 2005 the community needed a means to handle these (and the proportional amount of vandalism). They opted for automatisation. Automated system branded a lot of good faith edits as vandalism, which drove new comers away. A policy focus on good faith is part of the intentions to fix this.

 could be that the high hit count was made by false positives, which will have led to disabling the filter (TODO: that's a very interesting question actually; how do we know the high number of hits were actually leggit problems the filter wanted to catch and no false positives?)
--  we can't really? unless we study the edits themselves; I did this exemplarily for edits from the peak period in 2016; they were not false positives but a big spam wave.
\end{comment}

% Ethical discussion: open science vs thick descriptions which put individuals on the spot

\section{Limitations}

This work presents a first attempt at analysing Wikipedia's edit filter system.
It has several limitations (we could think of).
First, it focuses on English Wikipedia only.
We are convinced that there are valuable lessons to be learnt (about the communities, usefulness of filters, ..) from comparing edit filter use across different language versions.
Second, unfortunately, including an ethnographic analysis was not possible.
This is partially due to the fact that we employ a computer science perspective on the question and partially due to limited time.
Third, the manual filter classification was undertaken by one person only, so biases of this person have certainly shaped the labels.

Fourth, edit filter history table was not available, no hollistic quantitative analysis of the filters' development over time
Fifth, no access to the details of hidden filters, so no insights into the areas they target (although couple of educated guesses: bunch of persistent long term vandal, who often employ sockpuppets; harassment/personal attack cases hidden to protect the affected persons)

%TODO describe also negative results!

%Data
Following other pages looked interesting or related, but were left out, mainly because of insufficient time.
(Is there a better reasoning why I looked at the pages I looked at specifically, while left particularly these other pages for later?)

%************************************************************************

\section{Directions for future studies}
\label{sec:further-studies}
<insert long list of interesting questions here>

\begin{itemize}
	\item Die Zusammenfassung sollte das Ziel der Arbeit und die zentralen Ergebnisse beschreiben. Des Weiteren sollten auch bestehende Probleme bei der Arbeit aufgezählt werden und Vorschläge herausgearbeitet werden, die helfen, diese Probleme zukünftig zu umgehen. Mögliche Erweiterungen für die umgesetzte Anwendung sollten hier auch beschrieben werden.
\end{itemize}

\begin{comment}
In ``urgent situations'' however (how are these defined? who determines they are urgent?), discussions about a filter may happen after it was already implemented and set to warn/disallow edits without thorough testing.
Here, the filter editor responsible should monitor the filter and the logs in order to make sure the filter does what it was supposed to~\cite{Wikipedia:EditFilter}.
I think these cases should be scrutinised extra carefully since ``urgent situations'' have historically always been an excuse for cuts in civil liberties.
\end{comment}

\begin{comment}
also
* complete abuse\_filter\_history and real historical analysis: e.g. quantitative exploration of the usage and creation patterns
* access to the whole database and analysis of private filters
    -- it's possible to request access for research purposes (NDAs, ..)

\end{comment}

%\subsection{Harassment and bullying}

%TODO Do edit filter managers specialize on particular types of filters (e.g. vandalism vs good faith?) -- abuse\_filter\_history table is needed for this

* talk to edit filter managers (especially such who are simultaneously also bot operators)
