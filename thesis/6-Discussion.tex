\chapter{Discussion}
\label{chap:discussion}

I started this inquiry with following questions:\\ %TODO  either number the questions with Qx from the beginning and use it consistently or leave it be altogether
Q1: What is the role of edit filters among existing algorithmic quality-control mechanisms on Wikipedia (bots, semi-automated tools, ORES, humans)?\\
%-- chapter 4 (and 2)
Q2: Edit filters are a classical rule-based system. Why are they still active today when more sophisticated ML approaches exist?\\
%-- chapter 6 (discussion)
Q3: Which type of tasks do filters take over?\\ %-- chapter 5
Q4: How have these tasks evolved over time (are they changes in the type, number, etc.)? %-- chapter 5 (can be significantly expanded)

In what follows, I go over each of them and summarise the findings.

\section{Q1 What is the role of edit filters among existing quality-control mechanisms on Wikipedia (bots, semi-automated tools, ORES, humans)?}

When edit filters were introduced in 2009, various other mechanisms that took care of quality control on Wikipedia had already been in place for some time.
However, the community felt the need for an agent (mechanism, syn) preventing obvious but pervasive and difficult to clean up vandalism as early as possible.
This was supposed to take workload off the other mechanisms along the quality control process (syn) (see figure~\ref{funnel}), especially off human editors who could then use their time more productively elsewhere, namely to check less obvious (syn) cases.
%TODO is there another important findind from chapter 4's conclusion that is missing here?

It seems obvious/natural/... to compare the edit filters, being a competely automated mechanism, with bots.
What did the filters accomplish differently?
% before vs after
A key distinction is that while bots check already published edits which they eventually may decide to revert, filters are triggered before an edit ever published.
One may argue that nowadays this is not a significant difference.
Whether a disruptive edit is outright disallowed or caught 2 seconds after its publication by ClueBot NG doesn't have a tremendous impact on the readers:
the vast majority of them will never see the edit either way.
Still, there are various examples of hoaxes that didn't survive long on Wikipedia but the couple of seconds before they were reverted were sufficient for the corrupted version to be indexed by various/multiple/... news aggregators and search engines. %TODO find them!

% The infrastructure question: Part of the software vs externally run
Another difference between bots and filters underlined several times in community discussions was that as a MediaWiki extension edit filters are part of the core software whereas bots are running on external infrastructure which makes them both slower and generally less reliable.
(Compare Geiger's account about running a bot on a workstation in his apartment which he simply pulled the plug on when he was moving out~\cite{Geiger2014}.)
Nowadays, we can ask ourselves whether this is still of significance:
A lot of bots are run on Toolforge~\cite{Wikimedia:Toolforge}, a cloud service providing a hosting environment for a variety of applications (bots, analytics, etc.) run by volunteers who work on Wikimedia projects.
The service is maintained by the Wikimedia Foundation the same way the Wikipedia servers are, so in consequence just as reliable and available as the encyclopedia itself.
The argument that someone powered off the basement computer on which they were running bot X is just not as relevant anymore.

% general discussion on "platform" and what the metaphor hides? (e.g. bot develorpers' frustration that their work is rendered invisible?)


% more on bots vs filters
% collaboration possible on filters?
% who edits filters (edit filter managers, above all trusted admins) and who edits bots (in theory anyone approved by the BAG)
Above all the distinction of bots vs filters: what tasks are handled by which mechanism and why? slides (syn!) into the foreground over and over aagain.
After all the investigations I would venture the claim that from end result perspective it probably doesn't make a terrible difference at all.
As mentioned in the paragraph above, whether malicious content is directly disallowed or reverted 2 seconds later (in which time probably who 3 user have seen it, or not) is hardly a qualitative difference for Wikipedia's readers. %TODO (although I'm making a slightly different point in the paragraph above, clean up!)
I would argue though that there are other stakeholders for whom the choice of mechanism makes a bigger difference:
the operators of the quality control mechanisms and the users whose edits are being targeted.
The difference (syn!) for edit filter managers vs bot developers is that the architecture of the edit filter plugin supposedly fosters collaboration which results in a better system (compare with the famous ``given enough eyeballs, all bugs are shallow''~\cite{Raymond1999}).
Any edit filter manager can modify a filter causing problems and the development of a single filter is mostly a collaborative (syn!) process.
Just a view on the history of most filters reveals that they have been updated multiple times by various users.
In contrast, bots' source code is often not publicly available and they are mostly run by one operator only, so no real peer review of the code is practiced and the community has time and again complained of unresponsive bot operators in emergency cases.
(On the other hand, more and more bots are based on code from various bot development frameworks such as pywikibot~\cite{pywikibot}, so this is not completely valid either.)
On the other hand, it seems far more difficult/restrictive to become an edit filter manager: there are only very few of them, the vast majority admins or in exceptional cases very trusted users.
A bot operator on the other hand (syn) only needs an approval by the BAG and can get going.

The choice of mechanism also makes a difference for the editor whose edits have been deemed disruptive.
Filters assuming good faith seek communication with the editor by issuing warnings which provide some feedback for the editor and allow them to modify their edit (hopefully in a constructive fashion) and publish it again.
Bots on the other hand (syn) revert everything their algorithms find malicious directly.
They also leave warning messages on the user's talk page informing them that their edits have been reverted because they matched the bot's heuristic and point them to a false positives page where they can make a report.
It is still a revert first-ask questions later approach which is rather discouraging for good faith newcomers.
In case of good faith edits, this would mean that an editor wishing to dispute this decision should raise the issue (on the bot's talk page?) and research has shown that attempts to initiate discussions with (semi-)automated quality control agents have in general quite poor response rates ~\cite{HalGeiMorRied2013}.

%TODO Fazit?

\section{Q2: Edit filters are a classical rule-based system. Why are they still active today when more sophisticated ML approaches exist?}
%* What can we filter with a REGEX? And what not? Are regexes the suitable technology for the means the community is trying to achieve?

Research has long demonstrated higher precision and recall of machine learning methods~\cite{PotSteGer2008}. %TODO find quotes!
With this premise in mind, one has to ask:
Why are rule based mechanisms such as the edit filters still widely in use?
Several explanations of this phenomenon sound plausible.
For one, Wikipedia's edit filters are an established system which works and does its work reasonably well, so there is no need (syn) to change it  (``never touch a running system'').
Secondly, it has been organically weaven in Wikipedia's quality control ecosystem with historical needs to which it responded and people at the time believed the mechanism to be the right solution to the problem they had.
We could ask why was it introduced in the first place when there were already other mechanisms.
Beside the specific instancies of disruptive behaviour stated by the community as motivation to implement the extension,
a very plausible explanation here is that since Wikipedia is a volunteer project a lot of stuff probably happens because at some precise moment there are particular people who are familiar with some concrete technologies so they construct a solution using the technologies they are good at using (or want to use).

Another interesting reflection is that rule based systems are arguably easier to implement and above all to understand by humans which is why they still enjoy popularity today.
On the one hand, overall less technical knowledge is needed in order to implement a single filter:
An edit filter manager has to ``merely'' understand regular expressions.
Bot development on the other hand (syn!) is a little more challenging:
A developer needs resonable knowledge of at least one programming language and on top of that has to make themself familiar with stuff like the Wikimedia API, ....
Moreover, since regular expressions are still somewhat human readable and understandable (syn!) in contrast to a lot of popular machine learning algorithms, it is easier to hold rule based systems and their developers accountable.
Filters are a simple mechanism (simple to implement) that swiftly takes care of cases that are easily recognisable as undesirable.
ML needs training data (expensive), and it is not simple to implement.

%Fazit?

\section{Q3: Which type of tasks do filters take over?}

% TODO comment on: so what's the role of the filters, why were they introduced (to get over with obvious persistent vandalism which was difficult to clean up, most probably automated) -- are they fulfilling this purpose?
Juvenile and grave vandalism, spam, good faith disruptive edits (e.g. blanking an article instead of moving it because of unfamiliarity with the software and proper procedure), maintenance tasks.
This is what edit filters seem to be doing.
As demonstrated by the bot taxonomy proposed by Halfaker and Riedl~\cite{} and mentioned (syn!) in section~\ref{}, bots are also doing a lot of these or similar tasks.
So, when a new tasks (syn) arises, how does the community decide whether to implement a bot or a filter to handle it?

As mentioned (syn) in the discussion of Q2, it probably partially depends on who discovers/takes care of the problem and what technology they are familiar with and have access to.
There are also some guidelines (compare section~\ref{}) which underline that filters are most suitable for problems concerning all pages and point out different approaches for solving other (syn) issues:
* using page protection for problems with a single page
* using the title and spam blacklist for persistent spam waves or attempts to create abusive titles
* using bots for in depth checks or problems with a single page.
Moreover, it is stated that no trivial formatting mistakes should trip filters\cite{Wikipedia:EditFilterRequested}: this seems like a waste of computing power and unnecessary irritation to the user.
I also think that bots are more suitable to take care of such cases.
However, the community is not always consistently sticking to these guidelines, and they do occasionally implement filters that contradict them.
<insert examples>
These are mostly relatively fast switched off again, but there are also examples such as filter 432? .. %TODO verify

Some of the edit filter managers are actually also bot operators.
I've compiled a list of edit filter managers who are simultaneously also bot operators;
I've further assembled the bots they run and made notes on the bots that seem to be relevant to vandalism prevention/quality assurance.
I'm currently trying to determine from document traces what filter contributions the corresponding edit filter managers had and whether they are working on filters similar to the bots they operate.
Insight is currently minimal, since abuse\_filter\_history table is not available and we can only determine what filters an edit filter manager has worked on from limited traces such as: last modifier of the filter from abuse\_filter table; editors who signed their comments from abuse\_filter table; probably some noticeboards or talk page archives, but I haven't looked into these so far; the history web API for public filters.
Some seem to be rather bot operator who implement auxiliary filters and some–edit filter managers who implement auxiliary bots.

It is to be said at this point that I eventually took these lists out of the public repository.
This clashes with my overall open science aspiration, however I determined that precisely such individual-related records are an example of ethically and privacy critical thickening of traces (see section~\ref{})
and that I'm not willing to offer potential trolls ready-made lists.

Finally, it stands to reason that if we are interested in the question when do people (who have access to both) implement a bot and when a filter, all we have to do is ask (see directions for future research in section~\ref{}).

At the end, we should also ask ourselves why get certain filters and not others? What kinds of biases/problems are there?
Is it fair and justified that a great number of filters are triggered only by new (not confirmed) editors?
Why is it all right for an established editor to use swear words whereas it is not for newbies (see filter 384)?

%Fazit

%TODO what to do with this?
\begin{comment}
## Open questions

If discerning motivation is difficult, and, we want to achieve different results, depending on the motivation, that lead us to the question whether filtering is the proper mechanism to deal with disruptive edits.

\end{comment}


\section{Q4: How have these tasks evolved over time (are they changes in the type, number, etc.)?}

We have learnt following about temporal trends:
the overall number of active edit filters stays the same (condition limit);
it is mostly the same old filters who have been active trough the years;
the overall number of filter hits has risen since 2016 when a somewhat puzzling spike in filter hits took place;
the general tendency is that over time there are less good faith filter (hits) and more vandalism related filters (hits).
Since good faith filters issuing warning to users seemed as kind of a unexpected by-product (nobody mentioned these as a motivation to implement the extension), %TODO verify this!
I would've expected the opposite tendency: a maybe slight surge in the use of good faith filters once the community noticed they can put filters to such use.

This question should be explored further and deeper investigations into the hits peak or the temporal trends of filters creation and configuration/refactoring should be undertaken.


\begin{comment}
Claudia: * A focus on the Good faith policies/guidelines is a historical development. After the huge surge in edits Wikipedia experienced starting 2005 the community needed a means to handle these (and the proportional amount of vandalism). They opted for automatisation. Automated system branded a lot of good faith edits as vandalism, which drove new comers away. A policy focus on good faith is part of the intentions to fix this.
\end{comment}


%***************************************

\section{Limitations}

This work presents a first attempt at analysing Wikipedia's edit filter system.
Several limitations of this study come to mind.
Firstly, it focuses on English Wikipedia only.
This presents (syn!) an excellent starting point for analysis of the edit filter system, since this was also the first language version to which the mechanism was introduced.
However, valuable lessons can be learnt (about the communities, models of governance, usefulness of filters, etc.) from comparing edit filter use across different language versions.
Just recall, how for instance the role of edit filter managers doesn't exist in certain language versions (comapare chapter~\ref{}) and instead it is administrators who have an \emph{abusefilter-modify} permission next to their other rights.

Secondly, unfortunately, conducting a classical ethnographic analysis was not possible.
It would have been particularly insightful to talk to edit filter managers (above all such who are simultaneously also bot operators) and developers of the extension, as well as regular editors who have tripped a filter.
This is partially due to the fact that we employ a computer science perspective on the question and partially due to limited time.
I really only used ``found data'' (compare~\ref{sec:trace-ethnography}) (well I also attempted to interpret the found data and link it) and future studies can and should use the first insights of the current research as interview prompts

Thirdly, the manual filter classification was undertaken by one person only (me), so biases of this person have certainly shaped the labels.

Fourth, edit filter history table was not available, so no hollistic quantitative analysis of the filters' development over time was possible.

Fifth, no access to the details of hidden filters, so no insights into the areas they target (although couple of educated guesses: bunch of persistent long term vandal, who often employ sockpuppets; harassment/personal attack cases hidden to protect the affected persons)

%Data
Following other pages looked interesting or related, but were left out, mainly because of insufficient time.
(Is there a better reasoning why I looked at the pages I looked at specifically, while left particularly these other pages for later?)


%************************************************************************

\section{Directions for future studies}
\label{sec:further-studies}

Throughout the thesis, a variety of intriguing questions arose which couldn't be addressed due to various reasons, above all–insufficient time.
Here, a comprehensive list of all these pointers for possible future research is provided.

\begin{enumerate}
    \item \textbf{How have edit filters's tasks evolved over time?}: Unfortunately, no detailed historical analysis of the filters was possible, since the database table storing changes to individual filters (\emph{abuse\_filter\_history}) is not currently replicated (see section~\ref{sec:overview-data}). A patch aiming to renew the replication of the table is currently under review~\cite{gerrit-tables-replication}. When a dump becomes available, an extensive analysis (sym) of filter creation and activation patterns, together with .. will be possible (syn).
        (Actually there is some historical stuff: e.g. temporal overview of hits, broken down by filter action... Beware however, it is the *current* filter action they were plotted with and it is very possible that the corresponding filters had a different action switched on some time ago. %TODO check whether that's actually true
        (or another visibility level, different regex pattern which would've resulted in a different manual tag)
    \item \textbf{What are the differences between how filters are governed on EN Wikipedia compared to other language versions?}: Different Wikipedia language versions each have a local community behind them. %TODO quote?
        These communities vary widely in their modes of organisation, ..., and values. It would be definitely fascinating to explore differences between filter governance (and what typed of filters are applied) between the different languages.
    \item \textbf{Are edit filters a suitable mechanism for fighting harassment?}: Online harassment has been an increasingly important topic since.. %TODO quote ExMachina paper?
        It is also a problem recognised and addressed by Wikimedia/the Wikipedian community %TODO see 2015 Harassment survey; is there a newer one?
        According to the edit filter noticeboard archives~\cite{Wikipedia:EditFilterNoticeboardHarassment} there have been some attempts to combat harassment by means of filters.
        An evaluation of the usefulness and success of the mechanism at this task would be really interesting.
    \item \textbf{When an editor (edit filter manager who is also a bot operator) will implement a bot and when a filter} - ethnographic inquiry
    \item \textbf{Repercussions on affected editors}: What are the consequences of edit filters on editors whose edits are filtered? Frustration? Allienation? Do they understand what is going on? Or are for example edit filter warnings helpful and the editors appreciate the hints they have been given and use them to improve their collaboration?
\begin{comment}
%TODO where to put this?
Users are urged to use the term "vandalism" carefully, since it tends to offend and drive people away.
("When editors are editing in good faith, mislabeling their edits as vandalism makes them less likely to respond to corrective advice or to engage collaboratively during a disagreement,"~\cite{Wikipedia:Vandalism})
There are also various complaints/comments by users bewildered that their edits appear on an ``abuse log''
\end{comment}
    \item \textbf{Is it possible to study the regex patterns in a more systematic fashion? What is to be learnt from this?} For example, it comes to attention that a lot of filters target new users: ``!(""confirmed"" in user\_groups)'' is their first condition%is this really interesting?
    \item \textbf{(How) has the notion of ``vandalism'' on Wikipedia evolved over time?}: By comparing older and newer filters, or respectively updates in filter patterns we could investigate whether there is a qualitative change in the interpretation of the ``vandalism'' notion on Wikipedia.
    \item \textbf{False Positives?}: were filters shut down, bc they matched more False positives than they had real value?
    \item \textbf{What are the urgent situations in which edit filter managers are given the freedom to act as they see fit and ignore best practices of filter adoption (i.e. switch on a filter in log only mode first and announce it on the notice board so others can have a look)? Who determines they are urgent?}: I think these cases should be scrutinised extra carefully since ``urgent situations'' have historically always been an excuse for cuts in civil liberties.
%* is there a qualitative difference between complaints of bots and complaints of filters?
    \item \textbf{Is there a qualitative difference between the tasks/patterns of public and hidden filters?}: We know of one general guideline/rule of a thumb (cite!) according to that general filters are to be public while filters targeting particular users are hidden. Is there something more to be learnt from an actual examination of hidden filters? One will have to request access to them for research purposes, sign an NDA, etc.
    \item \textbf{Do edit filter managers specialize on particular types of filters (e.g. vandalism vs good faith?)} \emph{abuse\_filter\_history } table is needed for this
    \item \textbf{What proportion of quality control work do filters take over?}: compare filter hits with number of all edits and reverts via other quality control mechanisms
    \item \textbf{Do edit filter managers stick to the edit filter guidelines?}: e.g. no trivial problems (such as spelling mistakes) should trigger filters; problems with specific pages are generally better taken care of by protecting the page and problematic title by the title blacklist; general filters shouldn't be hidden
\end{enumerate}
