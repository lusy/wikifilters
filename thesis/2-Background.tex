\chapter{Background}
\label{chap:background}

In the present chapter we study scientific literature on vandalism in Wikipedia and the quality control mechanisms applied to counteract this vandalism in order to better understand the role of edit filters in this ecosystem.
There are works on vandalism in general/vandalism detection, as well as several articles dedicated to the role bots play in mainataining quality on Wikipedia (cite... ), a couple which discuss combating vandalism by means of semi-automated tools such as Huggle, Twinkle and STiki (cite).
Time and again, the literature refers also to more ``manual'' forms of quality control by editors using watchlists to keep an eye on articles they care about or even accidentially discovering edits made in bad faith.
There is one mechanism though that is very ostentatiously missing from all these reports: none of them ever mention (is this really true?) the edit filter mechanism.
\cite{AstHal2018} have a diagram describing the new edit review pipeline. Filters are absent.

\section{Vandalism on Wikipedia}
%TODO put here papers on vandalism

Papers discussing vandalism detection from IR/ML perspective:
- Martin Potthast, Benno Stein, and Robert Gerling. 2008. Automatic vandalism detection in Wikipedia. In European conference on information retrieval. Springer, 663–668.

\section{Quality-control mechanisms on Wikipedia}


Why is it important we study these mechanisms?
- their relative usage increases/has increased since they were first introduced
    \cite{GeiRib2010}
    "at present, bots make 16.33\% of all edits."
    %TODO more recent data? the last month argument via recentchanges (vgl \cite{Geiger2017}) doesn't hold here
- the whole ecosystem is not transparent, especially for new users (see~\cite{ForGei2012}: "As it is, Kipsizoo is not even
sure whether a real person who deleted the articles or a bot." )
"Keeping traces obscure help the powerful to remain in power"~\cite{ForGei2012}
- "inofficial", run and maintained by the community
    \cite{GeiRib2010}
    "often-unofficial technologies have fundamentally
    transformed the nature of editing and administration in
    Wikipedia"
    "Of note is the fact that these tools are largely
    unofficial and maintained by members of the Wikipedia
    community."
- higher entry barriers: new users have to orientate themselves in the picture and learn to use the software (decentralised mode of governance, often "impenetrable for new editors", vgl~\cite{ForGei2012})
- gamification concerns (is fighting vandalism becoming a game where certain users aim to revert as many edits as possible in order to get a higher score; and as a consequence these same users often times enforce reverts more rigorously than recommended and also pick cases that are easy and fast to arbitrate and do not require much additional research)
    \cite{HalRied2012}
    "Some Wikipedians feel that such
    motivational measures have gone
    too far in making Wikipedia like a
    game rather than a serious project.
    One humorous entry even argues that
    Wikipedia has become a MMORPG—
    a massively multiplayer online role-
    playing game—with “monsters”
    (vandals) to slay, “experience”
    (edit or revert count) to earn, and
    “overlords” (administrators) to submit
    to (http://en.wikipedia.org/wiki/
    Wikipedia:MMORPG)."
- they change the system not only in matter of scale (using bots/tools is faster, hence more reverts are possible) but in matter of substance: how everything interacts with each other
- they enable efficient patrolling of articles by users with little to no knowledge about the particular contents (thanks to their representation of the edits/information: e.g. diffs)
    \cite{GeiRib2010}
    !! tools not only speed up the process but:
    "These tools greatly lower certain barriers to participation and render editing
    activity into work that can be performed by "average
    volunteers" who may have little to no knowledge of the
    content of the article at hand"


---

socio-technical assemblages (see Geiger)

%Numbers
\cite{GeiRib2010}
Check Figure 1: Edits to AIV by tool (in the meantime 10 years old. is there newer data on the topic??)
not really, see:
\cite{Geiger2017}
"In the English-lan-
guage Wikipedia, 22 of the 25 most active editors (by
number of edits) are bot accounts, and July 2017, they
made about 20\% of all edits to encyclopedia articles."
Geiger's evidence:
https://quarry.wmflabs.org/query/20703
Percent of bot edits in previous month (enwiki, all pages)
\begin{verbatim}
is_bot	edits	Percentage of all edits
0	    7619466	79.4974
1	    1965083	20.5026
\end{verbatim}

https://quarry.wmflabs.org/query/20704
Percent of bot edits in previous month (enwiki, articles only)
\begin{verbatim}
is_bot	edits	Percentage of all edits
0	    4273810	80.2025
1	    1054966	19.7975
\end{verbatim}

However, a month is a relatively small period and you can't make an argument about general trends based on it.
For instance, these same quarries ran on April 12, 2019 render following results:
https://quarry.wmflabs.org/query/35104
Percent of bot edits in previous month (enwiki, all pages)
\begin{verbatim}
is_bot	edits	Percentage of all edits
0	    6710916	89.7318
1	    767948	10.2682
\end{verbatim}

https://quarry.wmflabs.org/query/35105
Percent of bot edits in previous month (enwiki, articles only)
\begin{verbatim}
is_bot	edits	Percentage of all edits
0	    3426624	92.1408
1	    292274	7.8592
\end{verbatim}


\subsection{Humans}

Despite steady increase of the proportion of fully and semi-automated tools usage for fighting vandalism %TODO quote!
some of the quality control work is still done ``manually'' by humand editors.
These are, on one hand, editors who use the ``undo'' functionality from within the page's revision history.
On the other hand, there are also editors who engage with the classical/standard encyclopedia editing mechanism (click the ``edit'' button on an article, enter changes in the editor which opens, write an edit summary for the edit, click ``save'') rather than using further automated tools to aid them.
When editors use these mechanisms for vandalism fighting, oftentimes they haven't noticed the vandalising edits by chance but rather have been actively watching the pages in question via the so-called watchlists~\cite{AstHal2018}. %TODO: quote watchlist, current paper by Halfaker
This also gives us a hint as to what type of quality control work humans take over: less obvious and less rapid, editors who patrol pages via watchlists have some relationship to/deeper expertise on the topic. %TODO quote needed.
%TODO vgl also funnel diagram incoming edits quality assurance by Halfaker


\subsection{Semi-automated tools}

Semi-automated tools used for vandalism fighting on Wikipedia are discussed by:
more popular/widely used:
STiki~\cite{WestKanLee2010}
\url{http://en.wikipedia.org/wiki/Wikipedia:STiki}
Huggle~\cite{GeiHal2013},~\cite{HalRied2012},\cite{GeiRib2010}
\url{https://en.wikipedia.org/wiki/Wikipedia:Huggle}
Twinkle
\url{https://en.wikipedia.org/wiki/Wikipedia:Twinkle}
AWB
\url{https://en.wikipedia.org/wiki/Wikipedia:AutoWikiBrowser}
less popular/older, mentioned in older accounts or not discussed at all (there are also more tools, see for example \url{https://en.wikipedia.org/wiki/Category:Wikipedia_counter-vandalism_tools})
VandalProof~\cite{HalRied2012}
ARV
AIV
Lupin's Anti-vandal tool~\cite{GeiRib2010}
\url{https://en.wikipedia.org/wiki/User:Lupin/Anti-vandal_tool}
"Please be aware that the original author of AVT (Lupin) is no longer active on Wikipedia. The script is very old and might stop working at any time."
"By using the RC feed to check a wiki-page's differences against a list of common vandal terms, this tool will detect many of the commonly known acts of online vandalism. "

In general, previous research seems to make a distinction of degree? between ``more'' automated tools such as Huggle and STiki and ``less'' automated ones such as Twikle~\cite{GeiHal2013}.

%Huggle
Huggle was initially released in 2008~\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:Huggle}}.
In order to use Huggle, editors need the ``rollback'' permission~\cite{HalRied2012}.
Huggle presents a pre-curated queue of edits to the user which can be classified as vandalism by a single mouse click which simultaneously takes action accordingly: the edit is reverted, the offending editor is warned~\cite{HalRied2012}.
Moreover, Huggle is able to parse the talk page of the offending user where warnings are placed in order to issue a next warning of suitable degree and also makes automated reports to AIV (Administrators Intervention Against Vandalism, explain!) if the user has exhausted the warning limit.
The software uses a set of heuristics for compiling the queue with potentially offending edits.
The defaults include placing higher edits containing large removal of content or complete blankings of a page, edits made by anonymous users or users whose edits have been reverted in the past.
Edits by users with warnings on their user talk page are sent to the top of the queue, while edits made by bots and other Huggle users are ignored altogether\cite{GeiRib2010}.
One can reconfigure the queue, however, some technical savvy and motivation is need for this and thus, as~\cite{GeiRib2010} warn, it makes certain paths of action easier to take than others.

%STiki
STiki was introduced by Andrew G. West in June 2010~\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:STiki}}.
Its defining characteristic is relying on ``spatio-temporal properties of revision metadata''~\cite{WestKanLee2010} for deciding the likelihood of an edit to be vandalism.
According to the authors, this makes the tool's vandalism detection more robust and language independent.
One of the following conditions must be fulfilled for an editor to obtain a permission to use STiki:
(1) they must have the rollback permission, or
(2) they must have made at least 1000 article edits, or
(3) they must have obtained a special permission via their talk page~\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:STiki}}.

According to~\cite{GeiHal2013} Huggle and STiki complement each other in their tasks, with Huggle users making swifter reverts and STiki users taking care of older edits.

\begin{comment}
\url{https://en.wikipedia.org/wiki/Wikipedia:STiki/leaderboard}

"Above all else, it should be emphasized that STiki is not a competition."
//compare also~\cite{HalRied2012} who warn against Wikipedia becoming gamified with vandals being "monsters"

" STiki users who operate the tool recklessly in the hope of inflating their statistics are not helping themselves or the project "

\url{https://en.wikipedia.org/wiki/Wikipedia:STiki}

"STiki is a tool available to trusted users that is used to detect and revert vandalism, spam, and other types of unconstructive edits made at Wikipedia. "

"STiki chooses edits to show to end users; if a displayed edit is judged to be vandalism, spam, etc., STiki streamlines the reversion and warning process. STiki facilitates collaboration in reverting vandalism; a centrally stored lists of edits to be inspected are served to STiki users to reduce redundant effort."

"STiki may only be used by editors with a Wikipedia account. Additionally, the account must meet some qualifications to reduce the probability of users misidentifying vandalism."

"The account must have any one of: (1) the rollback permission/right, (2) at least 1000 article edits (in the article namespace, not to talk/user pages), or (3) special permission via the talk page. We emphasize that users must take responsibility for their actions with STiki. "

"After login, users primarily interact with the GUI tool by classifying edits into one of four categories:
vandalism
good faith revert
pass
innocent "
//interestingly, at the initial tool presentation~\cite{WestKanLee2010}, there was no "good faith" option. It seemed to have been added quite promptly after though, since the screenshot of the tool on the page has the button already and claims to have been made on 28 February 2010

"Uncertainty over malice: It can be tricky to differentiate between vandalism and good-faith edits that are nonetheless unconstructive. "
\end{comment}

%Twinkle
Twinkle, a javascript based ``user interface extension that runs inside of a standard web browser''~\cite{GeiRib2010} seems to be less automated than the previous tools~\cite{GeiHal2013}.
It adds contextual links to other parts of Wikipedia which facilitates fulfilling particular tasks (rollback multiple edits, report problematic users to AIV, nominate an article for deletion) with a single click~\cite{GeiRib2010}.
A prerequisite for using Twinkle is being an autoconfirmed registered user~\footnote{\url{https://en.wikipedia.org/wiki/Wikipedia:Twinkle}}.

%TODO decide whether to elaborate more via https://en.wikipedia.org/wiki/Wikipedia:Twinkle/doc

%Lupin's anti-vandal tool
%VandalProof
Older tools which are not much used anymore include Lupin's anti-vandal tool which
``provides a real-time in-browser feed of edits made matching certain algorithms''~\cite{GeiRib2010}
and VandalProof which
``let[s] trusted editors monitor article edits as fast as they happened in Wikipedia and revert unwanted contributions in one click''~\cite{HalRied2012}.

%TODO: Note on collaboration semi-automated tools/edit filters. Maybe not the suitable chapter

\subsection{Bots}

\cite{Livingstone2016}
Bot Def
"bots are processes that run in the background, are normally
invisible, react to their environment, and most importantly, run autonomously. Leonard (1997) claims autonomy
is the “crucial variable”"

"What is a bot?
In the Wikimedia software, there are tasks that do all sorts of things, like count pages, count orphan pages, etc.
If these things are not in the software, an external bot could do them. It is just a small step to editing pages. The
main difference is where it runs and who runs it."

\cite{GeiRib2010}
BotDef
"Bots – short for „robots" – are fully-automated software
agents that perform algorithmically-defined tasks involved
with editing, maintenance, and administration in Wikipedia."

---

%ClueBot NG
"ClueBot\_NG uses state-of-the-art machine learning techniques to review all contributions to

ClueBot NG:
\cite{GeiHal2013}
"to scan every edit made to Wikipedia in real time"
"Built on Bayesian neural networks and trained with data
about what kind of edits Wikipedians regularly revert as
vandalism"
articles and to revert vandalism,"~\cite{HalRied2012}

%XLinkBot
"XLinkBot reverts contributions that create links to
blacklisted domains as a way of quickly and permanently dealing with spammers."~\cite{HalRied2012}

%HBC AIV Helperbots and MartinBot
"AIV Helperbot turns a simple page into a dynamic
priority-based discussion queue to support administrators in their work of identifying and
blocking vandals"~\cite{HalRied2012}


%AntiVandalBot
~\cite{HalRied2012}
"The first tools to redefine the
way Wikipedia dealt with van-
dalism were AntiVandalBot and
VandalProof."

"AntiVandalBot used a simple set
of rules and heuristics to monitor
changes made to articles, identify the
most obvious cases of vandalism, and
automatically revert them"

1st vandalism fighting bot:
"this bot made it possible, for the first
time, for the Wikipedia community
to protect the encyclopedia from
damage without wasting the time
and energy of good-faith editors"
"it
wasn’t very intelligent and could only
correct the most egregious instances
of vandalism."


Bots not patrolling constantly but instead doing batch cleanup works~\cite{GeiHal2013}:
AWB, DumbBOT, EmausBot
(also from figures: VolkovBot, WikitanvirBot, Xqbot)

\cite{GeiRib2010}
"“HBC AIV helperbot7” – automatically
removed the third vandal fighter's now-obsolete report."

%Note on collaboration bots/edit filters. Maybe not the suitable chapter

\subsection{ORES}

ORES is an API based FLOSS? machine learning service ``designed to improve the way editors maintain the quality of Wikipedia''~\cite{HalTar2015} and increase transparency of the quality control process.
It uses learning models to predict a quality score for each article and edit (based on edit/article quality assessments manually assigned by Wikipedians).
Potentially damaging edits are highlighted which allows editors who engage in vandal fighting to examine them in greater detail.
The service was officially introduced in November 2015 by Aaron Halfaker (principal research scientist at the Wikimedia Foundation) and Dario Taraborelli (who was Head of Research at Wikimedia Foundation at the time)~\cite{HalTar2015}. %TODO footnote https://wikimediafoundation.org/role/staff-contractors/
% http://nitens.org/taraborelli/cv
Since ORES is API based, in theory a myriad of services can be developed that use the predicted scores or, new models can be trained and made available for everyone to use.
The Scoring platform team reports that popular vandal fighting tools(syn?) such as Huggle have already adopted ORES for the compilation of their queues~\cite{HalTar2015}.
ORES development is ongoing, following people are involved: (Wikimedia Scoring Platform team).
What is unique about ORES is that all the algorithms, models, training data, and code are public, so everyone (with sufficient knowledge of the matter) can scrutinise them and reconstruct what is going on.
This is certainly not true for machine learning services applied by commercial companies who have interest in keeping their models secret.


\cite{HalTar2015}

"Our hope is that ORES will enable critical advancements in how we do quality control—changes that will both make quality control work more efficient and make Wikipedia a more welcoming place for new editors."

"ORES brings automated edit and article quality classification to everyone via a set of open Application Programming Interfaces (APIs). The system works by training models against edit- and article-quality assessments made by Wikipedians and generating automated scores for every single edit and article."

"English Wikipedians have long had automated tools (like Huggle and STiki ) and bots (like ClueBot NG) based on damage-detection AI to reduce their quality control workload.  While these automated tools have been amazingly effective at maintaining the quality of Wikipedia, they have also (inadvertently) exacerbated the difficulties that newcomers experience when learning about how to contribute to Wikipedia. "
"These tools encourage the rejection of all new editors’ changes as though they were made in bad faith," //NB!!!
"Despite evidence on their negative impact on newcomers, Huggle, STiki and ClueBot NG haven’t changed substantially since they were first introduced and no new tools have been introduced. " //what about the edit filters? when were Huggle,STiki and ClueBotNG introduced?

"decoupling the damage prediction from the quality control process employed by Wikipedians, we hope to pave the way for experimentation with new tools and processes that are both efficient and welcoming to new editors. "

caution: biases in AI
" An algorithm that flags edits as subjectively “good” or “bad”, with little room for scrutiny or correction, changes the way those contributions and the people who made them are perceived."

"Examples of ORES usage. WikiProject X’s uses the article quality model (wp10) to help WikiProject maintainers prioritize work (left). Ra·un uses an edit quality model (damaging) to call attention to edits that might be vandalism (right)." //interesting for the memo

further ORES applications:
"  But revision quality scores can be used to do more than just fight vandalism. For example, Snuggle uses edit quality scores to direct good-faith newcomers to appropriate mentoring spaces,[4] and dashboards designed by the Wiki Education Foundation use automatic scoring of edits to surface the most valuable contributions made by students enrolled in the education program"


\section{Algorithmic Governance}

maybe move it to edit filters chapter


So far, on grounds of literature study alone it remains unclear what the role/purpose of edit filters is.
