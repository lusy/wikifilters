\chapter{Methods}
\label{chap:methods}

This chapter describes the methodology applied throughout the thesis.

\section{Open Science}

The whole work tries to adhere to the principles of open science. %TODO what are the principle of open science? refs are missing
All the computations I have done and other artefacts I have used or compiled are openly accessible in the project's repository~\cite{github}.
And have been openly accessible since the very beginning.
Everyone interested can follow the process and/or use the data or scripts in order to verify my computations (syn) or run their own and thus continue this research along one of the directions suggested in section~\ref{sec:further-studies} or in a completely new one.

\section{Trace Ethnography}

A second important theoretical framework constitutes the trace ethnography.
The concept was first introduced/used by Geiger and Ribes in their 2010 work ``The work of sustaining order in Wikipedia: the banning of a vandal''~\cite{GeiRib2010} and introduced in detail in a 2011 paper~\cite{GeiRib2011}.
The scholars define trace ethnography as a methodology which
``combines the richness of participant-observation
with the wealth of data in logs so as to reconstruct
patterns and practices of users in distributed
sociotechnical systems''
and is especially practical for research in distributes technical systems (doppelt gemoppelt with quote) where direct partipants observation is impractical, costly and tend to miss phenomena due to..
They use documents and document traces: ... %TODO which ones
in order to reconstruct quite exactly single strands of actions and comprehend how different agents on Wikipedia work together towards the blocking of a single malicious user.
They (syn!) refer to ``turn[ing] thin documentary traces into “thick descriptions” of actors and events".
What is more, these traces are used by Wikipedians themselves in order to do their work efficiently.
Geiger and Ribes underline the importance of insider knowledge when reconstructing actions and processes based on the traces,
the need for ``an ethnographic understanding of the activities, people, systems, and technologies which contribute to their production''.

They alert that via trace ethnography only that can be observed which is recorded by the system and records are always incomplete.
%TODO pitfalls of using data produced for other purposes?

The researchers also warn of possible privacy breaching through thickening traces:
although records they use to reconstruct paths of action are all open, the thick descriptions they compile can suddenly expose a lot of information about single users which never existed in this form before and who never gave their informed consent for their data being used this way.

\begin{comment}

\cite{GeiHal2017}
"when working with large-scale “found data” [36] of the traces
users leave behind when interacting on a platform, how do we best operationalize culturally-specific
concepts like conflict in a way that aligns with the particular context in which those traces were made?"

Star: "ethnography of infrastructure":
"discusses the “veridical” approach, in which “the information system
is taken unproblematically as a mirror of actions in the world, and often tacitly, as a complete
enough record of those actions” (p. 388).
She contrasts this with seeing the data as “a trace or record
of activities,” in which the information infrastructure “sits (often uneasily) somewhere between
research assistant to the investigator and found cultural artifact."

"Trace
ethnography is not “lurker ethnography” done by someone who never interviews or participates in
a community."
trace literacy --> get to know the community; know how to participate in it

thick description of different prototypical cases:

vgl \cite{GeiHal2017}
iterative mixed method
combination of:
* quantitative methods: mining big data sets/computational social science
"begin with one or
more large (but often thin) datasets generated by a software platform, which has recorded digital
traces that users leave in interacting on that platform. Such researchers then seek to mine as much
signal and significance from these found datasets as they can at scale in order to answer a research
question"
* more traditional social science/qualitative methods, e.g. interviews, observations, experiments
\end{comment}

\begin{comment}
vgl \cite{GeiHal2017}
iterative mixed method
combination of:
* quantitative methods: mining big data sets/computational social science
"begin with one or
more large (but often thin) datasets generated by a software platform, which has recorded digital
traces that users leave in interacting on that platform. Such researchers then seek to mine as much
signal and significance from these found datasets as they can at scale in order to answer a research
question"
* more traditional social science/qualitative methods, e.g. interviews, observations, experiments

\cite{Geiger2014}
"the idea that Wikipedia only takes place on wiki-
pedia.org – or even entirely on the Internet – is a huge misunderstanding (Konieczny, 2009;
Reagle, 2010). Wikipedia is not a virtual world, especially one located entirely on the wiki."
e.g. in order to get hold of abuse_filter_history I had to engage with
- wikipedia.org
- mediawiki.org
- irc channels
- phabricator
- gerrit
- toolserver/cloudservices
----
other spaces Wikipedia takes place
- mailinglists
- WomenEdit/offenes Editieren @Wikimedia
- Wikimania
- Wikimedia's office and daily work
\end{comment}

\section{Grounded Theory}

Grounded theory describes a myriad/... of frameworks/... for building a scientific theory \emph{grounded} in (mostly qualitative) data analysis.

Here, I haven't developed a finished theory,
but instead just employed some methods used by grounded theory %TODO check whether it's written with caps
scholars, most prominently/above all–their coding processes.
There are different branches? in grounded theory that diverge slightly or more clearly/distinctly in their assumptions and proposed methods.
I followed the guidelines and .. of constructivist grounded theory proposed/described by Charmaz in~\cite{Charmaz2006}.
I've chosen Charmaz's interpretation of grounded theory (she speaks of ``grounded theor\emph{ies}'' and calls her own constructivist rendering of it ``\emph{a} way of doing grounded theory'') precisely because of her acknowledgement of the subjective nature of every (piece of) research which is shaped by the believes, background and theoretical understanding of the people who conduct it, who always \emph{interpret} the subject they study rather than give an exact portrayal of it:
``we are part of the world we study and the data we collect. We \textit{construct} our grounded theories through our past and present involvements and interactions with people, perspectives, and research practices''~\cite[p.10]{Charmaz2006}

She advocates for ``gathering rich–detailed and full–data and placing them in their relevant situational and social contexts''~\cite[p.10-11]{Charmaz2006} which is in line with Geiger and Ribes thick descriptions generated(syn) by trace ethnography
\footnote{As a matter of fact, both Charmaz and Geiger and Ribes refer to ``thick descriptions'' which were coined as a term by~\cite{Geertz1973}}.

Coding is a process of labeling data in an attempt to make sense of it in a systematic/orderly fashion.
It is about seeking patterns in data and latery–trying to understand these patterns and the relationships/correlations between them.
Above all (syn), I applied emergent coding in chapter~\ref{chap:overview-en-wiki} when trying to make sense of the tasks EN Wikipedia's edit filters are employed for.
Key characteristic of the method are to let the codes emerge during the process contrasted to starting the process with a set of preconcieved codes.
Scholars regard this as useful because that way the danger of trying to press data in predefined categories while potentially overlooking other, better fitting codes is reduced.
Instead, the codes emerge/stem directly from observations of the data.
Since coding and analysis take place simultaneously, it is also part of the process/common to come back later and re-code parts of the data with labels that have emerged (syn) later (syn) in the process.

\begin{comment}
Grounded Theory~\cite{Charmaz2006}
Chapter 2:
"Researchers treat extant texts \textit{as} data to address their research questions although these texts were produced for other–often very different–purposes." (p.35)

additional types of data we can use:
public records, government reports, organizational documents, mass media, literature, autobiographies, personal correspondence, Internet discussions, and earlier qualitative materials from data banks.

"To the extent possible, we need to situate texts in their contexts." (p.39)
"Where do the data come from? Who participated in shaping them? What did the authors intend? Have participants provided sufficient information for us to make a plausible interpretation? And do we have sufficient knowledge of the relevant worlds to read their words with any understanding?"(p.39)
"Much textual analysis is without context, or worse, out of context. [...] Providing a description of the times, actors, and issues gives you a start. Multiple methods help, such as intervieweing key participants, and using several types of documents also helps." (p.39)

TODO: Questions to ask of a text (p.39-40):
"
* How was the text produced? By whom?
* What is the ostensible purpose of the text? Might the text serve other unstated or assumed purposes? Which ones?
* How does the text represent what its author(s) assumed to exist? Which meanings are embedded within it? How do those meanings reflect a particular social, historica, and perhaps organizational context?
* What is the structure of the text?
* How does its structure shape what is said? Which categories can you discern in its structure? What can you glean from these categories? Do the categories change in sequential texts over time? How so?
* Which contextual meanings does the text imply?
* How does its content construct images of reality?
* Which realities does the text claim to represent? How does it represent them?
* What, if any, unintended information and meanings might you see in the text?
* How is language used?
* Which rules govern the constructuion of the text? How can you discern them in the narrative? How do these rules reflect both tacit assumptions and explicit meanings? How might they be related to other data on the same topic?
* When and how do telling points emerge in the text?
* What kinds of comparisons can you make between texts? Between different texts on the same topic? Similar texts at different times such as organizational annual reports? Between different authors who address the same questions?
* Who benefits from the text? Why?
"
# Coding in GT

"Grounded theory coding consists of at least two phases: initial and
focused coding." (p.42)

"From time to time, we may adopt our participants' telling
terms as in vivo codes."(p.42)

"During initial coding we study fragments of data-
words, lines, segments, and incidents-closely for their analytic
import."
"While engaging in focused coding, we select
what seem to be the most useful initial codes and test them against
extensive data."(p.42)

"Coding means naming segments of data with a label that simultaneously categorizes, summarizes, and accounts for each piece of data" (p.43)
"first step in moving beyond concrete statements in the data to making analytic interpretations." (p.43)

"codes stick closely to the data, show actions, and indicate how
dilemmas surrounding disclosure arise." (p.45)

"Coding is the pivotal link between collect-
ing data and developing an emergent theory
to explain these data." (p.46)

"The logic of grounded theory coding differs from quantitative logic that
applies preconceived categories or codes to the data."(p.46)

"Language plays a crucial role"(p.46)
"Specific use of language reflects views and values." (p.47)

"Coding impels us to make our participants' language
problematic to render an analysis of it. Coding should inspire us to examine
hidden assumptions in our own use of language as well as that of our participants." (p.47)

"we try to understand participants' views and actions from their perspectives." (p.47)

Initial coding questions:
"• 'What is this data a study of?' (Glaser, 1978: 57; Glaser & Strauss, 1967)
• What does the data suggest? Pronounce?
• From whose point of view?
• What theoretical category does this specific datum indicate? (Glaser, 1978)" (p.47)

"Try to see actions in each segment of data rather than applying preexisting categories to the data." (p.47)
"Attempt to code with words that reflect action." (p.47-48)

"Initial grounded theory coding can prompt you to see areas in which you lack needed data." (p.48)

active coding -> use gerunds
"We gain a strong sense of action and sequence with gerunds." (p.49)

"If you ignore, gloss over, or leap beyond participants'
meanings and actions, your grounded theory will likely reflect an outsider's,
rather than an insider's view." (p.49)
"Outsiders often import an alien professional lan-
guage to describe the phenomenon." (p.49)

"Make your codes fit the data
you have rather than forcing the data to fit them." (p.49)

To do while coding:
"
Remain open
Stay close to the data
Keep your codes simple and precise
Construct short codes
Preserve actions
Compare data with data
Move quickly through the data.
"

"Fresh data and line-by-line coding prompt you to remain open to the data
and to see nuances in it"(p.50)

Being critical:
"Line-by-line coding frees you from becoming so immersed in your respon-
dents' worldviews that you accept them without question. Then you fail to look
at your data critically and analytically. Being critical about your data does not
necessarily mean being critical of your research participants. Instead, being
critical forces asking yourself questions about your data." (p. 51)

in vivo codes: "codes of participants' special terms"(p.55)
"useful analytic point of departure" (p.55)
"preserve participants' meanings of their views and actions" (p.55)

3 kinds of useful in vivo codes:
"
* Those general terms everyone 'knows' that flag condensed but significant
meanings
* A participant's innovative term that captures meanings or experience
* Insider shorthand terms specific to a particular group that reflect their
perspective.
" (p.55)

"Pursue telling terms" (p.57)

Focused Coding:
"using the most significant and/or frequent earlier codes to sift through large amounts of data"
"which initial codes make the most analytic sense" (p.57)

Axial Coding:
"relates categories to subcategories, specifies the properties and dimensions of a category" (p.60)

Theoretical coding:
"Glaser (1978: 72) introduced theoretical
codes as conceptualizing 'how the substantive codes may relate to each other
as hypotheses to be integrated into a theory.' In short, theoretical codes specify
possible relationships between categories you have developed in your focused
coding."(p.63)

"When your analysis indicates, use theoretical codes to help you clarify and
sharpen your analysis but avoid imposing a forced framework on it with them."
"interrogate yourself about whether these theoretical codes interpret all the data" (p.66)

"each preconceived idea should earn
its way into your analysis-including your own ideas from previous studies"(p.68)

TODO: Check against my analysis:
"Be careful about applying a language of intention,
motivation, or strategies unless the data support your assertions. You cannot assume
what is in someone' s mind-particularly if he or she does not tell you."(p.68)

"Take an examined stance about whose point of view your codes reflect,"(p.69)

\end{comment}

%\section{Cooking Data With Care}
%or Critical data science? Or both?

\section{Validation}
