\chapter{Methods}
\label{chap:methods}

This chapter describes the methodology applied for the study of edit filters.

\section{Trace Ethnography}
\label{sec:trace-ethnography}

The main theoretical framework for the analysis presented in chapters~\ref{chap:filters} and~\ref{chap:overview-en-wiki} constitutes the trace ethnography.
The concept was first utilised by Geiger and Ribes in their 2010 work ``The work of sustaining order in Wikipedia: the banning of a vandal''~\cite{GeiRib2010} and introduced in detail in a 2011 paper~\cite{GeiRib2011} by the same authors.
They define trace ethnography as a methodology which
``combines the richness of participant-observation
with the wealth of data in logs so as to reconstruct
patterns and practices of users in distributed
sociotechnical systems''.
It is supposedly especially practical for research in such distributed systems, since there direct partipants observation is impractical, costly and tend to miss phenomena which manifest themselves in the communication between spatially separated sites rather than in the single location.

In~\cite{GeiRib2011} the scholars use documents and document traces: MediaWiki revision data, more specifically–edit summary fields of the single revisions and markers/codes left within the edit summaries; documentation of semi-automated software tools; and even use the tools (Huggle and Twinkle) themselves to observe what traces these leave;
in order to reconstruct quite exactly single strands of actions and comprehend how different agents on Wikipedia work together towards the blocking of a single malicious user.
They refer to ``turn[ing] thin documentary traces into “thick descriptions” of actors and events''.
What is more, these traces are used by Wikipedians themselves in order to do their work efficiently.
Geiger and Ribes underline the importance of insider knowledge when reconstructing actions and processes based on the traces,
the need for ``an ethnographic understanding of the activities, people, systems, and technologies which contribute to their production''.

They alert that via trace ethnography only that can be observed which is recorded by the system and records are always incomplete.
This consideration is elaborated on in more detail in~\cite{GeiHal2017}, where Geiger and Halfaker make the point that ``found data'' generated by a system for a particular purpose (e.g. revision history whose purpose is to keep a track of who edited what when and possibly revert (to) a particular revision) is rarely ideally fitting as a dataset to answer the particular research question of a scientist.
The importance of interpreting data in their corresponding context and the pitfalls of tearing analysis out of context are also underlined by Charmaz in~\cite{Charmaz2006}.
She cites intersecting(syn) data from multiple sources/of different types as a possible remedy for this problem. %TODO re-phrase, sounds weird

Geiger and Ribes~\cite{GeiRib2011} also warn of possible privacy breaching through thickening traces:
although records they use to reconstruct paths of action are all open, the thick descriptions compiled can suddenly expose a lot of information about single users which never existed in this form before and who never gave their informed consent for their data being used this way.


\section{Emergent Coding}
\label{sec:gt}

In order to gain a detailed understanding of what edit filters are used for on English Wikipedia, in chapter~\ref{chap:overview-en-wiki} all filters are labeled via emergent coding.
Different variations of coding are widely used by grounded theory scholars for making sense of (mainly qualitative) data.

Grounded theory describes a myriad/... of frameworks/... for building a scientific theory \emph{grounded} in (mostly qualitative) data analysis.
Here, no finished theory is developed, but instead I employed a grounded theory inspired coding process in order to understand what edit filters filter.
I've followed the coding guidelines proposed/described by Charmaz in~\cite[p.42-71]{Charmaz2006}.
%TODO edit: I don't really do anything with constructivist gt, so kick out "I've chosen this interpretation of GT"
I've chosen Charmaz's interpretation of grounded theory (she speaks of ``grounded theor\emph{ies}'' and calls her own constructivist rendering of it ``\emph{a} way of doing grounded theory'') precisely because of her acknowledgement of the subjective nature of every (piece of) research which is shaped by the believes, background and theoretical understanding of the people who conduct it, who always \emph{interpret} the subject they study rather than give an exact portrayal of it:
``we are part of the world we study and the data we collect. We \textit{construct} our grounded theories through our past and present involvements and interactions with people, perspectives, and research practices''~\cite[p.10]{Charmaz2006}
%TODO compare with section above that records are incomplete

She advocates for ``gathering rich–detailed and full–data and placing them in their relevant situational and social contexts''~\cite[p.10-11]{Charmaz2006} which is in line with Geiger and Ribes thick descriptions generated by trace ethnography
\footnote{As a matter of fact, both Charmaz, and Geiger and Ribes refer to ``thick descriptions'' which were coined as a term by~\cite{Geertz1973}}.

Coding is the process of labeling data in an attempt to comprehend it in a systematic fashion.
It is about seeking patterns in data and later–trying to understand these patterns and the relationships/correlations between them.
In the present inquiry, I applied emergent coding in chapter~\ref{chap:overview-en-wiki} when trying to make sense of the tasks EN Wikipedia's edit filters are employed for.
Key characteristic of the method are to let the codes emerge during the process contrasted to starting with a set of preconcieved codes.
Scholars regard this as useful because that way the danger of trying to press data in predefined categories while potentially overlooking other, better fitting codes is reduced.
Instead, the codes emerge/stem directly from observations of the data and stay linguistically close to the data.
A coding process is comprised of at least two phases: initial and focused coding.
During the initial phase (syn!) fragments of data are studied closely for ``their analytic import'' and potential promising codes.
During focused coding, the most promising initial codes are extensively tested against the data.
Since coding and analysis take place simultaneously, it is also part of the process/common to come back later and re-code parts of the data with labels that have emerged (syn) later (syn) in the process.
Finally, a third coding phase took place–the so called axial coding which ``relates categories to subcategories, specifies the properties and dimensions of a category''~\cite[p.60]{Charmaz2006}. % my organisation of the codes in vandalism/good faith/maintenance/unknown


\section{Open Science}

The whole work tries to adhere to the principles of open science and reproducible research. %TODO what are the principle of open science? refs are missing
All the computations I have done and other artefacts I have used or compiled are openly accessible in the project's repository~\cite{github}.
and can be re-used under a free license (which one?).
And have been openly accessible since the very beginning.
Everyone interested can follow the process and/or use the data or scripts in order to verify my computations (syn) or run their own and thus continue this research along one of the directions suggested in section~\ref{sec:further-studies} or in a completely new one.

